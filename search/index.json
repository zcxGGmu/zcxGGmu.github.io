[{"content":"[TOC]\n0 搭建思路 0.1 什么是 GitHub Pages？ Github Pages 是一个可以从你的 Github 源码仓库中直接生成个人、组织或者项目页面的静态站点托管服务。\nGithub Pages 只能托管静态站点，不能运行服务器端源码，比如 PHP、Java、Python 或者 Ruby。\n0.2 什么是 Hugo？ Hugo 是用 Go 语言写的静态网站生成器（Static Site Generator）。可以把 Markdown 文件转化成 HTML 文件。\n0.3 网站搭建思路 创建 2 个 GitHub 仓库 博客源仓库：储存所有 Markdown 源文件（博客内容），和博客中用到的图片等。 GitHub Pages 仓库：储存由 Hugo 从Markdown 文件生成的 HTML 文件。 将在博客源仓库中 Hugo 生成的静态 HTML 文件部署到远端 GitHub Pages 仓库中。 1 准备工作 1.1 hugo下载 前往【Hugo Github Tags】，选择对应版本下载，下载后解压即可，注意必须 Extended 版本否则会报错：\n注意，本文使用的是 hugo-0.129.0 ，其他版本 hugo 目录结构与配置流程和本文可能不一致。\n1.2 github建仓 创建 Github Page 仓库\n前往【Github官网】，创建仓库 {github用户名}.github.io 前往Setting -\u0026gt; Pages -\u0026gt; Branch选择main分支，然后保存，会自动开启 https://{github用户名}.github.io 的地址，这地址也是以后访问博客的地址 创建博客源仓库\nGithub创建一个新的仓库，用于存放Hugo的主文件\n前往Setttings -\u0026gt; Developer Settings -\u0026gt; Personal access tokens，创建一个token(classic)\ntoken选择永不过期，并勾选 repo 和 workflow 选项\n为保证安全，将生成的token，保存的仓库的变量中，前往Settings -\u0026gt; Secrets and variables -\u0026gt; Actions中设置，变量命名为 TOKEN\n2 部署流程 2.1 github page配置 hugo new site my-blog 创建 hugo 文件夹\n主题配置\n前往【Hugo Themes】，查找自己喜欢的主题，进行下载\n以【Stack主题】为例，将下载好的主题解压，放到/themes文件夹中\n将exampleSite样例数据中的 Content 和 hugo.yaml 复制到主文件夹中，并删掉hugo.toml和content/post/rich-content\n修改 hugo.yaml 中的 theme，将他修改为跟主题文件夹同名\n1 theme: hugo-theme-stack-xxx 再次启动hugo服务查看主题，具体主题配置修改 hugo.yaml，这里给出一份本人的配置，可以直接 copy 过去，注意替换掉带有个人信息的字段：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 baseurl: https://zcxggmu.github.io/ languageCode: en-us theme: hugo-theme-stack title: Example Site copyright: zcxGGmu # Theme i18n support # Available values: ar, bn, ca, de, el, en, es, fr, hu, id, it, ja, ko, nl, pt-br, th, uk, zh-cn, zh-hk, zh-tw DefaultContentLanguage: zh-cn # Set hasCJKLanguage to true if DefaultContentLanguage is in [zh-cn ja ko] # This will make .Summary and .WordCount behave correctly for CJK languages. hasCJKLanguage: true languages: zh-cn: languageName: 中文 title: zcxGGmu weight: 2 params: description: 演示说明 services: # Change it to your Disqus shortname before using disqus: shortname: \u0026#34;hugo-theme-stack\u0026#34; # GA Tracking ID googleAnalytics: id: pagination: pagerSize: 3 permalinks: post: /p/:slug/ page: /:slug/ params: mainSections: - post featuredImageField: image rssFullContent: true favicon: /favicon.ico # e.g.: favicon placed in `static/favicon.ico` of your site folder, then set this field to `/favicon.ico` (`/` is necessary) footer: since: 2024 customText: dateFormat: published: 2006-01-02 lastUpdated: Jan 02, 2006 15:04 MST sidebar: emoji: 🍥 subtitle: riscv, linux-kernel/kvm, llm-app/infer avatar: enabled: true local: true src: img/avatar.png article: math: false toc: true readingTime: true license: enabled: false default: Licensed under CC BY-NC-SA 4.0 comments: enabled: false provider: disqus disqusjs: shortname: apiUrl: apiKey: admin: adminLabel: utterances: repo: issueTerm: pathname label: beaudar: repo: issueTerm: pathname label: theme: remark42: host: site: locale: vssue: platform: owner: repo: clientId: clientSecret: autoCreateIssue: false # Waline client configuration see: https://waline.js.org/en/reference/component.html waline: serverURL: lang: pageview: emoji: - https://unpkg.com/@waline/emojis@1.0.1/weibo requiredMeta: - name - email - url locale: admin: Admin placeholder: twikoo: envId: region: path: lang: # See https://cactus.chat/docs/reference/web-client/#configuration for description of the various options cactus: defaultHomeserverUrl: \u0026#34;https://matrix.cactus.chat:8448\u0026#34; serverName: \u0026#34;cactus.chat\u0026#34; siteName: \u0026#34;\u0026#34; # You must insert a unique identifier here matching the one you registered (See https://cactus.chat/docs/getting-started/quick-start/#register-your-site) giscus: repo: repoID: category: categoryID: mapping: lightTheme: darkTheme: reactionsEnabled: 1 emitMetadata: 0 gitalk: owner: admin: repo: clientID: clientSecret: proxy: cusdis: host: id: widgets: homepage: - type: search - type: archives params: limit: 5 - type: categories params: limit: 10 - type: tag-cloud params: limit: 10 page: - type: toc opengraph: twitter: # Your Twitter username site: # Available values: summary, summary_large_image card: summary_large_image defaultImage: opengraph: enabled: false local: false src: colorScheme: # Display toggle toggle: true # Available values: auto, light, dark default: auto imageProcessing: cover: enabled: true content: enabled: true ### Custom menu ### See https://stack.jimmycai.com/config/menu ### To remove about, archive and search page menu item, remove `menu` field from their FrontMatter menu: main: [] social: - identifier: github name: GitHub url: https://github.com/zcxGGmu params: icon: brand-github - identifier: twitter name: Twitter url: https://twitter.com params: icon: brand-twitter related: includeNewer: true threshold: 60 toLower: false indices: - name: tags weight: 100 - name: categories weight: 200 markup: goldmark: extensions: passthrough: enable: true delimiters: block: - - \\[ - \\] - - $$ - $$ inline: - - \\( - \\) renderer: ## Set to true if you have HTML content inside Markdown unsafe: true tableOfContents: endLevel: 4 ordered: false startLevel: 1 highlight: noClasses: false codeFences: true guessSyntax: true lineNoStart: 1 lineNos: true lineNumbersInTable: true tabWidth: 4 github 部署\n回到hugo主目录下，执行命令hugo -D，会生成 public 静态资源文件夹\n在 public 执行以下命令，关联到远程 {github用户名}.github.io 仓库上：\n1 2 3 4 5 6 git init git add . git commit -m \u0026#34;first commit\u0026#34; git branch -M main git remote add origin {你的github仓库地址} git push -u origin main 上传成功后访问 https://{github用户名}.github.io，成功搭建属于自己的Hugo博客\n2.2 博客源配置 再配置一个博客源仓库的好处是：\n实现 Github Action 自动部署 blog源文件仓和展示仓分离，保证数据安全 在hugo主目录下创建一个.github/workflows/xxxx.yaml文件，将以下内容复制进去，想具体了解更多，可查看【Github Action文档】。注意，EXTERNAL_REPOSITORY 对应是的地址是和 ./public 关联的仓库地址。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 name: deploy # 代码提交到main分支时触发github action on: push: branches: - main jobs: deploy: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v4 with: fetch-depth: 0 - name: Setup Hugo uses: peaceiris/actions-hugo@v3 with: hugo-version: \u0026#34;latest\u0026#34; extended: true - name: Build Web run: hugo -D - name: Deploy Web uses: peaceiris/actions-gh-pages@v4 with: PERSONAL_TOKEN: ${{ secrets.你的token变量名 }} EXTERNAL_REPOSITORY: 你的github名/你的仓库名 PUBLISH_BRANCH: main PUBLISH_DIR: ./public commit_message: auto deploy 在hugo主文件创建.gitignore文件，来避免提交不必要的文件\n1 2 3 4 5 6 7 # 自动生成的文件 public resources .hugo_build.lock # hugo命令 hugo.exe 将hugo的主文件上传到仓库，上传成功后会触发Github Action，来自动部署你的静态页面:\n1 2 3 4 5 6 git init git add . git commit -m \u0026#34;first commit\u0026#34; git branch -M main git remote add origin {你的github仓库地址} git push -u origin main 3 适当美化 仅列出基本的美化，观感好一点就可以，没必要花里胡哨\n3.1 修改字体 前往【100font】，下载自己想要的字体，这边演示缝合像素字体，字体文件为 fusion-pixel-10px-monospaced-zh_hans.ttf\n把字体文件放入assets/font下(文件夹自己创建)\n将以下代码修改并复制到layouts/partials/footer/custom.html文件中(文件不存在就自己创建)\n字体名：给字体命名一个别名，随便填写就好，保持统一就行 字体文件名：字体文件的全名，带后缀名的，也就是 xxx.ttf 1 2 3 4 5 6 7 8 9 10 11 \u0026lt;style\u0026gt; @font-face { font-family: \u0026#39;字体名\u0026#39;; src: url({{ (resources.Get \u0026#34;font/字体文件名\u0026#34;).Permalink }}) format(\u0026#39;truetype\u0026#39;); } :root { --base-font-family: \u0026#39;字体名\u0026#39;; --code-font-family: \u0026#39;字体名\u0026#39;; } \u0026lt;/style\u0026gt; 博客字体修改完成\n3.2 动态背景 点线漂浮 particles.js\n【particles.js文档】 前往【配置页面】配置参数，参数按自己喜好即可，唯一注意要修改的参数是 detect_on，要改成 window\n下载配置文件，以及 particles.js 所需要的js文件\n把下载好的文件，解压并将以下两个文件放到assets/background文件夹下\nparticlesjs-config.json particles.min.js 在layouts/partials/footer/custom.html中，引入以下代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 \u0026lt;div id=\u0026#34;particles-js\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script src={{ (resources.Get \u0026#34;background/particles.min.js\u0026#34;).Permalink }}\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; particlesJS.load(\u0026#39;particles-js\u0026#39;, {{ (resources.Get \u0026#34;background/particlesjs-config.json\u0026#34;).Permalink }}, function() { console.log(\u0026#39;particles.js loaded - callback\u0026#39;); }); \u0026lt;/script\u0026gt; \u0026lt;style\u0026gt; #particles-js { position: fixed; top: 0; left: 0; width: 100%; z-index: -1; } \u0026lt;/style\u0026gt; 配置完成\n","date":"2024-10-17T10:40:32+08:00","permalink":"https://zcxggmu.github.io/p/hugo-github-blog/","title":"Hugo Github Blog"},{"content":"0 参考 华为 | 基于Rust的下一代虚拟化平台-StratoVirt - Rust精选 (rustmagazine.github.io)\nStratoVirt中MicroVM启动过程-CSDN博客\n1 StratoVirt概述 StratoVirt是计算产业中面向云数据中心的企业级虚拟化平台，实现了一套统一架构，支持虚拟机、容器和Serverless三种场景，在轻量低噪、软硬协同、安全等方面具备关键技术竞争优势。StratoVirt在架构设计和接口上预留了组件化拼装的能力和接口，因此StratoVirt可以按需灵活组装高级特性，直至演化到支持标准虚拟化，在特性需求、应用场景和轻快灵巧之间找到最佳的平衡点。\n本文主要讲述了：\n介绍StratoVirt的基本能力与特性，使读者了解StratoVirt的使用场景以及技术特点； 介绍虚拟机技术的演进； 介绍StratoVirt虚拟化技术原理，为后续实现做理论铺垫； 结合代码讲解StratoVirt的基本实现，从零开始打造一个具备基本功能的轻量级虚拟化平台。 Strato取自地球大气层中的平流层 (stratosphere)。大气层可以保护地球不受外界环境侵害，平流层则是大气层中最稳定的一层；类似的，虚拟化层是比操作系统更为底层的隔离层，既能保护操作系统平台不受上层恶意应用的破坏，又能为正常应用提供稳定可靠的运行环境。StratoVirt中的Strato寓意其为保护openEuler平台上业务平稳运行的轻薄保护层；同时，它也承载了项目的愿景与未来：轻量、灵活、安全和完整的保护能力。\nStratoVirt是openEuler平台依托于虚拟化技术打造的稳定和坚固的保护层，它重构了openEuler虚拟化底座，具有以下六大技术特点。\n**强安全性与隔离性。**采用内存安全语言Rust编写，保证语言级安全性；基于硬件辅助虚拟化实现安全多租户隔离，并通过 seccomp 进一步约束非必要的系统调用，减小系统攻击面。 **轻量低噪。**轻量化场景下冷启动时间 \u0026lt; 50ms，内存开销 \u0026lt; 4MB。 **高速稳定的I/O能力。**具有精简的设备模型，并提供了稳定高速的I/O能力。 **资源伸缩。**具有毫秒级别的设备伸缩时延，为轻量化负载提供灵活的资源伸缩能力。 **全场景支持。**目前支持x86和ARM平台。x86支持VT、鲲鹏支持Kunpeng-V，实现多体系硬件加速；可与容器Kubernetes生态无缝集成，在虚拟机、容器和Serverless场景有广阔的应用空间。 **扩展性。**架构设计完备，各个组件可灵活地配置和拆分；设备模型可扩展，可以扩展PCIe等复杂设备规范，向通用标准虚拟机演进。 2 发展背景 在开源虚拟化技术的发展历程中，QEMU/KVM 一直是整个虚拟化产业发展的基石和主线。随着多年的发展和迭代，QEMU也沉积了庞大的代码基线和繁多的历史设备。据统计 QEMU 已有 157 万行代码，而且其中很大一部分代码是用于历史遗留 (legacy) 功能或者设备的，功能和设备严重耦合在一起，导致在轻量化场景中无法轻装上阵。\nStratoVirt采用精简的设备模型，提供高速稳定的I/O能力，做到轻量低噪，达到毫秒级的资源伸缩能力，同时架构设计预留了组件化能力，支撑向标准虚拟化方向演进。\n另一方面，在过去十几年QEMU的CVE（Common Vulnerabilities \u0026amp; Exposures，通用漏洞披露）安全问题中，发现其中有将近一半是因为内存问题导致的，例如缓冲区溢出、内存非法访问等。如何有效避免产生内存问题，成为编程语言选型方面的重要考虑。因此，专注于安全的Rust语言脱颖而出。Rust语言拥有强大的类型系统、所有权系统、借用和生命周期等机制，不仅保证内存安全，还保证并发安全，极大地提升软件的质量。在支持安全性的同时，具有零成本抽象的特点，既提升代码的可读性，又不影响代码的运行时性能。同时，Rust语言拥有强大的软件包管理器和项目管理工具—— Cargo，不仅能够方便、统一和灵活地管理项目，还提供了大量的代码扫描工具，能进一步提升开发者的编码风格和代码质量。\n业界有很多厂商也在尝试使用Rust语言发展虚拟化技术。谷歌公司是最早尝试使用Rust语言进行虚拟化开发的厂商之一，推出了CrosVM项目，它是Chrome操作系统中用于创建虚拟机的应用。后来亚马逊公司基于谷歌公司开源的CrosVM项目的部分功能，也推出了基于Rust语言的轻量级虚拟化项目Firecracker。两个厂商在开发的过程中，将虚拟化软件栈所需的基础组件进行解耦化设计，却发现了很多重复的通用组件，为了不重复造轮子，成立了Rust-VMM开源社区，用于管理所有通用的基础组件，便于构建自定义的Hypervisor。英特尔公司主导的Cloud Hypervisor项目也是基于Rust-VMM来实现对标准虚拟化的支持。\nStratoVirt项目同样也是基于Rust-VMM开发的，旨在实现一套架构既能满足轻量级虚拟化场景，又能满足标准虚拟化场景的使用。\n3 StratoVirt架构设计 如图6-1所示，StratoVirt核心架构自顶向下分为以下三层。\n**OCI（Open Container Initiative，开放容器）兼容接口。**兼容QMP（QEMU Monitor Protocol，QEMU监控协议），具有完备的OCI兼容能力。 **引导加载程序 (BootLoader)。**抛弃传统的 BIOS+GRUB（GRand Unified Bootloader，多重操作系统启动管理器）启动模式，实现了更轻更快的BootLoader，并达到极限启动时延。 **轻量级虚拟机 (MicroVM)。**充分利用软硬件协同能力，精简化设备模型，低时延资源伸缩能力。如下图所示： StratoVirt源码主要分为四个部分，可参考StratoVirt网站主页（https://gitee.com/openeuler/stratovirt中的标签v0.2.0之前代码）具体如下。\naddress_space：地址空间模拟，实现地址堆叠等复杂地址管理模式。 boot_loader：内核引导程序，实现快速加载和启动功能。 device_model：仿真各类设备，可扩展、可组合。 machine_manager：提供虚拟机管理接口，兼容QMP等常用协议，可扩展。 当前 StratoVirt 开源代码中实现的是轻量化虚拟机模型，是能实现运行业务负载的最小设备集合，但已经包括虚拟化三大子系统：CPU子系统、内存子系统、I/O设备子系统（包括中断控制器和各类仿真设备、例如virtio设备、串行设备等）。下面分别介绍其基本功能和特性。\n3.1 CPU子系统 StratoVirt是一套软硬件结合的虚拟化解决方案，其运作依赖于硬件辅助虚拟化的能力（如VT-x或Kunpeng-V）。CPU子系统的实现也是紧密依赖于硬件辅助虚拟化技术（内核KVM模块），例如对于x86架构的CPU而言，硬件辅助虚拟化为CPU增加了一种新的模式，即非根模式，在该模式下，CPU执行的并不是物理机的指令，而是虚拟机的指令。这种指令执行方式消除了大部分性能开销，非常高效。但是敏感指令（如I/O指令）不能通过这种方式执行，而且还是强制将CPU退出到根模式下交给Hypervisor程序（内核态KVM模块/用户态StratoVirt）去处理，处理完再重新返回到非根模式，执行下一条虚拟机的指令。\n而StratoVirt中的CPU子系统主要围绕着KVM模块中对CPU的模拟来实现，为了支持KVM模块中对CPU的模拟，CPU子系统主要负责处理退出到根模式的事件，以及在客户机操作系统内核开始运行前对vCPU的寄存器等虚拟硬件的状态进行初始化。整个CPU子系统的设计模型如图6-3所示。\nStratoVirt为每个vCPU创建了一个独立的线程，用来处理退出到根模式的事件，包括I/O的下发、系统关机事件、系统异常事件等，这些事件的处理以及KVM对vCPU接口的run函数独占一个单独线程，用户可以自己通过对vCPU线程进行绑核等方式让虚拟机的vCPU获取物理机CPU近似百分之百的性能。\n在客户机操作系统的内核运行前，对vCPU寄存器虚拟硬件状态信息的初始化则是与StratoVirt的另一个模块BootLoader相互结合，在BootLoader中实现了一种快速引导启动Linux内核镜像的方法。在这套启动流程中，BootLoader将主动完成传统BIOS对一些硬件信息的获取，将对应的硬件表保存在虚拟机内存中，同时将提供一定的寄存器设置信息，这些寄存器设置信息将传输给CPU模块，通过设置CPU结构中的寄存器值，让虚拟机CPU跳过实模式直接进入保护模式运行，这样内核就能直接从保护模式的入口开始运行，从而让StratoVirt的启动流程更轻量快速。\nCPU子系统另一大职责就是管理vCPU的生命周期，包括创建(new)、使能(realize)、运行(run)、暂停(pause)、恢复(resume)和销毁(destroy)。创建和使能过程就是结构体创建和寄存器初始化的流程，运行过程即实现KVM中vCPU运作和vCPU退出事件处理的流程。同时，得益于Rust语言对线程并发和同步的精密控制，CPU子系统用一种简单的方式实现了暂停与恢复的功能。任意vCPU线程收到暂停或恢复的命令后，就会通过改变信号量的方式，将该线程vCPU的状态变化传递到所有的vCPU线程，实现整台虚拟机的暂停或恢复，流程如图6-4和图6-5所示。\n当虚拟机的生命周期结束时，同样要从对vCPU的销毁开始实现，在StratoVirt中，vCPU销毁分为两种情况。\n(1)客户机内部正常关机：将通过对VM-Exit事件中的客户机关机(GUEST_SHUTDOWN)事件进行捕获，执行销毁并传递到所有的vCPU。(2)通过外部的QMP接口执行销毁：接收到QMP下发的命令后，将遍历每一个vCPU，依次执行销毁函数。\n两种方式最终都会调用每个vCPU实例的destroy函数，让vCPU发生从Running到Stopping的状态转换，同步所有的vCPU状态后，再进入Stopped状态，完成关机流程。正常关机后，所有的vCPU都会处于Stopped状态，非此状态的生命周期结束则是异常关机，将进入错误处理流程。\nStratoVirt的CPU模型较为精简，许多CPU特性以及硬件信息都将直接透传到虚拟机中，之后将在现有架构的基础上实现更多的高级CPU特性。\n3.2 内存子系统 StratoVirt进程运行在用户态，StratoVirt会完成虚拟机启动之前的准备工作，包括虚拟机内存初始化、CPU寄存器初始化、设备初始化等。其中，内存初始化工作和虚拟机的地址空间管理都是由StratoVirt的内存子系统完成的。\n相关概念 (1)地址空间(AddressSpace)：是地址空间模块的管理结构，负责整个虚拟机的物理地址空间管理。\n(2)内存区域(Region)：代表一段地址区间，根据这段地址区间的使用者，可以分为表6-1中的类型。\n(3)平坦地址空间(FlatRange)：如图6-6所示，是根据树状拓扑结构中内存区域的地址范围和优先级(priority)属性形成的线性视图。\n在树状拓扑结构中，每个内存区域都会对应一个优先级属性，如果低优先级和高优先级的内存区域占用的地址区间重叠，则低优先级的内存区域的重叠部分将会被覆盖，即在平坦视图中不可见，具体体现为在平坦视图中没有对应的平坦地址空间。\n(4)平坦视图(FlatView)：其中包含多个平坦地址空间。在通过地址空间管理结构地址空间访问设备或者内存时，使用平坦视图可以找到对应的平坦地址空间，平坦地址空间中有指向其所属内存区域的指针。\nStratoVirt地址空间模块的设计采用树状结构和平坦视图结合的方案。通过树状结构可以快速了解到各个内存区域之间的拓扑结构关系，这种分层、分类的设计，可以管理并区分存储器域与PCI总线域的地址管理，并形成与PCI设备树相呼应的树状管理结构。平坦视图则是根据这些内存区域的地址范围和优先级属性形成的线性视图，在通过地址空间管理结构内存地址空间访问设备或者内存时，使用平坦视图可以更加方便快捷地找到对应的内存区域。\n树状拓扑结构的更新很大可能会带来平坦视图的更新，一些设备或者模块需要获取最新的平坦视图并执行一些相应的操作。例如vhost设备，需要将平坦视图中的全部内存信息同步到内核vhost模块，以便通过共享内存的方式完成消息通知的流程。另外，也需要将已经分配并映射好的GPA和HVA信息注册到KVM模块，这样可以借助硬件辅助虚拟化加速内存访问的性能。基于以上需求，引入了地址空间监听函数链表，该链表允许其他模块添加一个自定义的回调函数，被注册到该链表中的函数将在平坦视图更新后被依次调用，这样即可方便地完成信息同步。\n虚拟机物理内存初始化 StratoVirt作为用户态虚拟机监控器，实际是运行在宿主机上的用户态进程。在StratoVirt进程的虚拟地址空间中，存在多段StratoVirt进程本身使用的地址区间，该地址区间是宿主机上的VMA（Virtual Memory Area，虚拟内存区间）。\n在虚拟机启动前，StratoVirt会初始化一段内存给虚拟机使用，这段虚拟内存区间也是StratoVirt进程的虚拟地址空间中的一段VMA。StratoVirt使用mmap系统调用来实现虚拟机内存资源的分配，得到的内存映射关系如图6-7所示。\nmmap映射有两种方式：匿名映射和文件映射。文件映射是将一个文件映射到真实的内存中，通过读写内存的方式直接读写文件。文件映射需要建立FileBackend结构体，包括文件std：：fs：：File和文件内偏移量两个成员。匿名映射则不需要文件，直接分配宿主机上的一段虚拟内存给虚拟机使用。StratoVirt现在支持匿名映射和文件映射两种方式，并利用文件映射的机制实现了大页特性。\n前面提到内存映射的信息需要同步到KVM，在StratoVirt内存子系统中同步的方式为：在内存地址空间初始化时，添加默认的监听回调函数KvmMemoryListener。KvmMemoryListener的功能为：当图6-6中的树状拓扑发生变化从而引起平坦视图变化时，KvmMemoryListener中的回调函数被调用，将新增或者删除的内存映射信息同步到KVM。\n3.3 I/O子系统 I/O子系统负责前端虚拟机内部和后端主机交互，如果把虚拟机当作一个黑盒，那么I/O子系统就是这个黑盒和外界连通的管道。一台物理主机通常会包含基本的I/O设备，如保存数据的磁盘、与外部进行通信的网卡、进行登录操作的串口设备等，这些都是主机与外部进行交互的必要设备。与物理机类似，虚拟机要实现基本的交互功能，也需要实现块设备、网络设备、串口设备等。虚拟这些设备是虚拟化技术I/O子系统的职责，称为I/O虚拟化。\nStratoVirt作为一款轻量级的虚拟化软件，也实现了基本的设备交互功能。这些设备除了磁盘(virtio-block)、网卡(virtio-net)、串口(serial)之外，还有用作特定用途的virtio-vsock、virtio-console等设备。按照之前所述的I/O虚拟化方式，分为完全设备模拟、半虚拟化模拟、设备直通和单根I/O虚拟化，StratoVirt采用的是完全设备模拟和半虚拟化模拟的方式虚拟出各个I/O设备。完全设备模拟主要用在串行设备的模拟上，半虚拟化模拟用在磁盘、网卡等设备的模拟上，下文就以串口和磁盘为例分别介绍这两种方式。\n完全设备模拟: 串口 串口设备主要是管理员或操作人员与虚拟机进行交互的手段，如登录到虚拟机内部执行命令等。StratoVirt实现了对UART（Universal Asynchronous Receiver Transmitter，通用异步收发传输器）16550A串口设备的模拟，该设备在计算机发展历史上出现较早，设备功能比较简单。在物理主机场景，由CPU访问16550A的寄存器实现OS和外界的串口通信，这些寄存器被映射到特定的CPU地址空间，如x86架构下，串口设备被映射到0x3f8起始的8字节的PIO地址空间。16550A设备的寄存器如表6-2所示。\n在虚拟化场景中，I/O虚拟化技术负责完全模拟相应PIO端口的寄存器访问，这种模拟方法不需要修改任何前端客户机操作系统代码，虚拟机无感知，所以称为完全设备模拟方式。在6.4.6节的实验中会详细介绍16550A各寄存器的作用，以及StratoVirt如何通过软件模拟该设备。\n半虚拟化模拟: 磁盘 磁盘设备的作用是存放数据，这个数据可以是OS必需的镜像文件，也可以是用户数据，如物理主机上的SATA盘。StratoVirt模拟的是virtio-block设备，它是一种基于virtio前后端协议的块设备，在虚拟机内部呈现的是vda、vdb等盘符，虚拟机内部可以格式化、读写这些设备，就像在物理主机上操作一样。与物理主机使用的设备驱动不同（如SATA盘驱动），StratoVirt模拟的磁盘设备基于virtio驱动，这种驱动是专门针对I/O虚拟化设计的，所以要求虚拟机内部安装virtio驱动，这种行为会造成虚拟机感知到自己处于虚拟化环境，所以是种半虚拟化的设备模拟方式。virtio驱动是对virtio协议的实现，该协议的作用是将前端的I/O请求发送到后端用户态Hypervisor，即StratoVirt，然后由StratoVirt代替前端做真正的I/O执行，这样就可以控制前端的I/O行为，避免造成逃逸等安全问题。\nvirtio协议是I/O虚拟化技术中常用的一种协议，本质上是种无锁的前后端数据共享方案，它实现了非常高效的前后端数据传递，当前主流的几种虚拟化软件（如QEMU）都支持virito协议，常见的设备包含virito-block、virito-net、virtio-gpu等。与上面介绍的基于PCI协议的virito不同，StratoVirt基于MMIO协议，MMIO协议定义了设备寄存器的MMIO地址。virtio-mmio部分寄存器如表6-3所示。\nStratoVirt实现了最基本的基于MMIO协议的virtio-block设备，相对于标准的PCI协议，MMIO协议具有启动速度快的特点，符合轻量化的使用场景。在实现中，虚拟出一个MMIO总线，所有设备都挂在MMIO总线下。当前端CPU线程访问设备时，经过MMIO总线的读写函数，再到具体设备(virtio-block)的读写函数，然后在virtio-block中模拟各个寄存器的访问。virito协议规定，前端驱动准备就绪后，会往0x70寄存器写入DRIVER_OK标记，当StratoVirt收到该标记时会做磁盘的初始化动作，比如监听前端事件、异步I/O完成事件等。当前端有磁盘I/O需要下发时，会通知到后端，后端将这些I/O请求下发到异步I/O模块(aio)，aio封装请求再通过系统调用接口(io_submit)下发到主机内核。当异步I/O完成后，主机内核会通知StratoVirt给虚拟机内部发送中断，一次完整的I/O流程就完成了。这样就完成了磁盘设备的模拟。\n4 从零开始构建StratoVirt 4.1 总体介绍 从现在开始，将通过一系列的动手实践构建一个精简版StratoVirt，其架构如图6-8所示。\n精简虚拟化实践旨在使用 Rust 语言指导零基础开发一个基本功能完备的用户态虚拟机监控软件，基于 Linux 提供的硬件辅助虚拟化能力，实现在多平台（x86和鲲鹏平台）的运行。预期的实现结果为用户可以通过串口登入虚拟机内部并执行基本的Linux命令。\n精简虚拟机实践主要包括KVM模型实现、内存模型实现、CPU模型实现、BootLoader实现以及串口设备实现。下面涉及的代码存放在openEuler社区 StratoVirt 仓库中，对应的分支为 mini_stratovirt_edu，该分支的各个提交分别对应 4.2~4.8 小节中的内容。\n4.2 KVM模型 Linux 提供的 KVM 模块可以充分利用硬件辅助虚拟化的特性，并提供了CPU、内存和中断的虚拟化支持。构建一个完整的虚拟机需要 CPU 模型构建、设备模拟等。为简单起见，本节将借助KVM API，构建一个最小化且可运行的虚拟机，运行一段汇编代码。具体流程为：创建虚拟机和vCPU线程，提供一段汇编指令让vCPU线程执行，并捕获和处理虚拟机退出事件。\n在开始前，先用 Rust 的构建系统和包管理器 Cargo 来创建一个新项目 — StratoVirt-mini：cargo new StratoVirt-mini，这行代码新建了一个名为 StratoVirt-mini 的目录，该目录名也作为项目的名字。进入目录，可以看见 Cargo 生成了两个文件和一个目录：一个 Cargo.toml 文件，一个 src 目录用来存放代码，当前仅有一个 main.rs 文件，它还在项目目录下初始化了一个 git 仓库，便于管理代码。\nstep1: 定义汇编指令 最小化模型暂时并不运行一个操作系统内核，而是提供一段指令给vCPU执行，执行完毕后，vCPU退出。提供给vCPU执行的汇编指令定义在如下src/main.rs 中。\nStratoVirt-mini_stratovirt_edu/src/main.rs\n这段代码的逻辑为：\n将 0x3f8 存在寄存器 DX 中； 将 AL 寄存器和 BL 寄存器中的值相加，输出到 0x3f8 端口； 并将换行符 \\n 输出到 0x3f8 端口，最后执行 HLT 指令停止虚拟机的运行。 step2: 打开KVM模块并创建虚拟机 在打开 KVM 模块之前，需要引入 Rust 的第三方库 kvm-bindings 和 kvm-ioctls：\n其中 kvm-bindings 使用 Rust 语言对 KVM 模块使用的结构体进行封装； kvm-ioctls 库对KVM API进行封装。 引入的方法为：①在项目 Cargo.toml 的 [dependencies] 中描述第三方库的版本信息；②在需要使用库的文件头部处，通过 use 来引入需要使用的第三方库中对应的结构体或函数。代码如下：\nStratoVirt-mini_stratovirt_edu/Cargo.toml\nStratoVirt-mini_stratovirt_edu/src/main.rs\n引入第三方库 kvm-bindings 和 kvm-ioctls 之后，调用 kvm_ioctls：KVM的构造函数打开 /dev/kvm 模块，该函数会返回一个kvm对象。通过调用该对象的 create_vm 成员函数，可以得到所创建虚拟机的句柄 vm_fd。代码如下：\nStratoVirt-mini_stratovirt_edu/src/main.rs\nstep3: 初始化虚拟机内存 那么在启动虚拟机之前，如何配置虚拟机的内存大小呢？\n这里固定分配 64 KB的内存给虚拟机使用。下面的介绍中，用 GPA 代表客户机物理地址，用 HVA 代表宿主机虚拟地址。\n首先在 StratoVirt-mini 进程中使用 mmap 系统调用分配一段宿主机上的虚拟地址资源，并得到对应的 HVA。值得注意的是，Rust 语言中对 mmap 的系统调用需要使用第三方库 libc，为此在 Cargo.toml 中的 [dependencies] 后面添加 libc 第三方库以及它的版本信息libc=\u0026quot;\u0026gt;=0.2.39\u0026quot;。\n如下图所示，得到 HVA 之后，需要将 HVA 与 GPA 的映射关系，以及配置的客户机内存大小通知给KVM。其中映射关系和内存大小的信息保存在kvm-binding 提供的 kvm_userspace_memory_region 结构体中，然后通过 step2 中得到的虚拟机句柄 vm_fd 的 set_user_memory_region成员方法，将内存映射信息通知KVM。\n最后将已定义的汇编机器码写入 HVA 起始地址，vCPU在运行时，会退出到KVM，KVM借助硬件辅助虚拟化技术建立页表，从而 vCPU 可以执行这段内存中保存的汇编指令。代码如下：\nStratoVirt-mini_stratovirt_edu/src/main.rs\nstep4: 创建并初始化虚拟机和vCPU 接下来使用虚拟机句柄 vm_fd 的 create_vcpu 成员函数来创建vCPU。通过得到的 vCPU 句柄设置通用寄存器和段寄存器。其中 CS（Code Segment，代码段）寄存器设置为0，IP（Instruction Point，指令指针）寄存器为虚拟机内存起始地址，RAX 寄存器设置为 2，RBX 寄存器设置为 3，这些寄存器的具体功能将在 4.4 节中介绍。代码如下：\nStratoVirt-mini_stratovirt_edu/src/main.rs\nstep5: 处理vCPU退出事件 在 vCPU 执行 step1（定义汇编指令）中定义的汇编指令时，会访问 I/O 端口 0x3f8，该端口资源没有被映射，因此虚拟机退出，进入KVM。KVM 同样无法处理该访问请求，进一步退出到用户态 Hypervisor 程序中。在收到 VcpuExit::IoOut 的 vCPU 退出事件时，将访问请求的信息以一定格式打印出来。代码如下：\nStratoVirt-mini_stratovirt_edu/src/main.rs\n编写完最小化虚拟机模型后，执行 cargo run 可以编译并运行工程，可以得到以下运行结果。\nTerminal\n根据运行结果可知，共发生了三次虚拟机 VM_Exit 事件：\n前两次虚拟机退出均为访问 I/O 端口 0x3f8。其中，数据 53 为 step5 中设置的RAX寄存器、RBX寄存器以及数字0的ASCII码相加得到的结果；数据10为换行符对应的ASCII码。 最后一次虚拟机退出为 HLT 指令，虚拟机结束运行，Hypervisor 进程退出。 4.3 内存模型 4.2 节介绍了创建简单虚拟机的方法，并执行了一段简单的汇编指令。为将这段汇编指令保存到虚拟机内存中，分配了 64 KB的地址资源来存放这段汇编指令代码。但是，若要支持 StratoVirt 项目的进一步扩展，src/main.rs 文件中的内存实现存在如下很多不足：\n4.5 节中，将删去测试使用的这段汇编代码，而启动一台标准的虚拟机。如果运行客户机内核，那么 64 KB的虚拟机内存资源远远不够。如果新增内存热插拔特性，则需要新增多段内存映射关系。 在 Intel Q35 芯片组的地址空间布局中，4 GB以下的一部分地址资源被固定分配给Flash、中断控制器、PCI设备等，因此内存可占用的地址资源被分割成多个区间。 考虑到以上限制，以及增强地址资源管理灵活性的需求，本节新增了 memory 子模块，并在 src/main.rs 头部中声明：mod memory。\nmemory 子模块中主要包含地址资源管理、虚拟机内存管理、内存读写等功能。\n地址空间布局 StratoVirt 设置的客户机物理地址空间布局如下图所示：\nStratoVirt 设置的客户机物理地址空间布局如上图所示，其中：\n内存占用 0~3GB、4GB 以上的地址资源； 3~4GB 的地址资源提供给设备、中断控制器使用； 内存的 0~1MB 空间内，固定存放启动客户机内核的相关配置内容，这部分内容将在 4.5 节详细介绍。 为支持更大的虚拟机内存规格、模拟更多的设备类型，将 StratoVirt 项目中各个组件用到的地址资源范围定义在常量中，当新增其他类型的设备时，可以在全局变量中动态添加设备使用的地址资源。上图中定义的客户机物理地址空间布局定义在 src/memory/mod.rs 中，其中资源类型定义在 LayoutEntryType 枚举结构体中，每个资源的范围定义在常量 MEM_LAYOUT 中。代码如下。\nStratoVirt-mini_stratovirt_edu/src/memory/mod.rs\n内存地址映射管理 4.2 节构造最小化KVM模型时，初始化了一段宿主机虚拟内存提供给虚拟机使用，并将内存信息注册到KVM。但是，src/main.rs 中的实现方式不能够满足项目的可扩展需求，例如添加一个 virtio` 设备。virtio设备在和前端驱动交互过程中遵循virtio协议，通过前后端共享内存的方式通信，后端virtio设备需要从virtqueue取出前端驱动下发的事件处理请求，其中virtqueue就存放在内存中。因此，内存管理模块不仅需要为 HVA 和 GPA映射关系建立管理结构，而且需要提供访问接口供其他模块使用。\n如下图所示，管理一段内存映射的结构体定义为 HostMemMapping，该结构体通过 mmap 系统调用分配宿主机虚拟内存，并与客户机物理地址空间建立映射。映射关系会保存在 HostMemMapping 结构体中。在 HostMemMapping 结构体的析构函数中，会通过 unmap 系统调用释放这段宿主机虚拟内存资源。\n作为内存子模块对外的接口，GuestMemory 结构体保存所有的内存映射关系。例如，当设置虚拟机内存规格高于 3 GB时，内存将被分割为两部分：0~3GB和高于4GB的部分。这两部分的映射关系将分别保存在两个 HostMemMapping 对象中，这两个 HostMemMapping 对象将保存在GuestMemory 结构体的成员中。\nStratoVirt-mini_stratovirt_edu/src/memory/guest_memory.rs\nGuestMemory 提供的构造函数需要传入的参数为创建的客户机句柄、客户机内存规格，该构造函数会根据地址空间布局和内存大小来建立 HVA 和GPA 映射关系，并注册到KVM。\n内存访问管理 内存访问接口是其他模块访问内存的方式，需要达到简单易用的目的。首先，先实现最基本的接口：\n**写接口：**将长度已知的字节流写入客户机物理内存中的指定地址处，地址在函数参数中指定。 **读接口：**从客户机物理内存的指定地址处读出一段字节流，并保存在输入参数中。 StratoVirt-mini_stratovirt_edu/src/memory/guest_memory.rs\n在 4.5 节中，BootLoader模块将使用 read/write 接口访问客户机内存，并将客户机内核文件保存在内存中。使用上面代码中的接口，示例代码如下：\nStratoVirt-mini_stratovirt_edu BootLoader 对内存模块的调用：\n这段代码会将内核的数据保存在一个 Vec（Rust语言定义的数据结构，表示数组）中，然后将 Vec 中的数据写入内存。这里共存在两次复制，如果文件过大，复制的时间和空间开销都不容忽视。\nRust 优化设计：\n那么，内存模块的访问接口参数能否更加灵活，从而支持更多的参数类型？一个可行的方法是使用 trait（Rust语言中的类型，为一组方法的集合）和 trait 对象（trait object，Rust语言中实现了一组traits的数据对象）接口优化的实现，可以参考 StratoVirt 项目 mini_stratovirt_edu 分支中 GuestMemory 的 read/write 成员函数。\n内存访问 read/write 接口优化后，仍存在一个问题，对于如下代码中的 SplitVringDesc 数据结构，如果想将类型为 SplitVringDesc 的数据对象写入内存，仍需要先转换为 Vec，再将其写入内存。读者可以思考如何优化 GuestMemory 的访问接口，增强易用性。针对这个问题的接口优化，可以参考 StratoVirt 项目 mini_stratovirt_edu 分支中 GuestMemory 的 read_object/write_object 成员函数，以及src/helper/byte_code.rs 子模块的实现。\nStratoVirt-mini_stratovirt_edu VirtQueue 中 Vring 的 Descriptor 结构体：\nByteCode trait 定义在 src/helper/byte_code 子模块中，该trait主要实现数据结构和slice（Rust语言中的数据类型，数组）的相互转换。\n错误处理 Rust语言使用 Result（Rust语言中的数据类型，表示函数执行结果）进行错误处理和传递，错误类型可以自定义。在 src/memory/lib.rs 文件中定义了 memory 子模块相关的错误类型。通过为 Error 枚举类型实现 std::fmt::Display trait，可以自定义每种错误发生时的输出信息。通过定义 Result\u0026lt;T\u0026gt; 的别名，在本模块或者其他模块中可以直接通过 use crate::memory::Result 引入并使用该 Result 类型。代码如下：\nStratoVirt-mini_stratovirt_edu/src/memory/mod.rs\n4.4 CPU模型 在之前的章节中学习了如何使用 kvm_ioctls 来调用系统的KVM接口，以完成硬件辅助虚拟化的基本功能，实践了如何使用 Rust 语言对虚拟机内存子系统进行设计和抽象。在这一节的实验中，将继续构建 StratoVirt-mini 虚拟化程序，对虚拟机的 CPU 进行进一步的设计和抽象。\n在正式开始之前，先在项目的 src 目录下创建一个名为 cpu 的文件夹，文件夹下创建一个名为 mod.rs 的文件，本节的主要编程将在 mod.rs文件中进行。\nCPU基本结构的抽象 首先要先确定 CPU 子模块的功能界限。对于物理机来说，CPU的功能主要是解释计算机指令以及处理计算机软件中的数据；而在虚拟化程序中，CPU应该被抽象为两个基本功能：\n完成对计算机指令的模拟 处理一定的寄存器数据 计算机指令模拟的部分主要在内核 KVM 模块中进行处理，在虚拟化程序的 CPU 模块中，主要负责对 VM-Exit 退出事件的处理，也就是上一节中对vcpu_fd.run(...) 事件的处理，这部分代码将会全部封装在CPU模块中。\n同时在虚拟化程序的CPU子模块中，还需要进行一定寄存器数据的处理。为了便于操作，这些寄存器相关的数据，将被直接保存在抽象出的CPU数据结构中，可以通过KVM提供的相关接口，完成内核KVM模块中模拟的vCPU和Hypervisor中CPU结构寄存器信息的同步。\n沿着上面的思路，可以简单抽象出 CPU 的基本数据结构，代码如下：\nStratoVirt-mini_stratovirt_edu/src/cpu/mod.rs\n虽然目前CPU中的成员还比较少，但已经基本可以把CPU数据结构中的成员分为三类，之后对CPU结构的一切扩展都是围绕着这三类进行的：\n**该vCPU本身的相关信息：**如该vCPU的 ID 号等； **与内核KVM模块进行交互的接口：**如该vCPU的 VcpuFd，通过这个抽象出的文件描述符，可以直接调用到内核KVM模块所提供的 vCPU 相关接口； **寄存器的相关信息：**如目前保存的通用寄存器和段寄存器的相关信息，可以根据运行程序的需要，对这些寄存器的信息自由进行修改。 vCPU初始化 接下来为抽象出的 CPU 结构添加成员函数。首先需要添加一个构造函数 new 对CPU结构进行初始化，代码如下：\nStratoVirt-mini_stratovirt_edu/src/cpu/mod.rs\n在Rust语言的习惯中，new 一般意味着对数据结构的直接创建，作为单纯的构造函数而不包含别的逻辑，所以直接传入已经初始化完成的 VcpuFd 和 vcpu_id。new 函数运行后，可以直接获得一个初始化完成的 CPU 数据结构。\n此时对于该 CPU 数据结构而言，已经有了 vCPU 的唯一标识符 — vcpu_id，和内核中KVM模块的接口 VcpuFd，但是寄存器状态还都是初始值，所以还需要另一个函数来完成与KVM模块中寄存器数值的同步，代码如下。\nStratoVirt-mini_stratovirt_edu/src/cpu/mod.rs\nvCPU同步 简单将设备的生命周期中设备正式启动前的流程分为两个阶段：\n第一个阶段是初始化阶段(new)，包含最基本的数据结构的创建； 第二个阶段是使能阶段(realize)，包含对设备状态的使能； 在CPU结构中，设备状态的使能主要包含CPU中寄存器信息的设置。在 realize 函数中，将获取内核KVM模块中的寄存器数值，并同步到CPU结构的寄存器中。同步完成后，在此基础上对CPU寄存器进行应用程序运行所需要的修改，以前文那段汇编程序为例，需要对通用寄存器和段寄存器中的一些值进行相关设置，代码如下。\nStratoVirt-mini_stratovirt_edu/src/main.rs\n为了成功运行写入内存的汇编代码，对寄存器进行如下设置：\n对于通用寄存器 regs 而言： 会将地址寄存器 RIP 设置为该汇编代码在内存中的客户机物理地址，这样，CPU就会沿着地址寄存器设置的地址开始执行指令； 标志寄存器 RFLAGS 用于指示处理器状态并控制其操作，标志寄存器一共 32 位，其中第 2 位必须为 1，其他位均不需要设置，所以将标志寄存器设置为 2； RAX 和 RBX 是两个数据寄存器，在汇编代码中将会把 RAX 的值和 RBX 的值进行加法计算，这两个寄存器设置的值将会和程序最后的输出结果直接相关，这里设置成 2 和 3，输出结果就将是 5； 对于段寄存器 sregs 而言，需要设置它的代码段寄存器，该寄存器和运行代码的寻址相关，此处用最简单的寻址方式即可，base 和 selector 均设置为0。 在设置完 CPU 实例中寄存器的数值之后，还需要将设置完成的寄存器数值同步回内核的 KVM 模块，代码如下：\nStratoVirt-mini_stratovirt_edu/src/cpu/mod.rs\n该函数将 CPU 模块中修改的寄存器值重新设置到了内核KVM模块中，这样在虚拟机vCPU正式运行之前，所有的准备都完成了。（严格来说，CPU的使能步骤应该包括：获取kvm_vcpu中的寄存器值、修改kvm_vcpu中的寄存器值和设置kvm_vcpu中的寄存器值三个过程，为了便于说明，此处分为三个步骤来完成）。\nvCPU运行 下一步就能正式运行抽象出的CPU了，运行中最主要的部分还是内核KVM模块中vCPU的指令模拟和对陷出事件的处理，代码如下：\nStratoVirt-mini_stratovirt_edu/src/cpu/mod.rs\n将CPU模块抽象完成后，将 kvm_vcpu_exec 函数整合进 main.rs 中运行，可以得到如下和 4.2 小节末尾相同的结果：\nTerminal\n至此，成功把CPU最基本的功能封装到了CPU子模块中。\nCPU并发运行多个任务 在完成CPU基本功能的抽象后，继续对比虚拟机CPU和物理机CPU会发现，在实际的物理机中，一般不只有一个CPU，常常会有一个以上CPU的场景，这些CPU可以独立地运行程序指令，它们可以运行不同的程序，也可以运行同一个程序，利用并行计算能力加快程序运行的速度。\n为了充分发挥硬件本身的计算能力，需要添加对多CPU并行任务的支持。在新的CPU运行模型中，CPU的指令模拟和对陷出事件的处理将不在主线程中进行，每个vCPU都会对应一个单独的线程，通过分时复用的方式共享物理CPU。\nRust中同样也对多线程并发编程提供了很好的支持，主要包括 std::thread 和 std::sync 两个基本模块：\nthread 模块中定义了管理线程的各种函数； sync 模块中则定义了并发编程中常用的锁、条件变量和屏障。 将 CPU 创建线程并运行指令模拟和处理陷入陷出事件的操作封装起来，代码如下：\nStratoVirt-mini_stratovirt_edu/src/cpu/mod.rs\nStratoVirt-mini_stratovirt_edu/src/main.rs\n修改完成后尝试通过 cargo run 运行却失败了，发现如下报错信息。\nTerminal\n这是Rust编程中一个常见的静态生命周期检查失败的错误，原因是在创建进程时使用了一个闭包。闭包在一般情况下会按引用来捕获变量，因为添加了 move 关键字，会将 \u0026amp;self 的所有权转移到闭包中，而 \u0026amp;self 只是一个临时借用，无法通过生命周期检查，所以要将传入函数的参数 \u0026amp;self 改为 self 才能通过检查。确定了问题后，对该函数进行如下修改。\nStratoVirt-mini_stratovirt_edu/src/cpu/mod.rs\n编译通过，程序成功执行。但是这样改动也会带来一个问题：CPU实例的所有权将完全转移到自己的CPU线程中，在主线程将再也无法获取到CPU实例的所有权，无法对它进行任何查询和访问。这对之后查询和管理CPU信息是极为不利的，\n例如想在CPU开始指令模拟后得到CPU对应的ID信息，直接获取CPU实例的 id 号后，将会得到如下报错：value borrowed here after move。该报错意思是该CPU实例的所有权已经被转移到了CPU线程中，在主线程中因为没有获取到该CPU的所有权，将再无法获取实例的任何信息。那么有没有一种方法可以安全地在主线程和CPU线程中共享CPU实例呢？Rust提供了一种很强大的线程安全同步机制：Arc \u0026lt;T\u0026gt;。\nArc \u0026lt;T\u0026gt; 意思是多线程引用计数指针，是一个线程安全的类型，允许它被传递和共享给别的线程，可以直接通过 clone 方法来共享所有权，此时的 clone 方法并不是深复制，只是简单地共享所有权的计数。下面再次对分离CPU线程的代码进行如下修改：\nStratoVirt-mini_stratovirt_edu/src/cpu/mod.rs\nStratoVirt-mini_stratovirt_edu/src/main.rs\n同时将前面的 reset 步骤也加入CPU线程中来并发进行，以减少主线程中的时间损耗，提升程序运行的效率。\n当实现了多个CPU线程同时运行的方式后，可以支持用多个CPU来执行不同的任务。我们可以尝试同时运行两段代码，每段代码由各自的CPU线程并发运行。在上一节汇编代码的基础上添加第二段汇编代码，简单地对 0x8000 地址进行mmio读写后，该vCPU执行 HLT 指令。代码如下：\nStratoVirt-mini_stratovirt_edu/src/main.rs\n修改程序让两个vCPU运行各自的汇编代码，成功运行后将得到以下输出。\nTerminal\n以上就是一个简单的虚拟机CPU模型的全部内容，介绍了CPU模块的设计思路和功能抽象，以及在StratoVirt程序中CPU线程模型的简单实现。下面将不再运行简单的汇编小程序，而是通过对 BootLoader 模块的实现来启动一个完整的 Linux 标准内核。\n4.5 BootLoader实现 前文中已经实现了对CPU模型的抽象，可以通过启动更多的vCPU线程来并行执行更多的任务。那么是否可以通过它来运行更复杂的程序，而不只是简单的汇编代码，比如一个完整的Linux内核？下面将逐步构建启动引导模块BootLoader，直到能启动一个完整的Linux内核。\n内核文件读入内存 有了之前章节中运行汇编代码的经验，可以简单总结出通过KVM模块模拟出的vCPU和虚拟机内存来运行代码的方法：\n将要运行的代码读进虚拟机内存； 设置vCPU中的相关寄存器支持代码运行。 首先获取一个标准PE格式的Linux内核镜像 vmlinux.bin，通过Rust中的文件操作打开内核镜像并读入内存中。代码如下：\nStratoVirt-mini_stratovirt_edu/src/main.rs\nRust标准库中有对文件操作的封装，封装了打开、关闭、获取文件元数据信息等一系列文件操作，通过引入 std::fs 进行使用。\n上面这段代码中使用了 fs 的两个函数，分别是打开内核镜像文件和获取镜像的大小，通过 fs::File::open 函数打开的文件为 File 类型，该类型默认直接实现了标准库中的 Read trait，所以可以直接被虚拟机内存模块的GuestMemory 的 write 函数调用写入虚拟机内存中，通过调用该函数可以把整个内核镜像写入虚拟机内存中0x1000 处，并将 vCPU0 通用寄存器中地址寄存器的值改为内核代码的起始地址 0x1000，开始运行。\n运行后却发现屏幕上没有任何输出，这代表内核镜像在启动后没有任何的 VM-Exit 陷出，这显然并不符合Linux内核正常启动的情形，到底是哪一步出了问题呢？\nLinux内核引导流程 要想回答这个问题，需要先简单梳理一下由 Linux 启动协议规定的内核在物理机 Intel x86_64 平台上的引导-启动流程：\n当硬件电源打开后，8086结构的CPU会自动进入实模式，此时仅能访问1MB的内存，并且会加载BIOS到0xffff0到0xfffff的位置； CPU自动从0xffff0开始执行代码运行BIOS，BIOS将会执行某些硬件检测，初始化某些硬件相关的重要信息，并从物理地址0处开始初始化中断向量； 在BIOS执行完成后，该区域已经填充了内核头部所需要的所有参数，常见的BIOS执行完成后实模式低地址位1MB内存布局如下表所示： BIOS执行完成后就到了跳转至内核的入口点，此时内核将会执行两个函数：go_to_protected_mode 和protected_mode_jump：\n前者会将CPU设置为保护模式，在实模式下，处理器的中断向量表总是保存在物理地址0处，但是在保护模式下，中断向量表将存储在CPU寄存器 IDTR 中，同时两种模式的内存寻址模式也是不同的，保护模式需要使用位于CPU中 GDTR 寄存器中的全局描述符表，所以在进入保护模式前还需要调用 setup_idt 和 setup_gdt 函数来安装临时中断描述符表和全局描述符表。 完成后调用 protected_mode_jump 函数正式进入保护模式，该函数将设置CPU CR0寄存器中的 PE 位来启用保护模式，此时最多可以处理4GB的内存。之后将会跳转到32位内核入口点 startup_32，正式进行Linux内核的启动流程，引导完成。 了解了Linux内核的基本引导流程后，就可以知道之前内核启动失败的原因了：\nStratoVirt-mini 在执行内核时缺少了引导的步骤，也就是类似于物理机启动流程中BIOS功能的模块，没有引导Linux内核在实模式下配置内核头部所需要的参数，也没有进行一些重要寄存器的设置，Linux内核当然无法启动。\n接下来需要设计并实现一个 BootLoader 模块，引导标准PE格式的 Linux 内核启动，实现将内核镜像加载进内存并根据启动协议设置启动所必需的内存布局，并设置相应的CPU寄存器，以跳过实模式直接进入保护模式启动Linux内核。\n在正式编码前，先在项目的 src 目录下创建一个名为 boot_loader 的文件夹作为模块目录，本节的主要编码将在该目录下进行。\n首先根据Linux x86启动协议来设计 boot_loader 的内存布局，详见下表所示：\n内存布局中1MB以下的部分主要还是内核启动的相关配置，对照BIOS执行后的1MB内存布局做了相当多的简化； 1MB以上的部分开始读入内核保护模式的入口代码； 在内存末尾将存入一个简易文件系统 initrd，作为内核启动后的内存文件系统来使用； 配置Zero Page Zero Page（零页）是32位内核启动参数的一部分，用来存放内核启动的各种配置和硬件信息。零页也被称作Boot Params，它包含了很多配置结构体，在这些结构体中，除了配置一些精简的硬件信息外，还有两个结构体是需要特别处理的，即 RealModeKernelHeader 和 E820Entry，代码如下：\nStratoVirt-mini_stratovirt_edu/src/boot_loader/zeropage.rs\nKernelHeader 作为实模式下内核镜像的文件头，包含了许多内核的配置信息，其中有几项是需要特别设置的，如下表所示：\n接下来还需要进行 e820 表的配置。在物理机上，e820 是一个可以探测硬件内存分布的硬件，BIOS一般会通过0x15 中断与之通信来获取硬件内存布局，在 boot_loader 中不需要再通过这个流程来获取布局，直接根据对boot_loader 内存布局中准备好的值来进行 e820 表的配置。\ne820 表中的每一项都是一个 E820Entry 数据结构，表示一段内存空间，包含了起始地址、结束地址和类型。这里将根据整个虚拟机的内存布局来进行 e820 表的配置，代码如下。\nStratoVirt-mini_stratovirt_edu/src/boot_loader/zeropage.rs\n在对 e820 表的配置中，要把虚拟机的内存进行分类，此处的分类较为简单，仅分为两类：\n一类是可以作为内存使用的 E820_RAM； 还有一类是已经保留给特定功能使用的内存 E820_RESERVED； 低地址段的内存布局和前面的内存布局保持一致，高地址段的内存要注意的地方是在 x86_64 架构虚拟机的内存布局中，一般会存在一个内存空洞，在配置 e820 表时，需要根据虚拟机的内存布局跳过内存空洞进行配置。\n完成对零页的配置后，将调用GuestMemory的 write_object 接口将 BootParams 数据结构整个写入内存中。为了能成功地将这些数据结构写入内存，需要对上述所有的数据结构都实现 ByteCode Trait，写入地址为预设的0x7000。\n配置MPtable Linux内核通过零页获取到各种硬件、内存信息和配置项之后，还需要通过某种方式来**获取处理器和中断控制器的信息。**目前共有两种用来获取处理器信息的方式：\n是Intel x86平台的MP Spec（MultiProcessor Specification，多处理器技术规范）约定的方式； 另一种是ACPI的MADT表（Multiple APIC Description Table，多个高级可编程中断控制器描述表）约定的方式。 这里选择比较容易实现的MP Spec的方式。\nMP Spec的核心数据结构主要包含两个部分：MPF（MP Floating Pointer，多处理器浮点数指针）和MP Table（多处理器结构表），如图6-12所示。\n例如可以按如下代码直接初始化MPF：\nStratoVirt-mini_stratovirt_edu/src/boot_loader/mptable.rs\n其中关键字 MP 是固定的，用来搜索 MP 结构表，length表示整个结构的长度，以16字节为单位。spec规定版本号固定为1.4版本，除了传入的pointer外，其余项均置为0。成员pointer将直接指向存放MP Table的内存地址。\nMP Table结构用来真实反映处理器的硬件信息，它由一个Header（表头）和若干个Entry组成，表头信息内容如下表所示：\n这一步仅输入基本的硬件厂商相关信息［如OEM_ID（Original Equipment Manufacturer，原始设备制造商ID）、PRODUCT_ID（产品ID）］，支持自定义，以及固定的SPEC版本号、SIGNATURE字符。BASE_TABLE_LENGTH 以及 CHECKSUM 两项将在MP Table整个处理完毕后填入，其余项在 BootLoader 中不用实现，置为0即可。\n紧跟着MP Table表头，是一串Entry结构，每个Entry都代表一个单独的与处理器相关的硬件信息，每种硬件类型都有其特定的Entry结构，它们对应的Entry Type号和长度见下表。\n每个种类的 Entry 都有各自的数据结构，这里给出一份它们的Rust实现，代码如下：\nStratoVirt-mini_stratovirt_edu/src/boot_loader/mptable.rs\n为了成功启动Linux内核，这 5 种Entry都是必需的，对于每个CPU，都必须写入一次 ProcessEntry，代码如下。\nStratoVirt-mini_stratovirt_edu/src/boot_loader/mptable.rs\n其余Entry的配置方法大同小异，此处不再额外说明，读者自行查阅相关SPEC手册和示例代码来了解各项数值的配置。\n将全部的MP浮点指针 (MP Floating Pointer) 和MP表 (MP Table) 都写入内存地址 0x0009fc00 后，mptable 部分的配置全部完成。此时内核已经能获取启动所必需的硬件信息、配置信息和处理器相关信息。\n配置Initrd、Kernel和内核命令行 相关配置全部完成后，还需要将一些启动所必需的资源读入内存，这里主要有三种资源需要被读入内存：\nInitrd：用来作为内存文件系统，读入内存位置在内存末端； Kernel：PE格式的Linux内核镜像 vmlinux.bin 可以全部读入内存中，读入内存位置为 0x00100000； 内核命令行：Kernel启动额外的配置项作为输入，以字符串的形式传入，读入内存位置为 0x00020000。 内核命令行作为内核启动最后的入参，用来控制一些较为上层的行为，如内核的启动盘、输出设备等，这里给出一个较为简易的支持 Initrd 启动的配置：\n1 console=ttyS0 panic=1 reboot=k root=/dev/ram rdinit=/bin/sh 其中：\nconsole 用来标识该内核的输出设备； panic 和 reboot 定义了内核处理panic和reboot行为的模式； root 定义了内核的启动盘，这里使用内存文件系统来启动； rdinit 定义了内核启动完成后执行的第一个进程的路径。 注：\n在x86架构下，0x3f8和ttys0都与串口通信相关。\n0x3f8：\n0x3f8 是一个十六进制数，表示计算机中串口UART（通用异步收发器）的基地址； 它是串口通信的I/O端口地址，用于与串口进行数据交换； 0x3f8对应的是第一个串口（COM1）。 ttys0：\nttys0 是Linux操作系统中对应于第一个串口（COM1）的设备文件名； 在Linux系统中，串口设备通常以 ttys* 的形式命名，其中 * 代表一个数字（如ttys0、ttys1等）； ttys0 对应的是第一个串口（COM1）设备文件。 两者的关系：\n0x3f8是硬件层面上的串口I/O端口地址，用于访问串口硬件进行数据收发； ttys0是Linux操作系统中的设备文件名，用于通过文件系统接口访问串口设备； 在Linux系统中，可以通过打开和操作ttys0设备文件来进行对串口的读写操作，实现串口通信。其中，ttys0设备文件与0x3f8对应的串口硬件相连接。 综上，0x3f8和ttys0可以看作是硬件和软件之间的接口，用于在x86架构下实现与串口相关的数据通信。\n在x86架构下，对0x3f8发送字符会将字符发送到串口设备（如COM1）而不是直接显示在终端设备上。\n终端设备通常是指计算机上的显示器和键盘。串口设备是一种通过串行通信接口进行数据传输的设备，一般用于与外部设备进行数据交换，比如串口连接的终端设备（如终端机或串口控制台）。\n当向0x3f8写入字符时，字符会被发送到串口设备的发送缓冲区中。然后，通过串口的物理连接（如串口线）将数据发送到终端设备（如终端机或串口控制台）。在终端设备上，接收器将串行数据流转换为并行数据，并根据数据的ASCII码值将字符显示在终端上。\n所以，通过向0x3f8发送字符，可以将字符发送到串口设备，然后通过串口连接的终端设备上显示出来。请注意，确保终端设备的设置正确，并且串口通信的参数（如波特率、数据位、奇偶校验位等）与终端设备的设置匹配，以确保正确的数据传输和显示。\n进入保护模式 在完成所有的配置后，基本已经做完了一遍内核启动流程中实模式所做的工作，接下来就是让CPU从实模式进入保护模式的步骤了。\n在实模式和保护模式下，CPU寻址方式是不一样的：\n**在实模式下，**内存被划分为不同的段，每个段的大小为64KB，这样的段地址可以用16位来标识，内存段的处理通过和段寄存器关联的内部机制来进行，即将段寄存器本身的值作为物理地址的一部分，此时：物理地址 = 左移4位的段地址 + 偏移地址 ；\n**而在保护模式下，**为了能控制更多的内存，内存段被一系列称为描述符表的结构所定义，段寄存器由直接存储地址变为了存储指向这些表的指针。这些描述符表中，最重要就是 GDT（Global Descriptor Table，全局描述符表）。GDT 是一个段描述符数组，包含了所有应用程序都可以使用的基本描述符。GDT 必须是存在且唯一的，它的初始化一般是在实模式中由内核来完成的。想要跳过实模式到保护模式，必须完成对GDT 的配置。\nGDT 表的每一项由属性（flags，12位）、基地址（base，32位）和段界限（limit，20位）组成，共64位，可以通过64位数把它表示出来，相关代码如下：\nStratoVirt-mini_stratovirt_edu/src/boot_loader/gdt.rs\n在整个虚拟机内存中，GDT 表只有一张，并且可以存放在内存的任何位置。但是对CPU来说，它必须知道 GDT 表的位置，因此CPU中存在一个特定寄存器 GDTR 用来存放GDT的入口位置。之后在CPU进入保护模式后，它就能根据该寄存器中的值访问 GDT。根据对内存布局的规划，这里将GDT 表存放在 0x520 处，将用于中断的 IDT 表存放在 0x500 处。但此时 IDT 表的内容并不重要，可以将 64 位全部置 0。\n设置完GDT表后，需要根据GDT表的入口地址和内容来设置CPU的寄存器，这里主要设置 GDTR 寄存器的信息，相关代码如下：\nStratoVirt-mini_stratovirt_edu/src/boot_loader/gdt.rs\n根据 GDT 表的内容生成 GDTR 寄存器中代码段和数据段的内容，这些寄存器段信息包装好后将传递到CPU模块中，完成相关寄存器的设置。除了GDTR 寄存器外，针对内存布局，还有一些额外的信息需要传递到CPU模块中，需要将这些信息封装起来，作为 boot_loader 整个模块的输出，供别的模块（如CPU模块）使用。相关代码如下：\nStratoVirt-mini_stratovirt_edu/src/boot_loader/loader.rs\nBootLoader 结构体将记录 boot_loader 完成的部分内存布局信息和需要的寄存器设置，并传递给 CPU 模块来完成相关寄存器的设置。\n和普通汇编程序一样，这里也需要设置地址寄存器来定义程序的入口地址，同时将零页的存放地址告诉 RSI 寄存器，代码如下：\nStratoVirt-mini_stratovirt_edu/src/cpu/mod.rs\n为了成功进入保护模式，需要设置段寄存器的相关值，将GDT表的信息传递给 GDTR 寄存器和代码段寄存器，让保护模式下的 vCPU 能正确寻址，相关代码如下：\nStratoVirt-mini_stratovirt_edu/src/cpu/mod.rs\n此时 CPU 已经可以正常进入保护模式了，并设置好了低地址段的内存布局来引导内核正常运行。在正式开始运行内核前，还需要做两件事：\n**初始化更多的 vCPU 寄存器信息：**除了通用寄存器和段寄存器外，为了成功运行Linux内核，还需要初始化其他CPU寄存器，如 fpu、mp_state、lapic、msr 寄存器以及用来呈现虚拟机CPU特性的 cpuid，这部分代码为 CPU 本身硬件信息的初始化被封装在src/cpu/register.rs 中，可以直接使用。\n**通过 0x3f8 端口获取内核的输出：**在内核启动参数中设置了 console=ttyS0，此时内核在启动后，会通过 ISA_SERIAL 标准串口进行输出，该标准串口会在虚拟机退出时通过 0x3f8 串口输出字符信息，可以通过修改 IO_OUT 退出事件的处理函数来捕获这些信息，相关代码如下：\nStratoVirt-mini_stratovirt_edu/src/cpu/mod.rs\n将PE格式内核镜像 vmlinux.bin 和 initrd 文件放在代码中指定的地址，运行程序启动内核镜像，输出如下：\nTerminal\n可以发现内核成功启动，顺利进入 initrd 中 init 的步骤，但是却无法输入任何命令。这是因为对于内核来说，虚拟机只是捕获了 0x3f8 端口的输出，却没有真正实现 ISA_SERIAL 标准串口这一设备，虚拟机中的用户态程序无法识别到这个设备，也没有办法真正输入命令。\n以上就是一个简单的虚拟机 BootLoader 的全部内容。本小节介绍了Linux标准内核引导阶段的流程以及 BootLoader 模块的设计思路和简单实现，成功用简易的 Hypervisor 启动了一个标准Linux内核虚拟机。下面将更进一步，**完成标准输入输出设备的实现，**让虚拟机真正可用。\n4.6 串口实现 上文已经实现了BootLoader功能，内核与文件系统已经能够正常启动，但是发现启动信息打印非常慢。原因是当前仅将串口I/O端口 0x3f8 的内容使用 println 函数输出，而没有真正实现串口设备功能。这一小节将会详细介绍串口设备的实现。\n首先介绍串口设备使用的所有寄存器，如下表所示。\n串口设备的寄存器主要用于初始化协商和串口设备收发数据处理，下面介绍串口设备的初始化过程：\n首先，创建虚拟机内部通知后端的事件，用于通知虚拟机将内部数据输出到标准输出接口上； 其次向 KVM 注册串口设备中断号，中断用于通知虚拟机内部已有接收数据需要处理； 然后初始化串口寄存器，例如 LCR 寄存器设置数据长度为8位，LSR 寄存器设置发送数据寄存器为空且线路空闲，MSR 寄存器设置发送处理就绪、检测电话响起以及检测已连通，波特率设置为9600； 最后创建线程，监听标准输入接口（终端输入）是否有数据处理，将数据写入缓存中，通过寄存器 RBR 读操作，通知到虚拟机内部。 StratoVirt-mini_stratovirt_edu/src/device/serial.rs\n接下来介绍**寄存器的读操作实现。**偏移量0~7的值分别对应着 0x3f8~0x3ff 寄存器，各寄存器介绍如下表所示：\nStratoVirt-mini_stratovirt_edu/src/device/serial.rs\n最后介绍寄存器的写操作实现。偏移量0~7的值分别对应着 0x3f8~0x3ff 寄存器，寄存器介绍如下表所示。\nStratoVirt-mini_stratovirt_edu/src/device/serial.rs\n详细代码可切换至StratoVirt主页 mini_stratovirt_edu 分支中的串口实现。使用此 commit 节点进行编译，执行编译出来的stratovirt二进制，结果如下：\nTerminal\n此时仍有一个问题，只有在输入字符的时候，串口的输出信息才会显示出来，该问题将在下一小节中进行详细分析。\n4.7 Epoll实现 上一小节实现了串口设备，但是运行的结果却还是不如人意，只有在人工输入字符的时候，串口的输出信息才会显示出来。\n通过代码逻辑分析，创建 Serial 结构体时使用了互斥 (Mutex) 锁，当前有两类线程会同时访问 Serial 数据结构：\n一个是进行虚拟机内部串口数据的输入和输出处理的 vCPU 线程； 另一个是串行标准输入（终端输入）处理线程； 在串行标准输入处理线程中，首先会持有 Serial 数据结构的互斥锁，然后使用 std::io::stdin().lock().read_raw() 阻塞等待标准输入，此时 vCPU 线程无法获取互斥锁进行虚拟机内的输入和输出处理；当标准输入处理完数据后，才会释放互斥锁，vCPU 线程才能够获取互斥锁进行虚拟机内的输入和输出处理，这就导致只有在人工输入字符才有串口信息输出问题。\n解决这个问题核心思路是 Serial 标准输入处理线程不要处于长期阻塞等待处理的状态，导致 Serial 数据结构的互斥锁无法释放给其他线程访问。这就需要引入 Epoll 机制，只有在产生标准输入事件的情况下，才需要进行标准输入的读取操作，这样就能及时让出互斥锁资源。\n下面就介绍如何简易实现 Epoll 处理，当前使用的是 Crates.io 中 vmm_sys_util 封装箱，它已经提供了 Epoll 安全API接口 ( new/ctl,/wait )。\nEpoll框架简单封装 首先，创建Epoll管理结构体内容的对象，用于保存Epoll对象、监听事件和事件发生时的闭包处理（事件回调函数处理）。\nEpoll管理结构\nStratoVirt-mini_stratovirt_edu/src/helper/epoll.rs\n事件回调函数\nStratoVirt-mini_stratovirt_edu/src/helper/epoll.rs\n创建Epoll管理结构体内容的对象\nStratoVirt-mini_stratovirt_edu/src/helper/epoll.rs\n将监听的文件描述符加入Epoll\n其次，需要将监听的文件描述符加入Epoll中，传入的参数是上述代码块定义中的 EventNotifier 结构体，将其设置为监听事件的数据指针。代码如下：\n事件监听循环\n最后，创建一个线程进行事件监听循环处理，当监听事件发生时，获取监听事件的数据指针，调用其存储的闭包函数，进行事件函数处理，以下代码块为事件监听处理函数：\nStratoVirt-mini_stratovirt_edu/src/helper/epoll.rs\n串行设备处理集成Epoll机制 以串行设备处理为例，使用Epoll机制的如下示例代码：\nStratoVirt-mini_stratovirt_edu/src/device/serial.rs\n详细代码可切换至StratoVirt主页的 mini_stratovirt_edu 分支中的Epoll实现查看。\n重新编译运行，就顺利地解决了只有在输入字符的时候，串口的输出信息才会显示出来的问题。修改完成后，启动信息很快就输出到终端上，运行结果如下：\nTerminal\n//TODO: 4.8 鲲鹏平台支持: AArch64 6.4.1~6.4.7节实现了一个精简的VMM，并且可以在x86_64平台上成功启动客户机。本小节将通过扩展现有的模块实现，在鲲鹏服务器上启动客户机。本节扩展内容主要包括中断控制器、BootLoader、CPU模型和设备树(Device Tree)的实现。\n中断控制器 KVM提供了AArch64平台中断控制器GIC的模拟能力，因此可以直接创建VGIC（Virtual General Interrupt Controller，虚拟通用中断控制器），并配置该KVM设备的属性。因为在持续迭代过程中，StratoVirt会逐步增加新特性，MSI不可或缺，因此中断控制器模块选择GICv3版本，并添加GICv3 ITS设备。\n以下代码描述了VGIC的结构体，包含设置KVM VGIC设备属性的DeviceFd、VGIC中断分发器、VGIC中断再分发器的地址区间等。\nStratoVirt-mini_stratovirt_edu/src/device/gicv3.rs\n(1)创建GICv3中断控制器和ITS。通过第三方kvm-ioctls库，创建VGIC以及ITS对应的KVM设备。创建设备时，可以直接调用虚拟机句柄VmFd的成员方法，在传入的参数中指定需要创建的设备类型。代码如下。\nStratoVirt-mini_stratovirt_edu/src/device/gicv3.rs\n(2)属性设置与初始化。设置KVM设备属性，包含VGIC中断再分发器、ITS的地址信息以及中断数目等。中断控制器模块使用到的Group属性和Group下需要设置的属性值如下，读者可以参考VGIC v3的Linux内核文档[插图]和VGIC ITS的内核文档[插图]。\n①KVM_DEV_ARM_VGIC_GRP_ADDR（Group属性）。它包括：\n· KVM_VGIC_V3_ADDR_TYPE_DIST：VGIC中断分发器的基地址属性（该基地址为虚拟机内部物理地址空间的地址），必须64KB对齐。VGIC中断分发器的长度固定为64KB。· KVM_VGIC_V3_ADDR_TYPE_DIST：VGIC中断再分发器的基地址属性（该基地址为虚拟机内部物理地址空间的地址），必须64KB对齐。VGIC中断再分发器的长度固定为128KB。· KVM_DEV_ARM_VGIC_GRP_NR_IRQS：中断数目属性。该属性设置VGIC设备实例管理的总中断数目，最大值为1024。· KVM_VGIC_ITS_ADDR_TYPE：ITS的基地址属性，必须64KB对齐。ITS的长度固定为128KB。\n②KVM_DEV_ARM_VGIC_GRP_CTRL（Group属性）有。它：\nKVM_DEV_ARM_VGIC_CTRL_INIT：初始化VGIC设备和ITS需要设置该寄存器，请求KVM初始化VGIC设备和ITS设备。初始化设备请求，在设备属性设置完成后进行。完成初始化之后，StratoVirt中断控制器模块的初始化就全部完成。代码如下。\nStratoVirt-mini_stratovirt_edu/src/device/gicv3.rs\nBootLoader 与x86_64不同，AArch64平台上的低端地址区间提供给设备使用，客户机物理内存起始位置为1GB，如图6-13所示。AArch64平台的地址空间布局同样定义在内存子模块中。\nAArch64平台的BootLoader实现较为简单，主要包括：①将虚拟机内核和initrd镜像保存到内存中；②在虚拟机内存中为设备树预留空间，大小为64KB。虚拟机内核镜像、initrd镜像和设备树在内存中的布局如6-13所示。其中，设备树和initrd镜像存放在内存的结束位置处，内核镜像存放在内存起始位置处。\n将虚拟机内核镜像和initrd镜像保存到内存中后，将内核起始地址、initrd地址和设备树存放地址保存在AArch64BootLoader结构体中，相关代码如下。\nStratoVirt-mini_stratovirt_edu/src/boot_loader/aarch64/mod.rs\nCPU模型 AArch64平台的CPU基本结构可以复用6.4.4节中的内容，唯一的不同是寄存器数据结构的不同，这里采用了Rust中编译宏的编译技巧，将x86_64和AArch64的寄存器信息封装在不同的结构中，并添加到CPU数据结构中的同一成员处中，通过编译宏隔开。使用#[cfg(target_arch=\u0026ldquo;x86_64\u0026rdquo;)]，表示仅在x86_64的编译环境中编译下一代码块的代码，#[cfg(target_arch=\u0026ldquo;aarch64\u0026rdquo;)]则表示在AArch64平台需要编译的代码块，这样就可以通过一套代码来支持两个平台的CPU模型。\n和x86_64相同，AArch64运行内核代码前也需要设置相关vCPU寄存器的信息，其作用主要是让vCPU可以获取内核和设备树的起始地址信息，这两个值将被设置到USER_PT_REG_PC寄存器和USER_PT_REG_REGS寄存器的首位。和x86_64平台不同的是，Rust中的第三方kvm-ioctls库对AArch64平台的支持较差，只提供了最基本的获取寄存器信息和设置寄存器信息的函数。因此CPU模块对AArch64 vCPU寄存器内容进行了封装，代码如下。\nStratoVirt-mini_stratovirt_edu/src/cpu/aarch64/mod.rs\n通过封装好的内核寄存器结构，就可以直接调用VcpuFd的set_one_reg接口获取和设置指定寄存器的值。\n除了通过BootLoader得到的内核起始位置信息和设备树位置以外，CPU还需要初始化一些其他内容，用来完成CPU基本硬件状态的设置，包括MPIDR（多处理器亲和寄存器）的初始化等。读者可以通过查看StratoVirt项目mini_stratovirt_edu分支获取详细信息。\n构建设备树 6.4.5节中提到，在x86_64平台上BootLoader通过e820表将配置的虚拟机内存信息传递给客户机内核，通过MP表将处理器和中断控制器信息传递给虚拟机内核，通过命令行传递其他一些必要的配置项。与x86_64平台不同的是，AArch64平台通过设备树将硬件信息（例如内存信息、CPU信息等）传递给客户机内核。\n设备树是一种能够描述硬件信息的数据结构[插图]，该结构可以转换成字节流传递给操作系统，操作系统可以解析并获得硬件信息，进而执行一系列的初始化动作。设备树，顾名思义，为树状结构，其中存在一系列的节点，每个节点有一系列属性，并且可以存在若干子节点。\n在StratoVirt项目中，设备树的构建直接通过调用lfdt C接口完成，因此在编译StratoVirt Cargo项目时，需要将用到的库加入rustc的链接选项链表中，对应的编译命令为：cargo rustc-link-args=“-lfdt”。如果仍希望通过执行cargo build命令来编译项目，可以将链接选项写到项目默认编译配置。cargo/config文件中，设置方法可参照StratoVirt项目mini_stratovirt_edu分支。\n构建设备树使用到的C接口定义在src/helper/device_tree.rs中，代码如下。可以看到，这些函数为创建节点的最基本的函数，调用这些函数需要加Rust语言中的unsafe标志，为了保证设备树模块基本功能函数的安全性和封装性，需要对C接口函数进行封装，并达到以下目标：①C风格的参数类型，封装后应为Rust的数据结构；②保证封装函数的功能完整，错误处理严谨，确保在调用C接口的作用域内不会发生内存泄漏、越界访问等问题。\n为构建设备树，需要用到的主要接口包括：①创建空设备树；②添加子节点；③设置节点属性值。读者可按照这些需求自行实现，可参考StratoVirt项目mini_stratovirt_edu分支中的src/helper/device_tree.rs文件。\nStratoVirt-mini_stratovirt_edu/src/helper/device_tree.rs\n通常，构建好的设备树数据称为DTB（Device Tree Blob，设备树容器）。与DTB对应的是DTS（Device Tree Source，设备树源），DTS是文本格式的设备树描述，更加可视化，符合阅读习惯。利用DTC（device tree compiler，设备树编译器）工具[插图]，可以方便地在DTS、DTB两种格式之间转换。\n在src/helper/device_tree.rs文件中，提供了接口函数dump_dtb，用于将DTB保存到指定文件中。在src/main.rs中，将构建好的DTB数据保存在指定文件中。执行命令dtc-I dtb-O dts input.dtb-o output.dts，可以得到DTS文件。\n下面代码展现了DTB中的部分节点示例，其中“/”为根节点（有且仅有一个）。除根节点之外，其他节点有且只有一个父节点。节点的命名规则为“node_name@addr”，其中“node_name”为节点名字，addr为节点reg属性对应的值。节点内部包含了若干节点属性和对应的值。作为内存节点的父节点，根节点中定义了#address-cells和#size-cells，分别代表在子节点的reg属性中地址、地址范围长度所占用的下标数目。以内存节点为例，reg属性描述了节点的地址范围，在\u0026lt;0x000x40000000 0x00 0x20000000\u0026gt;中，前两个数字代表地址，后两个数字代表地址范围长度。\nStratoVirt-mini_stratovirt_edu stratovirt的部分DTS：\n在src/main.rs中添加设备树的构建之后，在宿主机上将PE格式内核镜像vmlinux.bin和initrd文件放在指定目录下（src/main.rs中指定的目录为/tmp，可修改），运行程序启动内核镜像可以得到如下输出，读者可以从日志中看到内存、中断控制器、串口等的内核日志。\nTerminal\n5 mini_stratovirt_edu简介 5.1 架构设计 mini_stratovirt_edu 的目的在于使用 Rust 语言学习从零开始开发出尽可能精简的虚拟机软件，充分利用 Linux 内核中的 KVM 模块所提供的硬件辅助虚拟化能力，实现虚拟机在多平台（ x86_64/aarch64 ）的运行。预期结果为用户可以通过串口登录虚拟机内部，并且可以执行基本的 Linux 命令。\nmini_stratovirt_edu 的主要流程如下所示：\n利用 KVM 模块接口创建虚拟机句柄；\n初始化虚拟机句柄对应的内存区域；\n初始化 vCPU；\n使用 BootLoader 加载内核镜像和加载 initrd 镜像，并且实例化 vCPU；\n创建相关的模拟设备，包括串口、GICv3；\n启动虚拟机的 vCPU。\nmini_stratovirt_edu 的实现包括了: KVM 模型实现、内存模型实现、CPU 模型实现、BootLoader 模型实现、串口设备实现以及对 aarch64 平台的额外支持。\nKVM模块 Linux 内核中的 KVM 模块提供了硬件辅助虚拟化的能力，并且提供了 CPU、内存和中断的虚拟化支持，其对外提供的接口为 ioctl 函数，具体细节可以参考内核社区的文档：kernel.org/doc/Documentation/virtual/kvm/api.txt\nRust 提供了 KVM 模块的封装库，分别是 kvm-bindings 和 kvm-ioctls 。其中 kvm-bindings 对 KVM 模块使用的结构体进行了封装，kvm-ioctls 对 KVM 模块提供的 API 进行了封装。\nsrc/main.rs 的 main 函数中展示了其基本用法：\n首先打开 /dev/kvm 的设备描述符 kvm ，随后利用该描述符创建虚拟机句柄， kvm.create_vm() ，该接口本质上是封装了 ioctl(fd, KVM_CREATE_VM, param) ； 然后固定分配 64KB 的内存给虚拟机使用，将 HVA 与 GPA 的映射关系通过虚拟机句柄 vm_fd 的 set_user_memory_region 成员方法通知 KVM； 接着使用虚拟机句柄 vm_fd 的 create_vcpu 成员函数创建 vCPU，设置 vCPU 寄存器； 最后使 vCPU 执行一段汇编代码并处理 vCPU 退出事件，这样借助 KVM API 构建出一个最小化且可运行的虚拟机。 内存模块 内存模块代码位于 src/memory ，其提供了内存管理、内存访问等功能。该模块的关键在于维护 HVA 与 GPA 之间的映射关系。\nmini_stratovirt_edu 分支中内存模块通过 HostMemMapping 结构体来实现该功能，该结构体利用 mmap 系统调用分配 Host 的虚拟内存， 并且将其与虚拟机的物理地址空间的映射关系保存下来。在 HostMemMapping 的析构函数中会调用 unmap 来释放宿主机的虚拟内存资源。\n此外，内存模块通过 GuestMemory 结构体向其他模块提供简单易用的内存访问接口，例如 read 函数和 write 函数。\nCPU模块 CPU 模块代码位于 src/cpu ， CPU 虚拟化的核心在于完成**对基本计算机指令的模拟和处理寄存器数据，**主要工作如下：\n指令模拟这一部分主要交由 KVM 模块完成，CPU 模块只需要处理相关的 VM_EXIT 陷出事件；\nvCPU 的初始化过程即调用 CPU 模块中的 new 函数，就是调用 KVM 模块的 create_vcpu 接口函数，函数返回 vCPU 的句柄，该接口实际底层封装的是 ioctl(fd, KVM_CREATE_VCPU) ；\nvCPU 的实例化会调用 realize 函数，该函数的主要内容就是根据 BootLoader 模块所给的数据信息设置相关寄存器，具体步骤分为三步：\n获取 vCPU 句柄中的寄存器值； 修改 vCPU 句柄中的寄存器值； 回写 vCPU 句柄中的寄存器值。 启动 CPU，即调用 CPU 模块中的 start 函数。该函数的主要内容就是创建一个用户态线程，线程内包含一个循环 loop ，循环之中运行 cpu.kvm_vcpu_exec 函数用于处理 KVM 的陷出事件。\nBootLoader模块 BootLoader 模块代码位于 src/boot_loader ，该模块负责**操作系统内核的启动引导，**主要需要实现三件事：\n按照 Linux Boot Procotol 设计 1 MB低地址虚拟机物理内存布局，根据启动所需的信息配置填充相应的数据结构； 将内核镜像文件和 initrd 写入虚拟机内存； 将写入内存的数据信息传递给 CPU 模块用于寄存器设置。 BootLoader 模块的工作流程如下：\n首先在 src/main.rs 的 main 函数按照不同的处理器架构选择对应的 load_boot_source 函数实现； 随后按照所对应的启动协议进行相关配置，调用 BootLoader 模块的 load_kernel 函数； 该函数将所需的数据结构和内核镜像文件写入虚拟机内存当中； 最后将之前配置的相关数据信息返回并且作为参数传递给 vCPU 实例化函数。 设备模块 设备模块代码位于 src/device 路径下，设备对外暴露的接口一般包括 new 函数、realize 函数、read 函数和 write 函数。\nBootLoader 模块可以让虚拟机完成内核的引导启动，但是想要和虚拟机交互并且让其执行基本 Linux 命令，还需要串口设备。串口设备用于输入输出。\n串口设备有众多的寄存器，这些寄存器主要用于初始化协商和收发数据处理。初始化过程主要包括：\n创建虚拟机向后端发送通知的事件 向 KVM 模块注册中断号用于后端向虚拟机发送中断 最后初始化相关寄存器 数据收发则是通过调用 read 函数和 write 函数实现。\n//TODO: AArch64架构额外支持 由于 aarch64 架构和 x86_64 架构的差异，需要对 aarch64 架构做一些额外支持，主要包括中断控制器、 BootLoader 模块、CPU 模型以及设备树 ( Device Tree )。\nKVM 提供了 aarch64 平台中断控制器 GIC 的模拟能力，因此可以直接利用 KVM 接口创建 vGIC v3设备，并且配置该设备的相关属性。\naarch64 的 BootLoader 设计相对 x86_64更简单，只需要将 Device Tree 、内核镜像和 initrd 文件写入内存，并且将三者的地址信息返回并提供给 CPU 模块和 Device Tree 使用。\naarch64 平台通过 Device Tree 来向内核传递硬件信息。 Device Tree 是一种能够描述硬件信息的数据结构，该结构可以转换成字节流传递给内核。内核可以解析该字节流，获得必要的硬件信息并执行一系列的初始化动作。\n对于 CPU 模块，aarch64 架构与 x86_64 架构相同，在运行内核代码前也需要设置一定的 vCPU 寄存器信息，其作用主要是让 vCPU 知道内核的起始地址和 Device Tree 的存放地址。由于 Rust kvm-ioctls 库对 aarch64 平台的支持较差，大部分 CPU 寄存器的信息在 CPU 模块中进行了手动封装。\n总结 在以上模块的支持下，mini_stratovirt_edu 实现了一个极其精简的虚拟机，并且可以实现与用户的简单交互，可以执行基础的 Linux 命令，达到了预期结果。\n5.2 使用指南 软硬件要求 最低硬件要求\n处理器架构：目前仅支持 aarch64 和 x86_64 处理器架构。aarch64 需要 ARMv8 及更高版本且支持虚拟化扩展；x86_64 支持 VT-x； 2 核 CPU； 4 GiB 内存； 16 GiB 可用磁盘空间。 软件要求\n操作系统: openEuler-20.03-LTS 及 openEuler 更高版本；\nrust 语言支持：安装 rust 语言以及 cargo 包管理器。\n具体安装过程 下载 mini_stratovirt_edu 源代码，命令如下：\n1 2 3 git clone https://gitee.com/openeuler/stratovirt.git cd stratovirt git checkout -b mini_stratovirt_edu remotes/origin/mini_stratovirt_edu 编译 mini_stratovirt_edu 源代码，具体命令如下所示：\n1 cargo build --release 编译成功之后，可以在 target/release/stratovirt 路径下找到生成的二进制文件。\n为了成功运行 mini_stratovirt_edu， 需要准备：\nPE 格式的 Linux 内核镜像\next4 文件系统，initrd 的文件系统镜像\nLinux 内核镜像下载地址：https://repo.openeuler.org/openEuler-21.03/stratovirt_img/\ninitrd 文件系统镜像制作方法：docs/mk_initrd.ch.md · openEuler/stratovirt - 码云 - 开源中国 (gitee.com)\n用 QEMU/Spike+KVM 运行 RISC-V Host/Guest Linux - 泰晓科技 (tinylab.org)\n可以根据处理器架构选择对应的内核镜像下载链接和使用制作方法生成的 initrd 镜像 ( initrd.img )，需要将两个镜像文件放置到指定路径 /tmp/ ，整个过程的具体命令编写为脚本，如下所示：\n1 2 3 4 5 6 7 8 9 arch=`uname -m` if [ ${arch} = \u0026#34;x86_64\u0026#34; ]; then wget https://repo.openeuler.org/openEuler-21.03/stratovirt_img/x86_64/vmlinux.bin mv vmlinux.bin /tmp/vmlinux.bin elif [ ${arch} = \u0026#34;aarch64\u0026#34; ]; then wget https://repo.openeuler.org/openEuler-21.03/stratovirt_img/aarch64/vmlinux.bin mv vmlinux.bin /tmp/vmlinux.bin fi 运行该脚本之后， /tmp/ 就多了 vmlinux.bin 文件；同时将制作的 initrd.img 也放入 /tmp/ ，如下所示：\n1 2 3 4 ls /tmp initrd.img vmlinux.bin # ... 将下载下来的两个镜像文件放到指定位置之后，就可以运行 mini_stratovirt_edu 的二进制程序了，具体命令如下所示：\n1 2 cd stratovirt ./target/release/stratovirt 至此，就完成了虚拟机的启动，并进入虚拟机环境中。\n6 总结 本章主要介绍了从零开始构建虚拟化平台StratoVirt的完整流程。首先，简要介绍了StratoVirt的使用场景、技术优势以及发展背景；然后介绍了StratoVirt的架构设计，主要分为CPU子系统、内存子系统、I/O子系统三部分进行阐述；最后基于硬件虚拟化技术，使用Rust语言实现了精简版的StratoVirt虚拟化软件，通过KVM模型、内存模型、CPU模型、BootLoader实现以及串口设备实现了组成虚拟化软件的最小集，可以支持x86和鲲鹏双平台的运行。\n7 //TODO：开发流程 7.1 在QEMU ARM系统上运行 由于 mini_stratovirt_edu 针对ARM系统的整体设计比较精简，先跑一下ARM架构的，构建流程如下：\n安装并编译 qemu-system-arm64； 安装交叉编译工具链 toolchain； 构建 qemu 所需的资源文件： 内核镜像 根文件系统 在qemu arm系统中构建并运行 mini_stratovirt_edu。 第一期自动搭建openEuler虚拟机运行QEMU运行环境\nQEMU启动ARM64 Linux内核_qemu-system-aarch64-CSDN博客\n7.2 依次测试下各commit 将各 commit 做成PATCH，依次测试功能\n1 2 ./configure --prefix=/opt/riscv64_gdb --with-arch=rv64imc --with-abi=xxx --enable-tui make ","date":"2024-10-17T10:07:36+08:00","permalink":"https://zcxggmu.github.io/p/kvmbook-stratovirt_5/","title":"Kvmbook Stratovirt_5"},{"content":"1 I/O虚拟化概述 CPU访问外部设备的主要接口是设备提供的I/O资源，然而一个计算机系统的物理外部设备资源是有限的。在虚拟化环境下，全部虚拟机所需要的I/O设备通常会多于硬件所能提供的I/O设备资源。\n虚拟机作为一台“虚拟”的机器，运行在虚拟机中的应用程序同样有访问外设的需求，所以如何处理虚拟机发起的I/O请求是I/O虚拟化所要解决的问题。Hypervisor需要将虚拟机中的I/O请求转换为对实际物理设备的访问。例如，当虚拟机中的应用发起对虚拟磁盘的读请求时，Hypervisor需要定位到物理磁盘的对应扇区，并将该扇区对应的数据返回给虚拟机中的应用程序。为了实现这一转换，Hypervisor会通过一系列软硬件机制截获客户机操作系统发起的PIO、MMIO和DMA等I/O访问请求，并处理虚拟机中需要执行的I/O操作，最后向客户机内的驱动程序返回正确的I/O结果。上述转换过程称为I/O虚拟化。\nI/O虚拟化在实现过程中存在一些问题，这些问题也是虚拟化技术一直以来的难点。首先I/O设备种类繁多并且异构性强，不同设备可能适用于不同的虚拟化方式，这增大了系统实现的复杂度。其次在纯软件实现的情况下，I/O虚拟化需要经过虚拟机与物理机中的两层I/O栈，导致I/O性能大幅下降，这使得I/O虚拟化成为虚拟化的性能瓶颈。\n1.1 I/O过程 I/O过程是CPU与外部设备相互访问和数据交换的渠道。外部设备会为CPU提供包括设备寄存器和设备RAM（Random Access Memory，随机存储器）在内的设备接口，CPU能够通过读写设备接口完成对设备的访问和操作。\n设备寄存器也称为“I/O端口”，通常包括控制寄存器、状态寄存器和数据寄存器三大类。\n控制寄存器用来存放CPU向设备发出的控制命令； 状态寄存器用来指示外设当前状态； 数据寄存器用于存放CPU与外设之间需要交换的数据。 在操作系统中，所有控制外部设备的操作必须由设备对应的驱动程序来完成。操作系统需要为不同设备提供相应的驱动程序，因此在操作系统的源码中，与设备驱动相关的代码占有很大的比例。**不同厂商生产的同一种设备，即使其内部逻辑电路和固件可能会有所不同，但都遵循同一种接口标准，为驱动程序提供相同的设备接口，即I/O端口。**当驱动程序访问I/O端口时，外设会根据接口标准中的要求通过设备固件实现相关功能。这种设计使得操作系统无须为每个外设厂商提供不同的驱动程序，例如：当用户更换不同厂商的鼠标时，系统中的鼠标驱动程序会自动适配该鼠标设备。\n根据数据传送过程是否需要CPU的参与，可以将I/O方式分为两类，即可编程I/O(Programmed I/O)与DMA。\nPIO 在可编程I/O方式下，外设接口需要通过一定的方式被映射到系统的地址空间才能被CPU访问。根据映射地址空间的不同，又可以将可编程I/O分为PMIO与MMIO。相较于PMIO，MMIO的应用更加广泛，几乎所有的CPU架构都支持MMIO，MMIO使用的地址空间是内存所在的物理地址空间。\nRISC（Reduced Instruction Set Computer，精简指令集计算机）的CPU（如ARM、PowerPC等）只支持MMIO，采用统一编址的方式将I/O端口编址到物理地址空间的特定区域，通过内存访问指令实现对I/O端口的访问。然而某些架构也支持将I/O端口映射到专门的I/O地址空间，例如x86架构一共有65536个8位的I/O端口，编号从0到0xffff，这样就形成了一个独立于物理地址空间的64KB的I/O地址空间。\n为了区分物理地址访问和I/O空间访问，x86架构提供了专用于端口访问的指令——IN/OUT。CPU可以通过IN/OUT指令向接口电路中的寄存器发送命令、读取状态和传送数据。当IN/OUT指令运行在内核态时，可以访问整个I/O地址空间，如果运行在CPU的低特权级，则只能根据I/O位图(I/O bitmap)的内容访问允许访问的端口。\n下图展示了非虚拟化环境下的一次端口映射I/O流程：\nDMA DMA是一种在外设与内存之间交换数据的接口技术，数据传输过程无须CPU控制。DMA实现了外设与内存之间的直接数据传输，传输速度基本取决于存储器和外设本身的速度，适用于一些高速I/O设备进行大批量数据传送的情景。\nDMA的过程由DMAC（DMA Controller，DMA控制器）控制，它是内存储器与外设之间进行高速数据传送的硬件控制电路，是一种实现直接数据传输的专用处理器。DMAC可以从CPU暂时接管地址总线的控制权，并对内存进行寻址，同时能够动态修改地址指针，完成数据在外设与内存之间的传送。\n在发起DMA访问前，设备驱动会向外设提供一组内存描述符，每个内存描述符会指定设备DMA内存区域的基地址和长度。每次DMA操作访问的是内存描述符对应的连续内存空间，但多个内存描述符代表的内存块之间并不一定连续。\n在DMA过程中，DMA控制器直接使用物理地址访问内存，并不需要线性地址到物理地址的转换过程。在虚拟化环境下，虚拟机的驱动程序提供的物理内存地址并不是实际的物理地址，这使得设备DMA操作可能会访问到不属于该客户机的物理内存空间，对虚拟化环境的安全性和隔离性造成破坏。由于DMA控制器是一个硬件控制电路，Hypervisor无法通过软件的方式截获设备的DMA操作，为了解决这一问题，Intel推出了VT-d（Virtualization Technology for Direct I/O，直接I/O虚拟化技术）技术，是IOMMU的标准实现。\n1.2 I/O虚拟化的基本任务 访问捕获 I/O虚拟化的一个基本要求是能够隔离和限制虚拟机对真实物理设备的直接访问。在虚拟机未被分配明确的物理设备时，Hypervisor不允许客户机操作系统直接与I/O设备进行交互。\n以磁盘设备为例，Hypervisor、宿主机和虚拟机会共享磁盘上的存储空间。如果客户机中的驱动程序拥有对磁盘的直接操作权限，则可能会访问到不属于该虚拟机的磁盘扇区，这样轻则导致数据泄露和丢失，严重时会威胁其他虚拟机甚至是宿主机的运行安全，无法满足虚拟化模型中的隔离性要求。为了避免这个问题，Hypervisor必须能够以某种形式截获客户机的I/O请求，防止客户机访问到不属于它的外部设备，同时使客户机保持该类设备可以被访问的错觉。\n提供I/O访问接口 上文提到，计算机使用一个外设需要操作系统的驱动程序和外设内部固件的配合。但是操作系统通常并不关心外设内部的逻辑，只需要访问外设提供给操作系统的设备接口。因此Hypervisor可以通过模拟目标设备的外部访问接口，并将模拟的软件接口提供给虚拟机，从而使客户机操作系统能够通过自身驱动程序发起对外设的I/O操作。通常虚拟机发起的I/O操作被称为“虚拟I/O”，与之对应的是宿主机操作系统发起的能够直接控制物理外设的“物理I/O”。与“物理I/O”不同的是，“虚拟I/O”操作的是Hypervisor提供的虚拟设备接口。\n实现设备的功能 在处理虚拟机发起的I/O操作过程中，Hypervisor不仅需要截获虚拟机对设备的访问，还需要实现虚拟机“期望”的设备功能，向虚拟机返回正确的结果。不同软件架构的Hypervisor实现虚拟设备功能的方式会有所不同，但基本思路都是解析虚拟机的I/O请求，并交由实际的物理设备执行，最终向客户机操作系统返回I/O操作结果。\n1.3 软件实现的I/O虚拟化 虽然在不同的虚拟化模式下，I/O虚拟化的基本任务相同，但如何实现上述基本任务存在着多种选择。例如Hypervisor可以使用软件模拟虚拟设备的接口，也可以在硬件层面将物理设备端口暴露给虚拟机。因此，与CPU虚拟化和内存虚拟化类似，I/O虚拟化实现方式也可以根据是否需要硬件支持分为纯软件I/O虚拟化和硬件辅助I/O虚拟化两种。同时纯软件实现的I/O虚拟化也可以根据客户机操作系统能否直接使用原生设备驱动进一步分为I/O全虚拟化和I/O半虚拟化。\n1 2 3 4 5 6 7 /* I/O虚拟化分类 /* 是否需要硬件支持？ */ +-\u0026gt; 纯软件I/O虚拟化 /* guest os是否直接使用原生设备驱动？ */ +-\u0026gt; I/O全虚拟化 +-\u0026gt; I/O半虚拟化 +-\u0026gt; 硬件辅助虚拟化 */ 设备模拟 (全虚拟化) 设备模拟属于纯软件实现的I/O全虚拟化方式。设备模拟的应用十分广泛，目前绝大部分Hypervisor，例如VMware Workstation、KVM、Xen、Xvisor都支持设备模拟。\n在全虚拟化环境下，为了满足虚拟机之间的隔离性和安全性等要求，虚拟机并不能直接访问真实的物理外设，虚拟机内部驱动程序操作的设备是由Hypervisor提供的一个虚拟的设备抽象，整体流程是这样：\n虚拟机启动过程中，这些由Hypervisor提供的虚拟设备会被虚拟BIOS和客户机操作系统检测到，然后挂载到虚拟的设备总线； 当客户机操作系统中对应的驱动程序向虚拟设备发起I/O请求时，由于I/O指令是敏感指令，会引发VM-Exit陷入Hypervisor。之后与这些I/O请求相关的信息会被Hypervisor截获，发送到相关的软件模块进行处理； 最后软件模块会模拟这些I/O请求并将I/O结果返回给虚拟机操作系统中的设备驱动程序。 在以上过程中，I/O请求的截获和处理对于客户机操作系统来说是透明的，客户机操作系统始终认为自己运行在一个真实的物理硬件平台，而实现设备模拟以及处理I/O请求和返回I/O操作结果的软件模块则称为设备模型 (device model)。\n设备模型主要由两部分组成：\n一部分是**以软件实现的形式向客户机操作系统提供的目标设备接口，**该接口主要用客户机操作虚拟设备； 另一部分是**设备具体功能的软件实现，**该部分会对截获的I/O请求进行解析，之后根据设备模型所处的运行环境执行相应的I/O处理过程，最终给客户机操作系统返回I/O操作结果。 由于设备模型是通过纯软件实现的，模拟的虚拟设备与宿主机的硬件并不存在直接关联，因此设备模型甚至可以模拟实际不存在的设备。设备模型可以运行在宿主机操作系统的用户态，也可以运行在Hypervisor，甚至可以运行在其他特权虚拟机。下图展示了设备模型运行在宿主机用户态和Hypervisor这两种情况。\nQEMU/KVM是把设备模型运行在用户态上这一类Hypervisor的典型代表。在QEMU/KVM中，设备模型作为用户进程运行在宿主机操作系统的用户空间。在宿主机中，设备模型可以通过使用Linux提供的系统调用以及各种库函数完成对虚拟机内I/O访问的模拟。图(a)展示了用户态设备模型的I/O模拟流程，在这种模式下，一次I/O模拟过程会发生多次上下文切换，包括：\n首先，客户机I/O操作会触发VM-Exit，发生由客户机操作系统到Hypervisor的上下文切换；然后Hypervisor将I/O操作通过I/O共享页传递给设备模型，此时会切换到用户空间的设备模拟进程；最后设备模型发起系统调用，切换到宿主机操作系统的内核态。\n与 Xvisor 类似的虚拟化方案则将设备模型整合在Hypervisor中。设备模型作为Hypervisor的一部分，与真实的设备驱动共同运行在内核态。图(b)展示了该模式下的I/O模拟流程，设备模型会解析Hypervisor拦截的客户机设备驱动发出的I/O请求，并将解析后产生的物理I/O请求发送给相应的真实物理驱动执行，最后设备模型会将物理驱动获得的I/O结果发送给客户机操作系统。相比于运行在用户态，这种实现方式避免了系统调用所产生的大量上下文切换，缩短了虚拟机I/O的模拟路径，提高了I/O的性能。但由于大量物理驱动实现在Hypervisor中，增加了Hypervisor设计的复杂性，并限制了方案的可移植性和通用性。\n半虚拟化 为了解决设备模拟中由于频繁的上下文切换导致的I/O性能大幅下降问题，I/O半虚拟化技术应运而生，其中的典型代表是Xen和virtio。下图展示了Xen前后端设备驱动模型。\n在I/O半虚拟化方式下，客户机操作系统中的原生驱动会被移除，取而代之的是简化后的前端设备驱动。Hypervisor会将原生物理驱动保留在指定的特权虚拟机(Domain 0)中，该特权虚拟机通常负责管理其他非特权虚拟机(Domain U)的生命周期，例如启动、销毁虚拟机。特权虚拟机中安装有后端设备驱动，后端设备驱动的功能是接收并解析来自非特权虚拟机中前端设备驱动的I/O请求，并将I/O请求转发到对应设备的原生物理设备驱动执行，最后将I/O操作结果返回给非特权虚拟机中的前端设备驱动。\n在半虚拟化模式中，客户机操作系统对自身所处的虚拟化环境具有清晰的认识，这样做的好处是：\n客户机能够以调用服务的形式主动向Hypervisor发起批量的异步I/O请求。相比于设备模拟中每次I/O请求都需要被Hypervisor截获，I/O半虚拟化方式极大地减少了上下文切换的开销。 而且前端驱动只需在遵守标准的前后端接口协议的基础上将I/O请求转发，无须实现原生设备驱动中复杂的逻辑，大幅简化了前端驱动的实现复杂度。 当然，I/O半虚拟化方式并不是完美的。由于需要修改操作系统源码，导致I/O半虚拟化方法难以在闭源的操作系统上应用。\n1.4 硬件辅助的I/O虚拟化 上节介绍了I/O虚拟化主要的两种软件实现方式——设备模拟和半虚拟化。设备模型为客户机操作系统提供虚拟的设备访问接口，但具体的软件实现对操作系统透明，所以使用设备模拟的方式可以运行未经修改的原生操作系统，具有较强的通用性。但是存在如下问题：\n由于I/O操作涉及大量的设备寄存器访问，在全虚拟化环境下会导致非常多的 VM-Exit，从而需要进行频繁的上下文切换； 全虚拟化需要经过客户机和宿主机的两层I/O栈，导致I/O的路径变长而且数据需要在内存中复制多次； 半虚拟化I/O方案使用前后端驱动的方式，能够批量处理虚拟机中的I/O请求，使得虚拟机能够获得与原生系统相近的I/O性能。但是使用半虚拟化I/O方式需要客户机操作系统的配合，客户机操作系统必须清楚自己运行在虚拟化环境中，并且需要安装特定的驱动程序同时修改操作系统的源码，这样就限制了I/O半虚拟化方案的通用性。 为了解决I/O虚拟化软件实现方案存在的弊端，Intel、AMD、ARM等处理器厂商推出了各自的硬件辅助I/O虚拟化技术。下图展示了设备直通访问模型与SR-IOV这两种最具代表性的硬件解决方案。\n设备直通访问 设备直通访问的实现主要依赖于设备和内存之间的IOMMU（Input-Output Memory Management Unit，输入输出内存管理单元）。IOMMU与MMU类似，位于主存和设备之间，为每个设备创建一个IOVA（I/O Virtual Address，虚拟I/O地址空间），并提供一种将IOVA动态映射到物理地址的机制。当虚拟机发起DMA时，设备驱动会使用IOVA作为DMA地址，之后设备会试图访问IOVA，这时IOMMU会将IOVA重新映射到合法的物理地址。\n在虚拟化环境中，IOMMU的引入能够在硬件层面限制设备对内存访问的范围，实现不同虚拟机之间I/O资源的隔离。IOMMU作为一种通用的解决方案，几乎所有知名处理器厂商都推出了支持IOMMU的CPU。本节选择介绍Intel提出的VT-d技术。\nVT-d技术是Intel为了提升I/O虚拟化性能提出的硬件方案，与VT-x技术同属于Intel硬件虚拟化技术。如上图所示，VT-d方案直接将物理设备分配给特定的虚拟机，使得客户机操作系统中的原生驱动程序可以通过外设接口直通访问并操作物理外设，该过程无须下陷到Hypervisor中处理。同时由于I/O路径长度几乎等同于非虚拟化环境下的I/O路径长度，因此虚拟机能够获得与裸机一致的I/O性能。\n在引入VT-d技术的CPU上，Hypervisor无须模拟客户机操作系统发起的I/O操作，这极大地降低了Hypervisor设计的复杂度。例如完全采用设备直通访问的方式为虚拟机分配物理外设资源的Jailhouse（西门子开发的Hypervisor），其代码行数只有不到4万行，而同为Type I型Hypervisor的Xen，由于同时还支持设备模型，其代码行数远超Jailhouse，约为30万行。\nSR-IOV 设备直通访问虽然能够将一个物理设备分配给虚拟机独占使用，但这种设备独占的方式降低了设备的利用率，会大幅增加设备的硬件成本，在某种程度上 “违背” 了虚拟化的初衷，并且不具备可扩展性。例如，目前市面上的网卡设备一般都拥有较高的带宽，同时正常情况下系统并不会一直使用网络功能。如果使用设备直通方式，很容易造成网卡带宽资源的浪费。\n为了解决这一问题，设备厂商**在硬件层面将一个物理设备虚拟成多个设备，然后将虚拟出来的设备分配给虚拟机，使得物理设备可以同时被多个虚拟机共享。出于兼容性考虑，PCI-SIG制定了SR-IOV规范。SR-IOV规范允许虚拟机能够在没有软件参与的情况下共享设备I/O端口的物理功能，**同时可以获得媲美非虚拟化环境I/O性能。\n2 I/O虚拟化的实现方式 与CPU和内存相比，外设的种类繁多，不同设备遵循的总线协议也有所不同。常见的设备有PCI设备、ISA设备、USB设备等。目前PCI设备是应用最广泛的设备，几乎所有主板都支持PCI设备，所以本章将基于x86架构，以PCI设备为例介绍设备模拟、半虚拟化、设备直通访问、SR-IOV这四种I/O虚拟化方式的具体实现。\n2.1 PCI设备简介 PCI总线可以被看作系统总线的延展，同时作为CPU的局部总线连接外部设备。PCI总线是一个典型的树结构：\n从图中看，与PCI总线相关的模块包括：\nHost Bridge\n比如PC中常见的North Bridge（北桥）。图中处理器、Cache、内存子系统通过Host Bridge连接到PCI上，Host Bridge管理PCI总线域，是联系处理器和PCI设备的桥梁，完成处理器与PCI设备间的数据交换。其中数据交换，包含处理器访问PCI设备的地址空间和PCI设备使用DMA机制访问主存储器，在PCI设备用DMA访问存储器时，会存在Cache一致性问题，这个也是Host Bridge设计时需要考虑的。此外，Host Bridge还可选的支持仲裁机制，热插拔等；\nPCI Local Bus\nPCI总线，由Host Bridge或者PCI-to-PCI Bridge管理，用来连接各类设备，比如声卡、网卡、IDE接口等。可以通过PCI-to-PCI Bridge来扩展PCI总线，并构成多级总线的总线树，比如图中的PCI Local Bus #0和PCI Local Bus #1两条PCI总线就构成一颗总线树，同属一个总线域；\nPCI-To-PCI Bridge\nPCI桥，用于扩展PCI总线，使采用PCI总线进行大规模系统互联成为可能，管理下游总线，并转发上下游总线之间的事务；\nPCI Device\n**PCI从设备：**被动接收来自Host Bridge或者其他PCI设备的读写请求； PCI主设备：可以通过总线仲裁获得PCI总线的使用权，主动向其他PCI设备或主存储器发起读写请求； **桥设备：**管理下游的PCI总线，并转发上下游总线之间的总线事务，包括PCI桥、PCI-to-ISA桥、PCI-to-Cardbus桥等； 一个插入在PCI插槽中的物理PCI设备可能具有多个功能单元，其中每个功能单元用功能号 (Function Number) 表示。操作系统会将某一物理PCI设备的不同逻辑设备视作多个独立的逻辑设备。本质上，CPU通过I/O访问的是PCI设备的某个逻辑设备。通过PCI总线的分层结构可以根据总线号和设备号确定某个物理PCI设备，进而一个PCI逻辑设备可以由总线号、设备号、功能号三个参数唯一确定，这三个参数就构成了PCI设备标识符。\n操作系统通过每个PCI设备的配置空间识别和访问PCI设备。PCI配置空间的大小为256字节，实际上是一组连续的设备寄存器。如图下所示，其中前64字节称为配置头，由PCI标准统一规定格式和用途。配置头的主要功能是识别设备、定义主机访问PCI卡的方式，例如设备号(Device ID)和厂商号(Vendor ID)用于表示设备的类型和生产该设备的厂商。PCI配置空间的剩余192字节由PCI设备自己定义。\nPCI配置空间有6个BAR（Base Address Registers，基地址寄存器），即PCI BAR，代表每个PCI设备最多能映射6段地址空间，编号分别为i=0，1，2，…，5。BAR记录设备所需要的地址空间的类型（内存空间或者I/O空间，用BAR的最后一位区分）、基地址以及其他属性。\nPCI设备标识符可以被视为设备PCI配置空间地址的一部分，主桥能够根据PCI设备标识符选择对应PCI设备并根据寄存器偏移地址获取设备配置空间的信息。\nPCI配置空间地址的结构如下图所示：\n其中 [23∶8] 为设备标识符，高8位总线编号 (Bus number) 字段代表设备所在的总线，中间 5 位设备编号 (Device number) 字段指示总线上的具体某个物理设备，最后 3 位功能编号 (Function number) 字段标识物理设备上的某个逻辑设备。根据每个字段的位数，系统最多可以有 256 条PCI总线，每条总线上最多有 32 个物理设备，同时每个设备最多拥有 8 个功能单元，即逻辑设备。为了方便表达，本文以 3 个字段的首字母缩写BDF 代表PCI设备标识符。\n2.2 设备模拟 由于外设的种类繁多，设备模型需要模拟每个设备拥有的不同接口和功能，并且提供多种设备访问方式，这导致设备模型的实现十分复杂。但是总体来看，虽然每个设备的接口数量以及内部逻辑有所差异，但CPU本质上都是通过读写一组设备寄存器或设备RAM实现对设备的访问，这就使不同虚拟设备的底层实现之间具有相似性。本节将从PIO、MMIO、DMA和PCI配置空间访问四个方面介绍设备模拟的实现。\nPMIO\nx86架构提供了 IN/OUT、INS/OUTS 等指令访问与设备寄存器相关的I/O端口。Hypervisor将这四条指令设定为会触发VM-Exit的敏感指令，当客户机通过发起这四条指令进行端口I/O操作时会触发VM-Exit，陷入Hypervisor。同时Hypervisor会保存访问的端口号、访问数据宽度、数据传输方向等相关信息，以达到截获客户机端口I/O操作的目的。下图展示了Hypervisor模拟虚拟机发起一次PMIO访问的过程。\nout 指令触发 VM-Exit； 模拟 out 指令并返回客户机； 设备模型在初始化阶段会将虚拟设备涉及的PMIO处理函数在Hypervisor中进行注册，并以数组的形式存储I/O处理函数的指针。Hypervisor会根据截获的端口号和访问的数据宽度与相应的PMIO处理函数进行匹配，找到相关函数的指针后，交由PIO处理函数进行进一步的模拟过程。\nMMIO\n内存映射I/O相较于端口I/O是一种应用更加广泛的I/O形式。MMIO地址空间属于物理地址空间中的高地址部分，程序可以使用内存访问指令进行MMIO。与端口I/O涉及的IN/OUT指令不同的是，内存访问指令并不属于MMIO的专属指令，Hypervisor不能将其设为敏感指令。那针对MMIO访问该如何触发VM Exit呢？\n为了截获客户机的MMIO操作，Hypervisor不会将虚拟机中MMIO所在的物理地址范围映射到主机的物理地址空间，即影子页表中不存在相应的页表项。每次当客户机操作系统发起MMIO时，都会产生一个缺页异常，产生VM-Exit，这样就可以截获客户机的MMIO访问并交由相关处理函数模拟。 MMIO相关的处理函数的组织形式通常不采用PMIO中的数组形式。这是因为MMIO占用的内存区域较大，一般有上百兆字节，如果使用数组保存每个端口的处理函数会导致巨大的内存占用。通常每个MMIO相关的处理函数能够处理对某一片内存区域发起的MMIO访问，这样就大大减少了处理函数的数量，从而降低了对内存的占用。\n设备模型首先会向Hypervisor申请MMIO区域并注册该MMIO的处理函数，之后当虚拟机发起MMIO访问时，Hypervisor会根据产生异常的内存地址找到对应的MMIO区域，最后该MMIO区域注册的处理函数会定位到要访问的I/O端口并执行相关软件模拟过程。\nDMA\n与ISA设备不同的是，PCI设备架构中并不存在DMA控制器，任何一个PCI设备都可以向PCI总线控制器发起总线请求，获得对总线的控制权。PCI设备发起DMA的过程是由操作系统中的设备驱动程序访问与DMA操作相关的特定硬件寄存器组实现的。设备驱动不仅可以通过写入相关硬件寄存器设置DMA操作的目的地址，而且可以通过向DMA命令寄存器写入DMA命令发起DMA请求。\n设备驱动程序只能以PMIO或MMIO的方式访问外设寄存器，所以Hypervisor通过截获客户机操作系统发起的PMIO和MMIO即可实现对客户机DMA请求的拦截。之后Hypervisor会执行DMA相关处理流程。首先，设备模型会借助Hypervisor的内存管理子系统，将客户机操作系统指定的DMA内存映射到自身的地址空间中。之后设备模型会执行相应的系统调用，将数据以DMA的方式读写到Hypervisor映射的DMA内存。最后当数据传输完毕，设备模型会通过虚拟中断控制器将对应的虚拟设备中断注入到虚拟机中，虚拟机结束此次DMA操作。\nPCI配置空间\nx86架构在I/O地址空间提供了两个32位寄存器，用于访问PCI设备的配置空间：\nconfig_address 寄存器（端口地址为0xcf8）用于存放目标PCI设备的设备标识符，以及要访问的设备寄存器在配置空间的字节偏移； config_data 寄存器（端口地址为0xcfc）用于存放设备寄存器的数据； CPU读取配置空间时，CPU首先将PCI配置空间地址写入 config_address，之后PCI主桥把设备号 config_address[11:15] 转译到PCI总线并通知对应PCI设备，PCI设备会把对应寄存器数据放入地址总线，之后PCI主桥读回数据，放到 config_data 中。最后CPU便可以通过 config_data访问配置空间的内容。由于访问过程涉及IN/OUT这两个端口访问指令，所以可以通过与PMIO一样的形式模拟客户机对PCI配置空间的访问。\n2.3 I/O半虚拟化 设备前后端分离是 I/O 半虚拟化的核心思想。在该方法下，虚拟机使用专门的前端驱动程序，通过特定数据传输接口与特权虚拟机或Hypervisor中的后端设备交互，后端设备则通过物理驱动程序完成对外设的访问。在I/O半虚拟化领域，一些知名的Hypervisor都推出了自己的解决方案，例如Xen提供的半虚拟化驱动、VMware提供的guest tools，本节将介绍一种被广泛使用的半虚拟化方案—— virtio。\nLinux作为使用最广泛的开源操作系统，能够被众多虚拟化方案支持，例如Xen、KVM、VMware等。虽然这些Hypervisor都支持Linux，但是在virtio出现之前，众多Hypervisor内都拥有属于自己的块设备、网络设备驱动，而且还需要维护功能重叠但实现又有所不同的设备模型，这就给日常维护以及性能优化带来了很大的麻烦。\n为了解决上述问题，IBM推出了virtio，**virtio的主要目标是为半虚拟化提供一个统一的前后端设备接口标准。**virtio将一组高效且通用的Linux前端virtio驱动加入Linux源码树中，并提供了一系列能够应用于不同Hypervisor的后端virtio设备，这样显著降低了日常维护的成本。其中：\nvirtio驱动程序是虚拟机中的软件部分，它依据virtio规范与virtio设备进行通信，主要作用是发现virtio设备并在虚拟机中分配用于前后端通信的共享内存； virtio设备会公开virtio接口，用于管理和交换信息。virtio接口一般包括设备状态(Device Status)、设备支持的特性(Virtio Feature)以及前后端数据传输的通道（Virtqueue队列）。前后端消息通知机制 (Notification) 同样也属于virtio接口的一部分，用于提醒前后端处理到来的信息。 virtio作为一个通用半虚拟化框架，可以在不同类型设备总线之上实现，本文将基于最普遍的PCI总线介绍virtio的具体实现。\nvirtio-pci 为了支持PCI总线，每种virtio设备需要对应一个 virtio-pci 代理设备。virtio-pci 代理设备能够通过与PCI设备相似的方式被虚拟机中的BIOS或客户机操作系统识别，并挂载到PCI总线。virtio-pci 代理设备的一个重要作用是提供virtio设备的访问接口，它会创建一条virtio总线，并将virtio设备挂载到virtio总线，这样virtio驱动便能够访问virtio设备。\nvirtio-pci 设备拥有专属的厂商号 0x1a4 以及特定的设备号的区间 0x1000-0x10ff。系统可以通过厂商号识别出该PCI设备为 virtio-pci 设备。子系统厂商号 (Subsystem Vendor ID) 和设备号可以用于指示该 virtio-pci 设备支持的virtio设备类型。\n系统通常会使用 virtio-pci 设备的第一个BAR指示的I/O空间对 virtio-pci 设备进行配置。该I/O区域包括一个 virtio-header 结构，用于存放virtio设备的通用配置项以及设备的专属配置。下图展示了 virtio-header 中的通用配置项：\n由于virtio设备需要挂载到 virtio-pci 设备提供的virtio总线，所以virtio设备探测与驱动加载需要在 virtio-pci 初始化完成的前提下进行。因此整个virtio前端初始化过程可以分为：virtio-pci设备探测和驱动加载和virtio设备初始化和驱动加载两个阶段。\nvirtio-pci 设备作为PCI设备的一种，可以通过与其他PCI设备一样的方式被探测。虚拟机启动时，虚拟BIOS和客户机操作系统会扫描PCI总线，进行PCI设备枚举过程。该过程会探测到 virtio-pci 设备，并将 virtio-pci 设备注册到PCI总线。之后，与设备和驱动匹配相关的match函数将根据PCI配置空间中的厂商号、设备号、子系统厂商号和子设备号为设备绑定对应驱动。\nPCI设备和驱动匹配的依据通常是驱动中的一个由pci_device_id 组成的 id_table 数组。在 virtio-pci 设备驱动中，将厂商号设置为0x1a4，而设备号、子系统厂商号、子设备号都被置为PCI_ANY_ID，表示可以匹配任何ID。这样无论是哪一种 virtio-pci 设备，都能与共用的virtio-pci 设备驱动绑定。virtio-pci 设备与 virtio-pci 设备驱动绑定后，会进入 virtio-pci 设备探测阶段。\nvirtio-pci 设备探测阶段通常使用 virtio_pci_device 结构体表示 virtio-pci 设备。该阶段会使能 virtio-pci 设备并初始化相应的virtio设备和virtio总线。\n最后virtio设备会被注册到virtio总线上，该过程会触发virtio总线上virtio设备与virtio驱动之间的匹配操作。匹配操作成功之后，virtio驱动会进行virtio设备探测过程（例如 virtioblk_probe ）。virtio设备探测的主要任务是读取 virtio-pci 配置空间中关于virtio设备配置的内容(virtio-header)，之后按照virtio配置初始化virtqueue，并根据virtio设备的特性初始化对应的物理设备。\n下图展示了 virtio 前后端架构：\n主要包含三部分，前端驱动、后端设备、virtqueue：\n前端驱动位于客户机操作系统内部，包括 virtio/virtio-pci 设备驱动，其作用是接收用户态请求，然后根据传输协议将封装后的I/O请求放入virtqueue，并向后端发送一个通知； 后端设备位于QEMU或宿主机内核中，包括 virtio/virtio-pci 设备。后端设备从virtqueue中接收前端驱动发出的I/O请求的基本信息，然后调用相关函数完成I/O操作，最后向客户机中的前端驱动发起中断； 不同virtio设备可能拥有不同数量的**virtqueue，**例如virtio块设备只有一个virtqueue，而virtio网卡设备则有两个virtqueue，一个用于发送数据另一个用于接收数据。 virtqueue **vring 是 virtqueue 机制的具体实现。**vring在虚拟机和QEMU之间引入一段共享的环形缓冲区作为前后端数据通信的载体。相较于传统的I/O方式，环形缓冲区的引入使得可以一次处理多个I/O请求，提高了每次I/O传递的数据量，并且显著减少了上下文切换的次数。\nvirtqueue 传输机制是 virtio 框架能够提升 I/O 虚拟化性能的关键。\nvring主要由三个部分组成：描述符表 (descriptor table) 、可用的描述符环 (available ring) 和已用描述符环 (used ring)。\ndescriptor table 描述符表用于保存一系列描述符，每一个描述符都被用来描述客户机内的一块内存区域。客户机中的前端驱动负责管理这些内存区域的分配和回收。描述符通过如下字段指定内存区域的各项属性：\naddr：该字段表示内存区域在客户机物理内存空间中的起始地址； flag：该字段用来标识描述符自身的特性，一共有三种可选值。 VRING_DESC_F_WRITE：表示当前内存区域是只写的，即该内存区域只能被后端设备用来向前端驱动传递数据； VRING_DESC_F_NEXT：表明该描述符的next字段是否有效； VRING_DESC_F_INDIRECT：表明该描述符是否指向一个中间描述符表； len：该字段的意义取决于该内存区域的读写属性。如果该区域是只写的，数据传递方向只能从后端设备到前端驱动，此时 len 表示设备最多可以向该内存块写入的数据长度；反之，如果该区域是只读的，此时 len 表示后端设备必须读取的来自前端驱动的数据量； next：在virtio中，一次前、后端的数据交互请求往往会包含多个I/O请求合并，而且一个I/O合并也可能涉及多个不连续的内存区域。通常的做法是将描述符组织成描述符链表的形式来表示所有的内存区域。next字段便是用来指向下一个描述符。通过flag字段中的值VRING_DESC_F_NEXT，就可以间接地确定该描述符是否为描述符链表的最后一个。 available ring 可用描述符环是 virtqueue 中的一块区域，用于**保存前端驱动提供给后端设备且后端设备可以使用的描述符。**可用描述符环由一个flags字段、idx索引字段以及一个以数组形式实现的环ring[]组成。其中：\nflags字段有0和1两种取值，1 表明后端设备使用完前端驱动分配的描述符时无须向虚拟机发送中断； idx 用来索引数组 ring 中下一个可用的位置。数组ring中存放的是描述符链表中作为链表头的描述符在描述符表中的索引，所以可以通过数组ring中元素的值找到对应描述符链表。当前端驱动为后端设备组织好一个可用描述符链表时，前端驱动会根据idx的值将该可用描述符链表头在描述符表中的索引加入数组ring的相应位置。 每次后端设备取可用描述符时，需要知道剩余可用描述符在数组ring中的起始位置。后端设备会维护一个变量 last_avail_idx，用来标记这个位置。当切换到主机中时，后端设备将检查 last_avail_idx/idx 的值，数组ring中位于 last_avail_idx ~ idx-1之间的部分就是可供后端设备使用的区域。\nused ring 已用描述符环的组成结构与可用描述符环类似，用于**保存后端驱动已经处理过并且尚未反馈给驱动的描述符。**与可用描述符环不同的是，已用描述符环中数组ring的每个元素不仅包含后端设备已经处理的描述符链表的头部描述符在描述符表中的索引，而且由于后端设备可能会向前端驱动写回数据或需要告知驱动写操作的状态，还需要包括一个len字段来记录设备写回数据的长度。\n当后端设备处理完一个来自可用描述符数组的描述符链表后，需要将链表头的描述符在描述符表中的索引以及写回数据的长度一起加入数组ring中。idx在该过程中的作用与在可用描述符环中相同。 同样的，当设备驱动回收已用的设备描述符时，需要知道剩余已用标识符在数组ring中的起始位置，前端驱动会维护一个变量 last_used_idx，用来标记这个位置。当切换到虚拟机中时，前端驱动将检查 last_used_idx/idx 的值，数组ring中位于 last_used_idx ~ idx-1 之间的部分便是可供前端驱动回收的区域。\nvirtqueue初始化 在前端驱动发起I/O操作前，驱动作为virtqueue的所有者需要初始化将要用到的virtqueue。下面是virtqueue初始化的具体过程：\n在virtio框架下，virtqueue的相关参数（例如地址和大小）都保存在上文中提到的virtio-header中。virtio-header存放在virtio-pci设备配置空间第一个BAR指向的I/O区域，所以将该区域映射进内核可以获得virtqueue相关属性； 根据后端设备种类不同，一个前端驱动可能拥有多个队列。可以将virtqueue的索引写入virtio-header中的Queue Select寄存器来通知设备所要初始化的具体队列； 为了给virtqueue分配空间，驱动还需要知道virtqueue的大小。前端驱动通过读virtio-header中的Queue Size寄存器，获得virtqueue内描述符的数量； 根据描述符数量计算并为virtqueue分配内存空间，并将内存空间的起始地址除以4096，转换成以页为单位的地址后写入virtio-header中的Queue Address寄存器； 后端设备收到该地址后，将该地址左移12位获得virtqueue的GPA，最后将该GPA转换为HVA。 virtio-net virtio能够支持各种不同的设备，其中虚拟网卡设备是virtio中比较复杂的设备。基于virtio实现的网络架构通常被称为 virtio-net。下图描述了virtio-net 的一次网络包发送过程：\n运行在客户机内核空间的 virtio-net 驱动会将要发送的数据包放置在虚拟机的内存缓冲区中； virtio-net 驱动为数据包所在的内存缓冲区创建一系列描述符，并组织成描述符链加入可用描述符环中； virtio-net 驱动以 MMIO 的方式写特定地址，造成 VM-Exit，陷入 KVM 中； KVM 分析 VM-Exit 的原因，将控制流转发给 QEMU 中的 virtio-net 后端设备； virtio-net 后端设备从可用描述符环中获得数据包所在的客户机物理地址，由于虚拟机的内存空间由QEMU分配，所以QEMU能够将客户机物理地址转换为QEMU在主机操作系统中的虚拟地址； virtio-net 后端设备根据上一步转化得到的主机虚拟地址得到网络包数据； virtio-net 后端设备以系统调用的方式将网络包通过内核网络栈发出，之后将描述符加入已用描述符环中； QEMU以中断注入的形式向虚拟机发送I/O完成通知，并最终将控制流交还给虚拟机。 vhost 上文主要介绍了后端设备位于用户态QEMU进程中的情况。在该场景下，一次virtio访问会涉及虚拟机内核与主机KVM模块之间、KVM模块与主机用户态QEMU进程之间、QEMU进程与主机内核中的驱动程序之间的多次特权级切换。每次特权级切换都需要保存上下文，造成了极大的性能损失。\n还有一种将设备模型置于Hypervisor的设备模拟方式，该方式避免了系统调用所产生的大量上下文切换，缩短了虚拟机I/O的模拟路径，提高了I/O的性能。vhost就是该方式的一种具体实现，如下图所示：\nvhost采用了与之类似的方式优化传统的virtio。vhost API是一个基于消息的协议，它允许Hypervisor将数据交互的工作卸载到运行在宿主机内核态的另一个组件—— handler。handler能够将数据转发的任务从QEMU的设备模型中解耦。使用此协议，Hypervisor (QEMU) 需要向handler发送Hypervisor的内存布局 (MemoryLayout) 和一对文件描述符 ioeventfd/irqfd。内存布局用于定位 virtqueue 队列和数据缓存在QEMU内存中的位置。文件描述符用于handler发送和接收通知。这些文件描述符在handler和KVM之间共享，因此handler可以和KVM直接通信，不需要QEMU的干预。\nvhost-net是一个内核驱动程序，是一种vhost协议处理程序，用于实现数据包的快速转发：\n加载vhost-net内核驱动程序时，会在 /dev/vhost-net 上公开一个字符设备。当QEMU在vhost-net的支持下启动时，QEMU会使用几个ioctl调用来初始化vhost-net； 在初始化过程中，QEMU进程会与vhost-net相关联，并将virtio feature、用于发送和接收通知的文件描述符、物理内存映射传递给vhost-net。vhost-net内核驱动程序会为客户机创建对应内核线程。这个线程称为 “vhost工作线程” 。 QEMU分配文件描述符 ioeventfd 并将其注册到vhost和KVM，该文件描述符负责传递I/O事件通知。当虚拟机访问特定端口时，KVM向ioeventfd 写入与该访问相关的内容，同时vhost内核线程会不断轮询 ioeventfd 并立即获得通知。因此对特定客户机内存地址（例如MMIO内存区域）的读/写操作不需要切换到QEMU中，可以直接路由到 vhost 工作线程，这样就实现了异步处理，不需要终止vCPU的执行，避免了上下文切换的开销。\n另外，QEMU分配另一个文件描述符 irqfd 并再次将其注册到KVM和vhost上，该文件描述符负责传递vCPU中断消息。vhost工作线程完成数据包读写之后，会对 irqfd 进行写操作，之后KVM从 irqfd 中读取与vCPU相关的信息，最后将vCPU中断注入客户机。该过程也是异步的，这种异步通知机制使得在数据处理过程中，vCPU可以继续运行，显著减少了性能损失。vhost的引入改变了发生VM-Exit之后KVM和QEMU的工作流程，但并不影响前端驱动的固有设计，vhost对前端驱动来说是透明的。\nvhost-user //TODO 上文的vhost-net方案比较适合客户机与主机网络控制器之间的通信或者客户机与外设之间的通信。在这种情况下，客户机与后端设备的通信只发生一次数据复制与用户态切换。但是vhost-net并不适用于客户机与主机上的某些用户态程序交互。因为如果使用vhost-net方案，客户机和KVM模块之间、用户态程序与后端设备之间都会产生上下文切换以及数据复制。由于客户机和用户态程序都位于用户态，数据可以直接在用户态进行传递，无须经过“用户态——内核态——用户态”这两次复制过程。\n一种将vhost从内核态迁移到用户态的vhost-user方案应运而生。vhost-user使用了和vhost-net一样的vhost协议，即都使用了vring完成前后端数据交互，而且还使用了相同的事件通知机制。不同的是，vhost将后端实现在内核态而vhost-user实现在用户空间中，用于用户空间中两个进程之间的通信。而且事件通知和数据交互的具体实现方式两者也有所不同。vhost-user基于客户/服务的模式，采用UNIX域套接字(UNIX domain socket)来建立QEMU进程与vhost-user之间的联系，进而将virtio feature、用于发送和接收通知的文件描述符、物理内存映射传递给vhost-user，而vhost-net中使用的是ioctl的方式。vhost-user采用套接字的方式大大简化了操作。vhost-user基于vring这套通用的共享内存通信方案，只要客户端和服务端按照vring提供的接口实现所需功能即可。常见的实现方案是将客户端集成在客户机的virtio驱动上，服务端实现在DPDK（Data Plane Development Kit，数据平面开发套件）[插图]等用于网络加速的用户态应用中\n图4-12展示了vhost-user/virtio-net-pmd架构下一次网络包的发送过程，具体过程如下。\n(1)虚拟机中与DPDK库链接的应用程序通过用户态virtio-pmd驱动发送数据包。首先将数据包写入缓冲区，并在可用的描述符环中添加它们对应的描述符。(2)主机中的vhost-user PMD（Poll Mode Driver，轮询模式驱动）会不断轮询virtqueue，因此它会立即检测到新的描述符，并进行处理。(3)对于每个描述符，vhost-user PMD通过查询设备TLB获得缓冲区的HVA。如果发生TLB未命中，将发送一个对QEMU的请求更新设备TLB。这种情况并不多见，因为DPDK使用了大页机制，TLB未命中的概率很低。(4)vhost-user PMD将缓冲区的数据复制到mbuf（DPDK应用程序使用的消息缓冲区）。缓冲区描述符被vhost-user添加到used描述符环中。这时虚拟机中同样一直在轮询virtqueue的virtio-net-pmd驱动会立即回收used描述符。(5)最后主机中的DPDK APP（应用程序）处理mbuf。DPDK绕过Linux内核网络栈，直接将数据包发送到NIC（Network Interface Controller，网络接口控制器）中，实现数据包的快速转发。\n容器作为一种进程级别的虚拟化技术在NFV（Network Functions Virtualization，网络功能虚拟化）领域具有很大的应用潜力。容器通常使用宿主机操作系统内核的网络栈来实现网络数据包转发。然而出于中断处理、网络包复制等原因，内核网络栈每秒最多只能环回1百万~2百万个数据包，并不能满足NFV在吞吐量、延迟和性能抖动等方面的要求。为了解决这一问题，绕过Linux网络栈，提升容器中的网络性能，Intel在2017年将vhost协议移植到了容器[插图]，提出了如图4-13所示的virtio-user解决方案。\nvirtio-user方案在DPDK框架中添加了一个新型的虚拟设备virtio-user，该设备有两种不同类型的网络接口。vhost-kernel接口用于连接主机内核中的vhost-net模块。vhost-user接口遵循上文提到的vhost-user规范，与运行在用户空间的虚拟机交换机连接，并通过虚拟机交换机直接与网卡交互。与QEMU/KVM不同的是，QEMU将整个内存空间布局与vhost后端共享，vhost-user只与vhost后端共享容器内存空间中由DPDK应用程序管理的区域。\n2.4 设备直通访问 前文曾提到，I/O虚拟化的一个最基本要求是能够**限制和隔离虚拟机访问不属于自身的I/O设备。**在设备直通访问中为了满足这一要求，Hypervisor需要做到：\n将外设分配给不同的虚拟机，这些设备称为该虚拟机的指定设备，虚拟机中的vCPU无法访问到其他虚拟机的设备； 指定设备的驱动程序只在拥有该设备的虚拟机中运行，并且能在不发生或发生少量VM-Exit情况下直接与设备硬件交互； 同时，一个虚拟机的指定设备只能访问属于该虚拟机的物理内存，而且必须将中断发送给虚拟机的中断控制器； 前文提到过，CPU通过PCI配置空间中的BAR获取I/O空间地址或物理地址来访问外设提供的I/O资源。一般情况下，客户机中的驱动程序使用的只是虚拟的外设I/O地址或物理地址空间（GPA），如果直接使用该地址访问设备会产生一系列问题。\n例如，当一个虚拟机中的物理驱动程序为I/O设备指定一块用于DMA操作的内存区域时，驱动使用的DMA地址并不是实际的物理内存地址，如果设备直接使用该地址可能会访问到属于其他虚拟机的物理内存，甚至是Hypervisor所在内存区域，给整个系统带来风险。设备模拟和半虚拟化等软件方案可以通过执行对应的I/O处理函数，将虚拟机中驱动程序使用的地址转换为实际的物理地址，并能够对地址的合法性进行检查。而在设备直通访问方案中，Hypervisor并不会参与I/O操作的模拟，也就无法帮助虚拟机完成地址转换过程，这就给虚拟机直接访问物理I/O设备带来了三个问题：\n第一个问题是如何保证虚拟机的原生驱动能够直接通过真实的I/O地址空间来操作I/O设备？ 第二个问题是在DMA过程中，如何控制外设访问到虚拟机所在的物理内存地址？ 第三个问题是如何将设备产生的中断发送到虚拟机的中断控制器中？ 其中第三个问题留在中断虚拟化文档中具体分析，本文将从PIO、MMIO、DMA三个方面回答前两个问题。\nPIO\n在内存虚拟化中，客户机操作系统的页表中存放的是虚拟机物理内存地址 (GPA)，需要通过一定方式（例如影子页表、EPT等）将虚拟机物理内存地址转换为主机物理内存地址 (HPA)，从而实现各虚拟机之间内存的隔离。设备直通访问同样也需要建立虚拟机I/O地址空间与实际外设I/O地址空间的映射。\n在CPU硬件辅助虚拟化中VMCS中存在I/O位图这一概念，I/O位图的一个作用是决定虚拟机是否可以直接访问某个端口，使得Hypervisor能够细粒度地管控I/O。在VM-Excution控制域内有一个I/O位图地址字段 (I/O bitmap Addresses)。该字段包括两个64位的物理地址，这两个地址分别指向两个大小为4KB的I/O位图。\nI/O位图中的每个位对应一个I/O端口。当Hypervisor将VM-Excution控制域中的启用I/O位图 (Use I/O bitmaps) 字段置1时，意味着Hypervisor会使用I/O位图控制虚拟机中的I/O指令。当客户机发起对某个物理端口的I/O操作时，如果该端口在I/O位图中对应的位值为0，此时不会发生VM-Exit，客户机会直接访问该物理端口。\n目前很多Hypervisor（例如Xen、Xvisor等）都支持多种I/O虚拟化方式。根据应用场景的需要，虚拟机拥有的I/O设备，其中部分是设备模拟技术提供的虚拟设备，另一部分则是Hypervisor直接分配的物理设备。在这种情况下，虚拟设备的PCI BAR由客户机中的虚拟BIOS配置，可能会与直接分配的物理设备的PCI BAR产生冲突，所以不能直接使用上文介绍的直接访问物理端口的方式。Hypervisor会维护一个直通设备的虚拟PCI BAR与真实PCI BAR之间的映射表，并将虚拟PCI BAR中的端口在I/O位图中对应的位置为1。这样虚拟机通过直通设备的虚拟PCI BAR发起I/O访问时会产生VM-Exit陷入Hypervisor中，然后Hypervisor会根据映射表将访问请求发送给直通设备的真实I/O端口。\nMMIO\n由于内存与MMIO同属于物理地址空间，且MMIO使用与内存访问相同的访存指令通过物理地址完成对I/O资源的访问，所以理论上可以使用内存虚拟化的相关机制解决MMIO问题。\n在内存虚拟化章节中，介绍了Intel VT提供的内存虚拟化支持，扩展后的MMU能够查询EPT，以硬件实现的方式完成GPA到HPA的映射过程。在开启EPT功能的物理机上，Hypervisor可以在客户机私有的EPT中建立反映虚拟MMIO物理地址与实际MMIO物理地址之间映射关系的页表项。之后当客户机以MMIO的方式访问那些分配给它的物理外设时，EPT机制会在不产生VM-Exit的情况下（假设不发生EPT Misconfiguration和EPT Violation）完成对物理外设的访问。\nDMA\n为了解决第二个问题，将DMA过程中对内存的访问限制在发起DMA的设备所在虚拟机的物理内存区域，Intel的VT-d技术提供了DMA重映射机制，在位于PCI总线树根部的北桥芯片中引入了DMA重映射硬件。\n在启动DMA重映射硬件的系统上，当根节点下的PCI设备尝试通过DMA的方式访问内存时，DMA重映射硬件会拦截该访问，并通过查询I/O页表的方式来确定本次访问是否被允许，并重映射到内存访问的实际位置。**I/O页表是与分页机制类似的页表结构，I/O页表的创建和维护由Hypervisor负责，**后面将会详细介绍I/O页表。\n在I/O虚拟化中，每个虚拟机都拥有与主机物理地址空间不同的物理地址空间视图。DMA重映射硬件将从I/O设备发过来的访问请求中包含的地址看作是DMA地址。根据不同的使用配置，**设备的DMA地址可以是分配给它的虚拟机的GPA、发起DMA请求的宿主机进程的HVA、客户机进程的GVA以及IOVA。**根据DMA地址空间的不同，DMA重映射硬件将来自I/O子系统的DMA访问请求分为如下两类：\n**不带PASID（Process Address Space Identifier，进程地址空间标识符）的请求：**这类请求使用的DMA地址一般是GPA和IOVA，并且通常会表明该请求的类型（读、写或原子操作）、DMA目标的地址、大小和发起请求的源设备的ID等信息； **带有PASID的请求：**这类请求使用的DMA地址一般是GVA和HVA，只有具备虚拟地址功能 (Virtual Address Capability) 的PCI设备才能发出这类请求。除了 1 提到的信息之外，这类请求还带有用于定位进程地址空间的PASID和一些其他信息。 虽然I/O子系统的内存访问请求有不同种类，DMA地址的类型也有所不同，但**重映射硬件的最终任务都是要将DMA地址转换成HPA，并实现对物理地址的访问。**DMA地址重映射发生在地址解码、查询处理器缓存或转发到内存控制器等进一步的硬件操作之前。下图是DMA地址重映射的一个例子：\n首先，I/O设备1和I/O设备2分别被分配给虚拟机1和虚拟机2。之后，Hypervisor为两个虚拟机分配系统物理内存，并开启DMA地址重映射功能。设备1和设备2分别发起与目的物理地址相同的DMA请求，但由于设备分属于不同的虚拟机，DMA重映射硬件会将其分别转换为设备所在虚拟机的实际物理地址。\nDMA重映射硬件每次捕获设备DMA请求时，需要先识别发起该请求的外设。一般使用源标识符 (Source Identifier) 标识发起DMA操作的设备。重映射硬件可以根据设备的类别以及I/O事务的具体实现方式确定外设的源标识符。例如，一些I/O总线协议中，源标识符是I/O事务的一个组成部分。对于PCI设备，PCI设备标识符能够唯一确定某个PCI设备，因此PCI总线使用PCI设备标识符作为源标识符。\n在x86架构下，内存的页表基地址会存储在CR3寄存器中，CPU通过读取CR3的值即可获得当前进程的页表。之后，通过查询页表过程即可以完成虚拟地址与物理地址的转换。设备的I/O地址转换过程则较为复杂，DMA重映射硬件支持两层地址转换，第一级转换 (First-level Translation) 负责将GVA重新映射到GPA，第二级转换则将GPA重新映射到HPA。I/O设备的DMA请求有两种——不带PASID和带有PASID，由于前者使用的是GPA，所以只需要第二级转换，而后者使用的是GVA，则需要两层地址转换过程。\nVT-d提供两种地址翻译模式，传统 (Legacy) 地址转换模式与可扩展 (Scalable) 地址转换模式。传统模式仅支持第二级地址转换，所以只能应用于不带PASID的请求。可扩展模式支持两层地址转换，能够同时支持不带PASID和带有PASID的DMA请求。下文分别介绍这两种模式。\n在传统模式中，如下图所示，根条目表作为最顶层的结构将设备映射到各自的虚拟机，所以根条目表也是DMA地址转换的起点。根条目表在系统内存中的位置是由根条目表地址寄存器 RTADDR_REG 决定的。\n下图描述了 RTADDR_REG 的字段分布情况。根条目表地址寄存器的转换表模式字段 (RTADDR_REG.TTM) 用于表示目前所使用的地址翻译模式。\n当RTADDR_REG 的TTM字段为00时，代表处于传统模式，RTADDR_REG 的根条目表地址字段 (RTADDR_REG.RTA) 保存的是指向根条目表的指针。\n根条目表 (Root Table) 的大小为4KB，包含256个根条目，每个根条目对应一条PCI总线。在检索根条目表时，使用源标识符字段中的总线号（高8位）作为索引定位到发起DMA的设备所在总线对应的根条目。根条目中包含一个上下文条目表的指针，该指针用于获取总线上所有设备共同的上下文条目表。每个上下文条目表包含256个条目，每个条目对应于总线上的某个PCI设备。对于PCI设备，将使用源标识符的设备号和功能号（较低的8位）索引上下文条目表，以获得目标设备对应的上下文条目。上下文条目中保存有一个第二级转换页表的基地址。使用DMA地址查询该转换页表时，会依次映射到I/O页表的多级地址转换结构，并最终将DMA请求中的GPA转换为HPA。\n当 RTADDR_REG.TTM 为01b时，此时DMA重映射硬件处于下图所示的可扩展地址转换模式，RTADDR_REG 的RTA字段保存指向可扩展模式下根条目表的指针。\n可扩展模式根条目表与根条目表类似，大小都是4KB，并且同样包含256个根条目，每个根条目对应一条PCI总线。与根条目中将 [64∶127] 设为保留字段不同的是，每个可扩展模式根条目分为[0∶63]与[64∶127]两部分。如下图所示，低64位中的LCTP（Lower Context Table Pointer，低上下文表指针）字段指向下半部(lower)上下文条目表，高64位中的UCTP（Upper Context Table Pointer，高上下文表指针）字段指向上半部(upper)上下文条目表。\n下半部上下文条目表的大小为4KB，包含128个可扩展模式上下文条目，对应总线上设备号在 0~15 的PCI设备。上半部上下文条目表的大小也是4KB，包含128个可伸缩模式上下文条目，对应总线上设备号在 16~31 的PCI设备。在可扩展模式下，处理不带PASID的请求与带有PASID的请求类似，一样可以从可扩展模式上下文条目的RID_PASID字段中获得PASID值。可扩展模式上下文条目的PASIDDIRPTR字段包含指向可扩展模式PASID目录的指针。\n请求的PASID值的高14位 (19∶6) 会用于索引可伸缩模式的PASID目录。每个现有的可伸缩模式PASID目录条目的SMPTBLPTR字段都包含一个指向可伸缩模式PASID表的指针。请求的PASID值的较低6位(5∶0)被用于索引可伸缩模式的PASID表。PASID表条目的FLPTPTR字段和SLPTR字段分别包含第一级地址转换和第二级转换页表的指针。同时，PASID表条目的PGTT字段表明需要进行哪些地址转换过程，PGTT为001b时代表只进行第一阶段地址转换，010b代表只进行第二阶段地址转换，011b则意味着需要执行嵌套转换以达到双层地址转换的目的。\nDMA重映射在第一级地址转换和第二级地址转换中都使用了多级页表结构。第一级地址转换支持4级结构或5级结构。第二级地址转换支持N级结构，其中N的值取决于由 Capability 寄存器支持的GAW（Guest Address Width，客户机地址宽度）。每级页表的大小都为4KB，拥有512个页表项。页表的翻译过程与内存分页机制类似，在此不再过多介绍。I/O页表同样支持4KB、2MB、1GB等粒度的页面大小。通过 Capability 寄存器可以查询系统支持页面大小的粒度信息。\n在内存分页机制中，CPU能够借助Cache、TLB等缓存机制来加速CPU对内存的访问过程。重映射硬件也可以通过缓存与重映射地址转换相关的数据结构来加速地址转换过程。\n下面是重映射硬件地址翻译缓存 (Translation Cache) 的几种类型：\n**上下文条目缓存：**该缓存的作用是缓存上下文条目，用于地址翻译请求。翻译请求中的源标识符用于索引对应的缓存条目； **PASID缓存：**该缓存的作用是缓存可伸缩模式的PASID表条目，这些条目用于地址转换。该缓存只在启用可缩放模式转换时使用； **IOTLB：**IOTLB（I/O Translation Lookaside Buffer，I/O翻译后备缓存器）中的每个条目负责保存请求地址的页号与对应物理页之间的映射； **分页结构缓存：**该缓存的作用是缓存经常使用的分页结构条目，这些条目用于引用其他分页结构条目。缓存的分页结构条目可以是PML5缓存、PML4缓存、PDPE缓存或PDE缓存。 在重映射硬件上，翻译缓存可以支持多个外部设备的请求。**缓存效率取决于平台中同时活动的DMA流的数量和DMA访问的地址局部性。**重映射硬件持有的翻译缓存资源十分有限，某些情况下可能无法满足设备使用需求。一种扩展缓存资源的方式是让外设参与重映射过程，将翻译缓存实现在外设中。设备上的这些翻译缓存称为设备TLB。设备TLB减轻了北桥芯片中翻译缓存的压力，同时使设备在发起DMA请求之前，通过翻译缓存使实现地址转换成为可能。设备TLB可以用于对访问延迟要求较高的设备（如实时设备），以及具有高DMA工作负载或多个DMA流的设备。\n2.5 VFIO 由于Linux遵循GPL（General Public License，通用公共许可证）开源协议，根据GPL协议内容，所有基于内核的驱动程序同样应该遵守GPL开源协议。一些商业公司推出的设备驱动会采用闭源策略，例如英伟达公司的GPU驱动基本是闭源的。AMD公司虽然积极参与Linux GPU驱动开源工作，但是AMD闭源GPU驱动的性能通常优于开源驱动。为了绕过Linux的GPL协议，厂商通常使用用户态驱动框架来达到在Linux上闭源的目的，例如英伟达公司将CUDA驱动和OpenGL驱动实现在用户态。除了能够绕开开源协议，这种UIO（Userspace I/O，用户空间I/O）拥有开发工作量少、易于调试以及能够集成到特定应用程序（例如前文提到的DPDK）中等优势。\nUIO虽然拥有上述诸多优势，但是作为一个运行在用户态的驱动它有一个致命的缺点，就是无法动态申请DMA区域。因为如果UIO可以随意发起DMA请求，那么意味着普通用户可以读写任何物理地址的内容，这样就会给系统的安全性造成极大的威胁。幸运的是该问题能够被上文介绍的VT-d（GVA/HVA -\u0026gt; HPA）技术解决，因此基于IOMMU的另外一套用户态驱动框架——VFIO（Virtual Function I/O，虚拟功能I/O）应运而生。\nVFIO会为用户态进程提供一系列交互接口。同UIO一样，VFIO使用一个非常简单的内核模块为用户提供名为 /dev/vfio/$Group 的用于设备访问的文件接口。用户进程可以通过一组ioctl与VFIO进行交互，例如ioctl函数可以配置IOMMU，并将DMA地址映射到进程地址空间，从而允许用户态驱动发起DMA操作，除此之外，还可以通过ioctl配置对应的PCI配置空间、获取相关中断信息并注册中断处理函数。VFIO相较于UIO更加安全，能够限制用户态驱动对内存地址的访问，这一特性使得VFIO能够应用于虚拟化环境。同时VFIO也可以被集成在DPDK等特定应用程序中，相较于UIO，VFIO的使用优先级更高。\n2.6 SR-IOV SR-IOV架构如下图所示：\n首先介绍 PF/VF 的概念：\n**PF是一种支持SR-IOV功能的PCI功能，**它可以对设备所有的物理资源进行配置，将其划分为多个资源子集，并在资源子集之上虚拟出VF； **VF是一种轻量级PCIe功能，**可以看作是从PF中分离出来的部分I/O功能，它与PF相关联，拥有属于自己的一组用于数据交互的资源。这些资源包括用于数据传输的内存地址空间、中断以及I/O端口等，这些资源会记录在VF专属的配置空间中。 在SR-IOV标准下，Hypervisor或者某一特权虚拟机通过调用PF驱动，开启设备的SR-IOV功能，并创建和管理多个VF设备。之后使用类似于直通访问的模式，将VF设备分配给其他虚拟机。虚拟机无须通过Hypervisor提供虚拟设备来访问I/O设备，可以通过PIO、MMIO、DMA等方式直接与VF交互。\n在系统启动时，物理BIOS会自行为VF分配配置空间并初始化，并将分配给设备的内存地址、I/O端口地址记录在BAR中。出于安全性和隔离性等方面的考虑，客户机并不能直接使用BAR中保存的HPA，也不能直接修改VF配置空间。Hypervisor需要为客户机提供虚拟的VF配置空间，将配置空间的地址信息替换为GPA，同时建立并维护GPA到HPA的映射表。Hypervisor会截获客户机对VF的访问请求，然后根据GPA与HPA的映射关系定位到实际的地址空间执行。\n3 QEMU/KVM虚拟设备的实现 本节将以QEMU/KVM为基础，深入代码层面介绍I/O虚拟化在Hypervisor中的实现。\n本节选择Masaryk大学编写的edu设备作为示例，edu设备属于PCI设备，设备源代码位于QEMU的 /hw/misc 路径下。**edu设备结构比较简单，并且不与实际的物理设备交互，是一个纯粹的 “虚拟” 设备，**但它的功能较为全面，以该设备为例能够清晰地展示在QEMU中实现一个虚拟设备的整个过程。\n本节的讲解流程如下：\n首先会通过描述QEMU中的QOM（QEMU Object Model，QEMU对象模型）机制来展示edu设备对象的注册与创建过程； 之后会介绍主板芯片的模拟和PCI总线结构的创建与初始化过程； 然后介绍PIO和MMIO在QEMU中的处理过程； 最后，我们将以实验的形式**为edu设备编写配套的设备驱动，**来展示在QEMU中如何模拟一个虚拟PCI设备，包括模拟虚拟设备的访问接口、实现设备DMA以及自定义的设备功能、完成设备中断发送等过程。 3.1 QEMU对象模型 在QEMU/KVM架构中，QEMU在整个架构中作为一个用户态进程运行在VMX根模式的Ring3特权级，**与vCPU创建和设备模拟相关的内容由QEMU负责。**经过多年的发展，QEMU能够模拟多种架构的CPU和大量的设备。不同架构的CPU之间以及同种架构不同型号CPU之间拥有通用属性同时也有自身的特性。对于设备来说也存在这种情况。例如网卡作为一种PCI设备，拥有自己的功能，也遵循PCI通用标准，同样PCI设备也属于设备的一种类别。\n熟悉面向对象编程语言的朋友应该会想到这种情况适合面向对象的思想，可以将不同类型设备之间的共性抽象成一个设备父类，某一类设备同时也是特定设备的父类。\nC语言本身并不支持面向对象，早期QEMU的每种设备都有不同的表示方式，无法利用不同设备之间的共性，导致代码混乱且冗余。为了改变这一情况，QEMU推出了QOM。从某种程度上来说，**QOM也可以看作QEMU在C语言的基础上实现的一套面向对象机制，**负责将CPU、内存、总线、设备等都抽象为对象，其中总线和设备模拟占了很大的比重。所以在讲总线和设备初始化之前，首先以edu设备对象的初始化为例介绍QOM。\n在QOM中，一类设备被抽象为一个对象类，一个设备实例被抽象为一个对象实例，对象和对象实例均存在继承关系，其中ObjectClass是所有对象类的基类，Object是所有对象实例的基对象，有点类似于C++中的类和对象。\n除了上述对象类和对象实例外，QOM对象初始化还涉及TypeInfo和TypeImpl两个数据结构。TypeInfo是对对象类的描述，往往包含类名、父类名、类初始化函数、类实例大小等描述性信息。TypeImpl由TypeInfo注册得到，存储在全局type_table中。TypeImpl与TypeInfo最大的不同在于，TypeImpl持有对其对象类的引用，因此要从TypeInfo得到ObjectClass，必须先将TypeInfo转化为TypeImpl。\nQOM中对象的初始化可分为四步：①将TypeInfo注册为TypeImpl；②创建对象类；③创建对象实例；④具现对象实例。\n设备对象类注册 TypeInfo注册为TypeImpl包含两个步骤：\n首先将TypeInfo转换为ModuleEntry； 然后通过ModuleEntry存储的初始化函数将TypeInfo转换为TypeImpl，并添加到全局type_table中。 以edu设备为例，TypeInfo转换为ModuleEntry的具体代码如下：\nqemu-4.1.1/hw/misc/edu.c\nedu设备代码中会静态定义TypeInfo（即 edu_info），type_init 函数则是由CRT (Crun-time) 负责执行。\ntype_init 函数接受一个初始化函数指针作为参数，创建一个ModuleEntry存储初始化函数指针及ModuleEntry的类型。QEMU中定义了几种不同类型的ModuleEntry结构体，同一种类型的ModuleEntry链接为ModuleTypeList，全部ModuleTypeList则存储于全局数组 init_type_list 中。组织结构如下图所示：\nedu_info 注册的ModuleEntry对应的类型为MODULE_INIT_QOM，其余类型还有MODULE_INIT_BLOCK、MODULE_INIT_OPTS等。为edu_info注册对应的Mod-uleEntry后，通过 module_call_init 函数便可以将edu_info转换为TypeImpl，整个函数的调用流程如下图所示：\nmodule_call_init 函数遍历ModuleTypeList中的ModuleEntry并执行其存储的初始化函数。对于edu_info而言，其对应的初始化函数就是type_init 函数传入的函数指针，即 type_register_static(\u0026amp;edu_info) 函数。\ntype_register_static 函数通过 type_register 调用 type_register_internal 函数注册edu设备的TypeInfo。type_register_internal函数调用 type_new 函数将TypeInfo转换为TypeImpl，并调用 type_table_add 函数将得到的TypeImpl添加到全局的 type_table 中。\n设备对象类创建 完成edu设备对象类注册之后还需要创建该对象类。创建对象类有两种方式：\n一种是主动调用 object_class_get_list 接口函数，比如 object_class_get_list(TYPE_DEVICE,false) 函数，创建TYPE_DEVICE类型的ObjectClass； 另一种是被动调用，如 object_class_by_name 函数、object_class_get_parent函数、object_new_with_type函数，object_initialize_with_type 函数。 无论是主动调用还是被动调用，这些接口最终都会调用 type_initialize 函数，type_initialize 的调用过程如下图所示：\ntype_initialize 函数接受一个TypeImpl结构体作为参数，代码流程如下：\n首先为该TypeImpl对应的对象类分配内存空间，并将TypeImpl的class成员指向该内存区域； 然后调用 type_get_parent 函数获取其父对象类的TypeImpl；type_get_parent 函数最后会调用 type_get_by_name 函数，而前面提到type_get_by_name 函数最终也会调用 type_initialize 函数，从而实现对父类的初始化。这样就形成了递归调用，逐级向上初始化父对象类，直至到达根对象类ObjectClass； type_initialize 函数随后调用 memcpy 函数将父对象类复制到其内存空间的前面，这样只要知道父对象类和子对象类的大小，就可以轻松实现父类和子类之间的转换； 最后 type_initialize 函数将调用父类的 class_base_init 函数和该TypeImpl的 class_init 函数进行初始化。edu_info定义edu对象类的 class_init 函数为 edu_class_init 函数。 edu_class_init 函数设置了edu设备的realize函数，该函数用于下文提到的edu对象实例具现化，同时还设置了edu设备的厂商号与设备号等设备属性。edu_class_init 函数的具体代码如下：\nqemu-4.1.1/hw/misc/edu.c\n设备对象实例创建 QOM将一个具体的设备抽象为一个对象实例，每个对象实例都对应一个 XXXState 结构体，记录设备自身信息。在edu设备源码中定义了edu设备对象的结构体EduState，EduState中包含MMIO内存区域信息、设备状态、中断返回状态、DMA相关信息等属性。定义EduState的具体代码如下：\nqemu-4.1.1/hw/misc/edu.c\n**对象实例与对象类类似，也存在继承关系，**Object数据体是所有对象实例的根对象实例。定义Object的具体代码如下：\nqemu-4.1.1/include/qom/object.h\n根据上述定义，对象实例持有对其所述对象类的引用。因此在创建对象实例时，需要创建相应的对象类，也就是上文提到的被动创建对象类。在完成edu设备对象类的初始化后，QEMU已经能够创建edu设备对象实例。一般的做法是在QEMU启动命令行中添加 -device edu 参数。QEMU在检查到该参数后，会调用 qdev_device_add 函数添加edu设备。\n用于创建对象实例的接口包括 object_new 函数和 object_new_with_props 函数，它们最终都会调用 object_new_with_type 函数，qdev_device_add 函数使用的是 object_new 接口函数。object_new_with_type 函数的调用路径如下图所示：\nobject_new_with_type 函数接收edu设备的TypeImpl结构体作为参数，代码流程如下：\n首先调用 type_initialize 函数确保edu设备对象类被初始化； 然后为edu对象实例分配大小为 sizeof(EduState) 的内存空间； 最后调用 object_initialize_with_type 函数初始化对象实例。 object_initialize_with_type 函数首先为EduState中的属性分配Hash表； 然后调用 object_init_with_type 函数。 object_init_with_type 函数首先判断该实例对象是否有父实例对象，若有，则递归调用object_init_with_type 函数对其父实例对象进行初始化； 最后调用TypeImpl的 instance_init 函数。TypeImpl中的 instance_init 函数在TypeInfo注册为TypeImpl时设置，edu设备在edu_info中将该函数设置为 edu_instance_init 函数，该函数将初始化EduState并设置edu设备的DMA掩码。edu设备的DMA掩码默认为28位，即只支持256MB地址范围。 edu_instance_init 函数的具体代码如下：\nqemu-4.1.1/hw/misc/edu.c\n前面提到，所有对象实例的根对象实例都是Object，各对象实例之间的继承关系如下图所示，此处仅列出它们的类型：\n设备对象实例具现化 此时已经创建了edu设备对象实例并调用了实例初始化函数 edu_instance_init，但由于此时EduState里的属性并未分配，所以并不能立即使用，必须具现化该对象实例。\n所谓具现对象实例是指调用设备对象实例的realize函数（如创建一个磁盘设备对象实例）后，它仍不能使用，只有在realize函数中将其挂载到总线上后，相应的I/O指令才能真正访问到该设备。\n此处仍以edu设备为例进行说明，TYPE_DEVICE类型的对象实例对应的TypeInfo结构体为 device_type_info，其定义如下：\nqemu-4.1.1/hw/core/qdev.c\n根据上述定义，设备对象实例对应的结构体为DeviceState，它的 instance_init 函数为 device_initfn 函数。前面提到创建对象实例时会逐级向上递归调用其父类型的 instance_init 函数，所以在创建edu设备对象实例时会调用 device_initfn 函数。\ndevice_initfn 函数则会调用 object_property_add_bool 函数为设备对象实例添加一个名为realized的属性。与属性名一同传入的还有两个回调函数，device_get_realized 函数和 device_set_realized 函数，它们分别为realized属性的 getter/setter 函数。\n若后续调用 object_property_get_bool/object_property_set_bool 函数读取/设置realized属性时最终会调用到device_get_realized/device_set_realized 函数，device_set_realized 函数则会调用DeviceState中存储的realize成员。**因此每次调用object_set_property_bool 设置realized属性时会触发设备的realize回调。**具体代码如下：\nqemu-4.1.1/hw/core/qdev.c\n不同设备对象实例对应的realize函数不同，上文提到，edu设备对象实例在其类实例初始化函数 edu_class_init 中将realize函数设置为pci_edu_realize 函数。在此简要介绍该函数的功能。pci_edu_realize 函数具体代码如下：\nqemu-4.1.1/hw/misc/edu.c\npci_edu_realize 函数会初始化edu设备配置空间中的部分数据并设置设备的功能，代码流程如下：\n该函数首先调用 pci_config_set_interrupt_pin 函数，将PCI配置空间中Interrupt Pin寄存器(0X3D)的值设置为1，这意味着edu设备使用INTA#脚申请中断； 之后调用 msi_init 函数设置PCI配置空间以开启MSI功能； timer_init_ms 函数用于注册定时器，不间断地查看是否有DMA传送需求； qemu_thread_create 函数用于创建一个线程，该线程用于阶乘计算，属于edu设备的一个特定功能； memory_region_init_io 函数初始化一个MMIO内存区域，该内存区域大小为1MB，并指定该MMIO内存区域的读写函数edu_mmio_ops。在 edu_mmio_ops 函数中指定 edu_mmio_read/edu_mmio_write 函数作为MMIO读写回调函数，该回调函数就是内存虚拟化文档中提到的MMIO处理函数，负责模拟虚拟设备的MMIO访问； pci_register_bar 函数把上一步设置好的MMIO参数注册到设备配置空间的第0号BAR。 至此edu设备的具现化便完成了，此时用户或客户机可以通过设备驱动使用该设备。\n本节通过edu设备这一例子，介绍了QEMU中使用的QOM工作机制，阐述了一个PCI设备在QEMU中注册并初始化对象类、创建和初始化对象实例以及最终具现化对象实例的过程。但本节并未涉及与PCI设备在总线上注册相关的内容，这部分内容会在3.2节介绍。\n3.2 主板芯片组与总线模拟 在虚拟机启动之前，QEMU会模拟并初始化主板上的芯片组，例如南北桥芯片。在命令行输入 qemu-system-x86_64-machine help，终端会显示QEMU支持 i440FX+PIIX 和 Q35+ICH9 这两种芯片组。QEMU最初只提供 I440FX+PIIX 架构，该架构诞生年代久远，不支持PCIe、AHCI等特性，为了顺应芯片组的发展，Jason Baron在2012年的KVM forum上为QEMU加入Q35芯片组支持。\n本文仅介绍QEMU默认的I440FX架构，对QEMU中与Q35架构相关内容可以阅读QEMU提供的文档与源码。\nI440FX是Intel公司在1996年推出的第一款能够支持Pentium Ⅱ的主板芯片，它集成了多种系统功能，在主板上作为北桥，负责与主板上高速设备以及CPU的连接。PIIX（PCI ISA IDE Xcelerator，南桥芯片）本质上是一个多功能PCI设备，被称作南桥，PIIX由I440FX引出，负责与主板上低速设备的连接。下图为**QEMU中模拟的I440FX主板架构，**该图所示的架构与芯片组实际架构基本对应：\nI440FX中的PMC（PCI Bridge and Memory Controller，PCI桥内存控制器）提供了控制内存访问的接口，PCI主桥则作为控制和连接PCI总线系统的PCI根总线接口，因此I440FX可以向下连接内存并且可以通过PCI根总线接口扩展出整个PCI设备树，其中PIIX南桥芯片位于PCI 0号总线。I440FX同时还可以通过连接HOST总线向上与多个CPU相连。如上图所示，PIIX作为南桥可以与IDE控制器、MA控制器、USB控制器、SMBus总线控制器、X-Bus控制器、USB控制、PIT、RTC（Real Time Clock，实时时钟）、PIC设备相连，同时PIIX还提供了PCI-ISA桥，用于连接ISA总线进而实现与传统ISA设备的连接。\nI440FX芯片初始化 在QOM工作机制中，QEMU的所有设备都被抽象为对象，对于整个机器来说也不例外，虚拟机同样拥有属于自己的对象类型。在QEMU中定义了机器的对象类型，使用 MachineClass 数据结构表示。MachineClass 的类别与主板芯片类型相关联，通常由特定的宏来定义。例如DEFINE_Q35_MACHINE/DEFINE_I440FX_MACHINE 分别定义了Q35主板架构与I440FX主板架构的机器。\n下面将介绍**I440FX架构初始化过程，**部分代码如下：\nqemu-4.1.1/hw/i386/pc_piix.c\nDEFINE_I440FX_MACHINE 宏由两部分组成：\n首先该宏定义了I440FX虚拟机的初始化函数 pc_init_# #suffix，其中suffix代表I440FX的版本，该函数通过调用 pc_init1 函数来完成对整个虚拟机的初始化。pc_init1 函数是整个虚拟机初始化的核心，涵盖虚拟机的方方面面，I440FX主板芯片组的初始化也由该函数负责。 第二部分 DEFINE_PC_MACHINE 也是一个宏，在不同主板架构的机器间通用，**负责虚拟机对象类型的注册与初始化。**具体代码如下： qemu-4.1.1/include/hw/i386/pc.h\n该宏的具体工作如下：\n定义了虚拟机对应的TypeInfo，即 pc_machine_type_##suffix，并将父类型设置为TYPE_PC_MACHINE，同时把 class_init 函数设置为pc_machine_##suffix##_class_init 函数。该函数负责创建虚拟机对象类型，并把类的初始化函数设置为上文提到的 pc_init_##suffix 函数。 之后 type_init(pc_machine_init_##suffix) 函数负责注册虚拟机对象类型，注册的具体过程上文已经介绍，不再赘述。 在虚拟机初始化过程中，之前提到的 pc_init1 函数会对I440FX主板进行初始化。其中 i440fx_init 函数和 piix3_create 函数分别是I440FX北桥芯片和PIIX3南桥芯片的初始化函数。pc_init1 函数的部分代码如下：\nqemu-4.1.1/hw/i386/pc_piix.c\ni440fx_init 函数需要传入多个参数，这里主要关注前三个参数。其中host_type与pci_type参数对应于 pc_init1 函数的后两个宏定义参数：\nhost_type 代表I440FX芯片的PCI主桥部分； pci_type 代表I440FX芯片在PCI总线上的部分。 该PCI设备的设备实例用PCII440FXState表示。\u0026amp;i440fx_state 参数传入的是 pc_init1 函数中定义的PCII440FXState类型指针。宏定义的具体代码如下：\nqemu-4.1.1/include/hw/i386/pc.h\n与I440FX芯片的结构相对应，I440FX芯片初始化分为PCI主桥（即北桥）和PCII440FX初始化两部分。i440fx_init 函数的核心代码如下：\nqemu-4.1.1/hw/pci-host/piix.c\nqdev_new(host_type) 函数的作用是创建PCI主桥，该函数与添加edu设备时调用的 qdev_device_add 函数类似，都是通过调用object_new 接口函数，根据传入的设备类型创建设备对象实例；\nx86架构在PCI主桥提供 config_address 寄存器（端口地址为0xCF8）与 config_data 寄存器（端口地址为0xCFC）这两个32位寄存器，用于访问PCI设备的配置空间。在PCI主桥设备实例创建和具现化过程中完成对这两个寄存器的初始化，并将其加入I/O地址空间中。在pci_host.c文件中定义了函数 pci_host_config_write、pci_host_config_read、pci_host_data_write 和 pci_host_data_read。这四个函数负责 config_address 寄存器和 config_data 寄存器的读写。\nPCI主桥设备实例创建和具现化的具体代码如下：\nqemu-4.1.1/hw/pci-host/piix.c 创建实例\nqemu-4.1.1/hw/pci-host/piix.c 具现化\n然后 pci_root_bus_new 函数会调用 qbus_create 函数创建一条PCI总线，该总线也称为0号总线，之后调用 pci_root_bus_init 函数对总线进行初始化并将其挂载到PCI主桥；\n然后从 pci_root_bus_new 函数退出，执行 i440fx_init 函数中的 qdev_init_nofail 函数，该函数最终会调用object_property_set_bool(OBJECT(dev)，\u0026quot;realized\u0026quot;，true，errp) 函数。object_property_set_bool 函数会将北桥设备的realized属性设置为true，触发北桥设备具现化函数的回调。pci_root_bus_new 函数的具体代码如下：\nqemu-4.1.1/hw/pci/pci.c\n至此，I440FX芯片第一阶段的PCI主桥初始化结束。\n在I440FX初始化第二阶段，pci_create_simple 函数直接调用 pci_create_simple_multifunction 接口函数，并最终调用 object_new 函数与 object_property_set_bool 函数完成PCI I440FX设备的创建和具象化过程。最终PCI I440FX设备会被挂载到PCI 0号总线（根总线）的0号插槽。\n在较新版本的QEMU源码中，I440FX和PIIX3的初始化由 pc_init1 函数中的两个 i440fx_init 函数和 piix3_create 函数分别执行。i440fx_init 执行结束后会把PCI根总线返回给 pc_init1 函数，随后 pc_init1 函数会将PCI根总线作为参数传入 piix3_create 函数。然而在QEMU 4.1.1版本，PIIX3设备的创建过程也由 i440fx_init 函数执行，i440fx_init 函数使用与PCI I440FX设备相同的pci_create_simple_multifunction 接口创建和具现化PIIX3设备。\n在PIIX3设备对象的具现化函数 piix3_realize 中，会通过 isa_bus_new 函数创建一条ISA总线，该ISA总线会挂载到PIIX3下。最后 pci_bus_irqs 函数和 pci_bus_set_route_irq_fn 函数会设置PCI根总线的中断路由信息。QEMU 4.1.1版本PIIX3设备创建的部分代码如下：\nqemu-4.1.1/hw/pci-host/piix.c\nPCI总线是一个多级结构，PCI设备、PCI-PCI桥、PCI-ISA桥设备会注册到总线上。桥设备会扩展PCI总线结构，例如PCI-PCI桥设备会创建下一级PCI总线。这样就形成了 “总线—设备—总线—设备” 的树形结构。目前 pci_root_bus_new 函数已经在主桥下创建了PCI根总线，pc_init1 函数之后会将系统默认的一些PCI设备（例如e1000网卡、VGA控制器）注册到PCI根总线上。\nPCI设备注册到总线上 **PCI设备的注册是在PCI设备具现化过程中完成的。**下文仍以edu设备为例介绍PCI设备的具现化过程。\n3.1节提到，edu设备初始化过程会调用父类型的实例初始化函数。edu设备属于PCI设备，其父类型为PCIDeviceClass，该类型的初始化函数为pci_device_class_init 函数。pci_device_class_init 函数会将PCIDeviceClass的realize函数设为默认的 pci_qdev_realize 函数。pci_device_class_init 函数的具体代码如下：\nqemu-4.1.1/hw/pci/pci.c\npci_qdev_realize 函数首先会调用 do_pci_register_device 函数执行通用的PCI设备初始化流程，包括设置edu设备在对应总线上的插槽号、初始化edu设备的地址空间、分配edu设备的配置空间并初始化配置空间里的部分寄存器、设置配置空间的读写函数、将edu设备加入所在总线的devices数组中。具体代码如下：\nqemu-4.1.1/hw/pci/pci.c\n之后 pci_qdev_realize 函数会执行edu设备的realize函数，该函数的作用3.1节已经介绍，此处不再赘述。部分PCI设备可能会有专属的设备ROM，在 pci_qdev_realize 函数中，最后会执行 pci_add_option_rom 函数将ROM文件注册到PCI设备的BAR中。如果edu设备不存在ROM，进入 pci_add_option_rom 函数后会直接返回。\n至此，edu设备便被完全初始化并挂载到对应的PCI总线之上。\n3.3 QEMU/KVM设备访问的模拟 本节将完整介绍QEMU/KVM架构处理虚拟机发起的PIO与MMIO请求过程。\n在QEMU/KVM架构下，KVM会将x86架构提供的用于端口访问的 IN/OUT、INS/OUTS 指令设置为敏感指令，使得虚拟机在发起PIO时会产生VM-Exit，进而陷入KVM和QEMU中进行处理。在介绍QEMU/KVM如何将上述指令设置为敏感指令之前，首先回顾一下I/O位图的相关概念。\nVM-Excution 控制域的I/O位图地址字段会包含两个64位的物理地址，这两个物理地址分别指向两个大小为4KB的I/O位图A和I/O位图B。两个I/O位图中的每位都对应一个I/O端口，其中I/O位图A包含的I/O端口地址范围为 0x0000~0x7fff，I/O位图B包含的I/O端口地址范围为0x8000~0xffff。当某个I/O端口在I/O位图中的对应位为1时，代表当虚拟机访问该端口时会发生VM-Exit。因此，KVM通过将VMCS中的I/O位图全部置为1，便可以实现对虚拟机中端口访问指令的截获。\n虚拟机发起PIO访问 从VM-Exit到处理IO 当vCPU因虚拟机执行PIO指令发生VM-Exit时，vmx_vcpu_run 函数将 vmx-\u0026gt;exit_reason 设置为EXIT_REASON_IO_INSTRUCTION后会返回至vcpu_enter_guest 函数。之后 vcpu_enter_guest 函数通过kvm_x86_ops的handle_exit成员调用 vmx_handle_exit 函数。\nvmx_handle_exit 函数根据前面的 vmx-\u0026gt;exit_reason 为EXIT_REASON_IO_INSTRUCTION，进而调用全局数组 kvm_vmx_exit_handlers 中用于处理PIO产生的VM-Exit的处理函数 handle_io 。该过程的具体代码如下：\nlinux-4.19.0/arch/x86/kvm/vmx.c\nlinux-4.19.0/arch/x86/kvm/vmx.c\nlinux-4.19.0/arch/x86/kvm/vmx.c\nKVM的IO处理函数 (内核态) handle_io 函数首先会调用 vmcs_readl 函数从VMCS中读取VM-Exit相关信息，包括端口访问方式in（读或写）、访问数据大小size以及端口号port。然后 handle_io 函数会将上述信息作为参数传递给 kvm_fast_pio 函数做进一步处理，具体代码如下：\nlinux-4.19.0/arch/x86/kvm/vmx.c\nkvm_fast_pio 函数根据in的值调用 kvm_fast_pio_in 函数或 kvm_fast_pio_out 函数。在此以 kvm_fast_pio_out 函数为例:\nkvm_fast_pio_out 函数首先会调用 kvm_register_read 函数获取要写入的数据； 之后进入 emulator_pio_out_emulated 函数将写入的数据复制到 vcpu-\u0026gt;arch.pio_data 中； 最终控制流会进入 emulator_pio_in_out 函数，emulator_pio_in_out 函数首先调用 kernel_pio 函数，尝试在KVM中处理该PIO请求，若KVM无法处理，它将 vcpu-\u0026gt;run-\u0026gt;exit_reason 设置为KVM_EXIT_IO并最终返回0，这导致 vcpu_run 函数退出循环并返回至QEMU中进行处理。 具体代码如下：\nlinux-4.19.0/arch/x86/kvm/x86.c\nlinux-4.19.0/arch/x86/kvm/x86.c\nlinux-4.19.0/arch/x86/kvm/x86.c\nlinux-4.19.0/arch/x86/kvm/x86.c\n注：QEMU对PIO和MMIO的处理放在一起分析，在下一节。\n虚拟机发起MMIO访问 由于MMIO需要将I/O端口和设备RAM映射到物理地址空间，并且CPU需要使用内存访问指令进行对设备进行MMIO访问，所以QEMU/KVM架构下对MMIO的模拟也需要利用EPT机制。\n与PIO的处理过程类似，当虚拟机发起MMIO访问时，同样会发生VM-Exit陷入KVM和QEMU中进行处理。**但与PIO不同的是，MMIO是通过缺页异常产生VM-Exit，**以下是QEMU/KVM中的具体实现过程。\nQEMU设备初始化 首先QEMU在设备初始化的过程中，会通过前文edu设备具现化中介绍的 memory_region_init_io 函数初始化一个MMIO内存区域，代码如下：\nqemu-4.1.1/memory.c\n在如上的 memory_region_init_io 函数的原型中，该函数并未调用 memory_region_init_ram 设置mr-\u0026gt;ram，因此该MemoryRegion并未实际分配内存。此时，该MemoryRegion被加入MemoryRegion树，会触发KVM的listener，从而调用listener的 kvm_region_add 函数。然后kvm_region_add 函数会调用 kvm_set_phys_mem 函数，该函数会检查MemoryRegion的类型，如果不是RAM类型，则直接返回。代码如下：\nqemu-4.1.1/accel/kvm/kvm-all.c\nqemu-4.1.1/accel/kvm/kvm-all.c\nKVM初始化设置 在KVM初始化过程中，会执行 kvm_arch_hardware_setup 函数进行硬件设置。该函数会通过kvm_x86_ops的 hardware_setup 函数最终调用vmx.c中的 ept_set_mmio_spte_mask 函数，将全局变量 shadow_mmio_mask 的最低三位设置为110b。具体代码如下：\nlinux-4.19.0/arch/x86/kvm/vmx.c\nlinux-4.19.0/arch/x86/kvm/mmu.c\nKVM中MMIO处理 当虚拟机第一次访问虚拟设备的MMIO MemoryRegion时，由于先前没有给该MR分配RAM，因此会产生一个EPT Violation缺页异常。在EPT页表缺页的情况下，KVM会调用缺页异常处理函数 tdp_page_fault，完善EPT。流程如下：\ntdp_page_fault 函数会调用 try_async_pf 函数，该函数会检查参数 gfn 是否在KVM的memslots的范围中。如果不在，则向pfn中写入KVM_PFN_NOSLOT； 最后 tdp_page_fault 函数会进入 _direct_map 函数，并经过 mmu_set_spte -\u0026gt; set_spte -\u0026gt; set_mmio_spte 函数调用链最终进入set_mmio_spte 函数。 具体代码如下：\nlinux-4.19.0/arch/x86/kvm/mmu.c\nset_mmio_spte 函数会检查当前pfn中是否被写入KVM_PFN_NOSLOT，如果被写入，则将EPT中这段客户机物理内存（即gfn）对应的EPT页表属性标记上代表MMIO的特殊值—— shadow_mmio_mask，以后的读写都会引起 EXIT_REASON_EPT_MISCONFIG 类型的VM-Exit。\n此处不讨论建立EPT映射的过程，关于处理EPT缺页异常涉及的函数，请阅读内存虚拟化文档。\n在全局数组 kvm_vmx_exit_handlers 中，EXIT_REASON_EPT_MISCONFIG 对应的VM-Exit处理函数为 handle_ept_misconfig。该函数会先尝试调用 kvm_io_bus_write 函数，在KVM中寻找能够处理本次MMIO请求的设备，如果KVM无法处理，则会调用 kvm_mmu_page_fault 函数，从而进入 handle_mmio_page_fault 函数中处理。具体代码如下：\nlinux-4.19.0/arch/x86/kvm/vmx.c\nlinux-4.19.0/arch/x86/kvm/mmu.c\nlinux-4.19.0/arch/x86/kvm/mmu.c\nhandle_mmio_page_fault 函数的处理主要有三个阶段：\n第一个阶段是通过 mmio_info_in_cache 函数检查MMIO对应的地址是否在缓存中存在，如果存在，则立即返回到 kvm_mmu_page_fault函数，返回值为RET_PF_EMULATE； 如果第一阶段未返回，则进入第二阶段，该阶段会通过 walk_shadow_page_get_mmio_spte 函数获取MMIO地址对应的spte，并通过check_mmio_spte 函数判断该地址是否代表MMIO区域，如果是，则返回RET_PF_EMULATE； 第三阶段，返回到在 kvm_mmu_page_fault 函数之后，该函数会检查 handle_mmio_page_fault 函数的返回值。如果返回值为RET_PF_EMULATE，则跳转到 x86_emulate_instruction 函数并最终进入QEMU中处理。 QEMU中PIO/MMIO处理 当KVM将控制流交给QEMU后，重新进入 qemu_kvm_cpu_thread_fn 函数执行 kvm_cpu_exec 函数。vCPU中用于保存VM-Exit相关信息的run成员变量会通过mmap映射到QEMU所在的内存空间，所以QEMU中的 kvm_cpu_exec 函数可以通过检查 kvm_run 结构中的 exit_reason，根据其退出原因进行进一步处理。\n下面是 kvm_cpu_exec 函数中的相关代码：\nqemu-4.1.1/accel/kvm/kvm-all.c\n其中：\nKVM_EXIT_IO 与 KVM_EXIT_MMIO 分别代表PIO与MMIO的exit_reason，kvm_handle_io 函数与address_space_rw 函数分别用于模拟PIO与MMIO请求。 在vCPU中的run成员中包含io与mmio这两个数据结构，用于描述PIO与MMIO相关信息。 io中定义了数据传输的方向，0代表读端口，1代表写端口，方向信息保存在 direction 成员中。size/port/count 成员分别定义了每次读写的数据长度、端口号、数据读写次数等信息。data_offset 中保存了数据在kvm_run中的偏移地址。这些信息都会作为参数传递给 kvm_handle_io 函数，用于进一步的PIO模拟； mmio结构相对简单，phys_addr 用于保存64位目的物理地址，data 用于保存读写的数据，len 代表数据长度，is_write 函数确定是读还是写。这些信息同样会作为参数传入mmio处理函数 address_space_rw 中。 io/mmio 结构如下：\nqemu-4.1.1/linux-headers/linux/kvm.h\n在QEMU 4.1.1版本中，PIO的处理函数 kvm_handle_io 本质上调用了与MMIO一样的 address_space_rw 接口，区别在于PIO传入的是I/O地址空间 address_space_io 函数，而MMIO传入的是内存地址空间 address_space_memory 函数。之后 address_space_rw 会完成对相应地址的读写操作。具体过程在内存虚拟化文档中已经介绍，这里不再赘述。具体代码如下：\nqemu-4.1.1/accel/kvm/kvm-all.c\nQEMU模拟设备功能 驱动程序通过访问设备提供的寄存器接口来使用设备的特定功能，所以**QEMU不仅要实现对虚拟设备的端口和设备内存的读写，同时需要模拟虚拟设备的功能。**下面仍以 edu 设备为例介绍虚拟设备的功能实现。\n与 edu 设备具象化相关的章节提到了 memory_region_init_io 函数，该函数会初始化一个大小为1MB的MMIO内存区域，并为该MMIO内存区域注册读写函数 edu_mmio_ops 。edu_mmio_ops 是一个MemoryRegionOps类型的结构体，作为成员变量保存在 edu 设备对应的MMIO MemoryRegion中。edu_mmio_ops 中注册的 edu_mmio_read 函数与 edu_mmio_write 函数会根据每次MMIO访问的位置和数据长度执行对应的功能函数，具体功能会在后文介绍。\n下图展示了虚拟机向 edu 设备MMIO内存区域发起读访问时的函数调用流程：\n注：①QEMU执行ioctl函数进入KVM；②KVM退出到QEMU，执行QEMU的MMIO模拟函数。\naddress_space_rw 函数经过层层调用最终会进入 memory_region_read_accessor 函数，memory_region_read_accessor 函数通过 mr-\u0026gt;ops-\u0026gt;read(mr-\u0026gt;opaque,addr,size) 会引起edu设备中 edu_mmio_read 函数的回调。具体代码如下：\nqemu-4.1.1/memory.c\nedu_mmio_read/edu_mmio_write 就是QEMU对edu设备功能的模拟，这两个函数会根据每次MMIO访问的位置和数据长度执行对应的功能函数，具体在3.4节实验中介绍。\n3.4 实验: 为edu设备添加设备驱动 Masaryk大学编写edu设备的初衷是用于内核设备驱动的教学，Linux内核中并不存在edu设备的驱动程序。\n本节将以实验的形式**为edu设备编写相应的驱动程序，**目的是为了更加清晰直观地展示虚拟设备背后的运行原理。本节分为三部分：\n第一部分：分析与edu设备功能相关的寄存器； 第二部分：介绍在驱动中如何访问edu设备的配置空间和MMIO空间、发起DMA请求以及处理设备中断； 第三部分：演示实验的整体流程。 上文提到的 edu_mmio_read 函数和 edu_mmio_write 函数是edu设备的核心，当访问的地址在MMIO内存区域的偏移小于 0x80 时，只允许4字节大小的访问；当地址偏移大于或等于 0x80 时，允许4字节或8字节的数据访问。\nedu设备功能模拟 edu设备在MMIO内存区域内会设置一些特殊的地址并赋予这些地址不同的读写权限，驱动读写这些地址时会触发相应的功能。下文会介绍这些特殊的地址。具体代码如下：\nqemu-4.1.1/hw/misc/edu.c\nqemu-4.1.1/hw/misc/edu.c\n0x00(RO)：0x00权限为只读，读0x00时会返回edu设备的标识符 0x010000edu；\n0x04(RW)：0x04权限为可读可写，读0x04时会返回edu设备中 addr4 成员变量的值，写0x04时会将写入的数据取反之后赋值给 addr4；\n0x08(RW)：0x08用于阶乘计算，权限为可读可写。\n读0x08时，会返回edu设备中 fact 成员变量的值，fact 表示阶乘结果；\n写0x08时，会将写入的数据赋值给 fact，然后edu设备的 status 会与宏变量 EDU_STATUS_COMPUTING 做或运算，并将运算结果赋值给 status 。这个宏变量的值为0x1，代表此时edu设备处于阶乘计算状态。\n之后会通过 qemu_cond_signal 函数唤醒在edu设备具象化时创建的 edu_fact_thread 线程，该线程用于阶乘计算。edu_fact_thread 函数在阶乘计算结束后会执行 atomic_and(\u0026amp;edu-\u0026gt;status,~EDU_STATUS_COMPUTING)，改变edu设备的status。\n之后 edu_fact_thread 函数会检查 status 和 EDU_STATUS_IRQFACT(0x80) 与运算的结果，等价于检查 status 的第7位是否为1。若为1，代表edu设备被设置为执行完一次阶乘后需要发送中断，此时 edu_fact_thread 函数会调用 edu_raise_irq 函数向虚拟机发送中断。\nedu_fact_thread 函数代码如下：\nqemu-4.1.1/hw/misc/edu.c\n0x20(RW)：0x20权限为可读可写。读0x20时会返回edu设备的 status。对0x20写时，会将写入数据第7位的值赋给 status 的第7位，用于决定每次阶乘后是否向虚拟机发送中断。\n0x24(RO)：0x24权限为只读。对0x24读时会返回edu设备的 irq_status，代表中断产生的原因。驱动中的中断处理程序可以通过读0x24来获取 irq_status。\n0x60(WO)：0x60权限为只写。向0x60写数据时，会调用 edu_raise_irq 函数，edu_raise_irq 函数通过 pci_set_irq 接口向虚拟机发送中断，同时会把 irq_status 和写入数据的或运算结果赋值给 irq_status。具体代码如下：\nqemu-4.1.1/hw/misc/edu.c\n0x64(WO)：0x64权限为只写。0x64用于中断应答，将中断在 irq_status 中清除，停止生成该中断。向0x64写数据时，会调用edu_lower_irq 函数。edu_lower_irq 函数会把写入的数据取反后和 irq_status 进行与运算，并将最终结果赋值给 irq_status。通常驱动中的中断处理程序向0x64端口写入的值为 irq_status，这样便可以将 irq_status 的值置零。具体代码如下：\nqemu-4.1.1/hw/misc/edu.c\n0x80(RW)：0x80的权限为可读可写。读0x80时，会返回edu设备的 dma.src。对0x80写时，会将写入数据赋值给edu设备的 dma.src。dma.src 代表DMA的源地址。\n0x88(RW)：0x88的权限为可读可写。读0x88时，会返回edu设备的 dma.dst。对0x88写时，会将写入数据赋值给edu设备的 dma.dst。dma.dst 代表DMA的目的地址。\n0x90(RW)：0x90的权限为可读写。读0x90时，会返回edu设备的 dma.cnt。对0x90写时，会将写入数据赋值给edu设备的 dma.cnt。dma.cnt 代表DMA传输的字节数。\n0x98(RW)：0x98的权限为可读可写，被用作DMA命令寄存器。\n第0位为1代表开始DMA传输； 第1位决定DMA数据传输的方向： 0代表从RAM到edu设备； 1代表edu设备到RAM； 第2位决定是否在DMA结束之后向虚拟机发起中断，并将 irq_status 设置为0x100。 为edu设备添加设备驱动 本节将介绍如何在虚拟机中为edu设备添加相应的设备驱动，并设计测试程序使用edu设备。\n当加载edu设备驱动模块时，PCI总线会遍历总线上已经注册的设备，调用总线的match函数判断是否有匹配的设备，匹配的依据是驱动提供的pci_device_id。edu设备源码中定义的 vendor_id 和edu设备id会被加入驱动代码中的 pci_device_id 数组 pci_ids[] 中，以实现**驱动和edu设备的匹配。**具体代码如下：\nedu_driver.c\n为edu设备驱动编写 file_operations 中的 write 函数和 read 函数可以按照PCI设备驱动编写的一般方法，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 static ssize_t edu_read(struct file *filp, char __user *buf, size_t len, loff_t *off) { ssize_t ret; u32 kbuf; if (*off % 4 || len == 0) { ret = 0; } else { kbuf = ioread32(mmio + *off); if (copy_to_user(buf, (void *)\u0026amp;kbuf, 4)) { ret = -EFAULT; } else { ret = 4; (*off)++; } } return ret; } static ssize_t edu_write(struct file *filp, const char __user *buf, size_t len, loff_t *off) { ssize_t ret; u32 kbuf; ret = len; if (!(*off % 4)) { if (copy_from_user((void *)\u0026amp;kbuf, buf, 4) || len != 4) { ret = -EFAULT; } else { iowrite32(kbuf, mmio + *off); } } return ret; } 由于edu设备的特殊性，edu设备驱动需要为edu设备设计专门的中断处理函数与 probe 函数以及用于控制edu设备的多种功能的 ioctl 函数。\npci_probe 当驱动和设备完成匹配之后会调用 probe 函数执行设备的相关初始化工作。\npci_probe 函数中首先使用 register_chrdev 函数来注册edu设备，第一个参数为0代表使用系统动态分配的主设备号。 pci_iomap 函数会返回用于表示edu设备的PCI BAR的I/O地址空间的 __iomem 类型指针。后续 iowrite* 和 ioread* 函数会通过获得的__iomem 地址对edu设备的MMIO区域进行读写。 pci_probe 函数最后会向内核注册edu设备的中断服务函数 irq_handler，该函数是一个回调函数。当中断注入虚拟机时会调用 irq_handler 函数，并将设备号传递给它。具体代码如下： edu_driver.c\nirq_handler pci_probe 函数注册了 irq_handler 函数用于中断处理。该函数首先根据主设备号判断中断是否属于edu设备，之后 irq_handler 函数会读取edu设备的 irq_status 并判断产生该中断的原因。**为了区分DMA读中断、DMA写中断以及阶乘运算产生的中断，edu设备源码需要被修改。**具体改动如下：\n当产生DMA读中断时会将edu设备的 irq_status 设置为0x100； 当产生DMA写中断时会将 irq_status 设置为0x101； 当 irq_status 等于0x1时代表阶乘运算中断。 打印出edu设备中断的原因后，irq_handler 函数会调用 iowrite32 函数，将 irq_status 写入上文提到的x64寄存器，以此向edu设备发送一个中断应答。edu设备会按照上文介绍的方式将 irq_status 置零，并拉低中断线电平。具体代码如下：\nedu_driver.c\nedu_ioctl edu设备的特殊功能较多，为了使代码结构更清晰，需要设计 edu_ioctl 函数对edu设备的特性进行控制。用户程序的 ioctl 函数与驱动层的edu_ioctl 函数配合，实现向设备传递控制命令。\nioctl 函数的cmd参数具有以下五种控制命令，每条命令分别控制edu设备的一项功能：\nDMA_WRITE_CMD 代表发起一次DMA写操作，主要设置DMA的源地址、DMA的目的地址、DMA传输的数据长度以及edu设备定义的DMA组合指令。DMA_CMD|DMA_TO_MEM|DMA_IRQ 这一组合指令代表进行DMA写，并且在DMA结束后向虚拟机发送中断。 DMA_READ_CMD 代表发起一次读操作，过程与 DMA_WRITE_CMD 类似。 PRINT_EDUINFO_CMD 代表打印edu设备的基本信息，包括edu设备MMIO区域的大小、配置空间前64字节的信息、edu设备申请的硬件中断号、MMIO区域部分初始化的值。 SEND_INTERRUPT_CMD 命令会写0x60寄存器，此时edu设备会发送中断，并将 irq_status 设置为0x12345678。 FACTORIAL_CMD 命令代表发起一次阶乘运算，首先edu驱动会向 0x20 寄存器写入值0x80，这步的作用是设置edu设备在阶乘结束后发送中断。之后用于阶乘运算的值会被写入0x8寄存器，实验中用于阶乘计算的值是10。 edu_ioctl 代码如下：\nedu_driver.c\n整体实验流程 为了更好地展示实验结果，本节设计了一个简单的用户态测试程序，并在edu设备源码的关键位置添加了相应的输出信息。\n实验第一步是在QEMU中启动带有edu设备的虚拟机，本次实验的启动参数如下：\nboot.sh\n进入虚拟机后，在终端输入 lspci 命令，根据edu的设备号以及 vendor ID 在PCI设备列表中可以查询到edu设备被挂载到了0号总线的04号槽：\nlspci\n接着输入 lspci-s 00:04.0-vvv-xxxx 命令，会显示edu设备的基本信息，包括edu设备的中断信息、MMIO地址空间信息以及设备配置空间信息等。\nlspci-s 00:04.0-vvv-xxxx\n上文介绍了 edu_mmio_read 函数的回调过程，所以实验的第一步首先对 edu_mmio_read 函数的调用过程进行验证。具体过程如下：\n在QEMU中启动虚拟机后，打开另外一个超级终端，通过 ps-aux|grep qemu 命令来查询QEMU的进程号； 之后在终端中启动GDB调试，在GDB命令行中输入 attach + QEMU进程号，调试正在运行的QEMU程序； 接着在GDB中为 edu_mmio_read 函数设置断点，输入 c 继续执行QEMU程序； 然后在虚拟机中加载edu设备的驱动模块，此时GDB会显示QEMU的执行停在了 edu_mmio_read 函数处。 GDB中提供了 backtrace 命令用于查看函数的调用栈。最后输入 backtrace 命令，GDB会显示以下结果：\nGDB backtrace\nbacktrace 的输出结果展示了地址转换的过程，我们可以将该过程与内存虚拟化文档中介绍的QEMU中内存地址转换过程对比。经过对比可以发现，GDB中显示的 edu_mmio_read 函数的函数调用栈与上节描述的一致。\n然后将在GDB中设置的断点取消，继续运行QEMU进程。虚拟机随后运行先前编写的用户态测试程序，虚拟机的 dmesg 输出显示edu设备的配置空间信息与 lspci-s 00:04.0-vvv-xxxx 打印的结果一致。以下是 dmesg 中的部分信息：\ndmesg\ndmesg 的最后几条输出信息展示了edu功能的执行结果。驱动首先接收到了 irq_status 为0x12345679的设备中断，irq_status 与 pci_ioctl 函数中设置的一致。QEMU输出了以下对应信息，包括 irq_status 的设置以及中断状态清除等。\nLog in QEMU\n然后驱动再次接收到了 irq_status 为0x1的设备中断，并判断该中断为阶乘计算产生的，最后输出了阶乘计算的结果 0x375f00。在QEMU中的信息展示了设置设备状态、分配阶乘对象 fact 以及发出阶乘运算中断的过程。\nLog in QEMU\n紧接着测试程序会发起DMA命令，会在edu设备中设置与DMA相关的信息。edu设备的 edu_timer 定时器检测到 dma.cmd 被设置为可运行状态后，会先根据 dma.cmd 第1位的值判断出DMA数据的传输方向，之后会检查DMA操作是否越界，如果未越界，则将DMA信息传入pci_dma_read/pci_dma_write 函数发起DMA操作。\npci_dma_read/pci_dma_write 函数的返回值用于判断DMA操作是否成功完成。当DMA操作完成后，edu设备会返回相应的DMA中断。如下QEMU中的输出信息展示了这一系列过程。\nLog in QEMU\nLog in QEMU\n4 //TODO: GiantVM中的I/O处理 ","date":"2024-10-17T10:06:54+08:00","permalink":"https://zcxggmu.github.io/p/kvmbook-io_4/","title":"Kvmbook Io_4"},{"content":"[toc]\n1 概述 内存是计算机系统中的重要部件。在任何广泛使用的计算机体系结构中，CPU若没有内存提供指令、保存执行结果，则无法运行，CPU从设备读取的数据也将无法保存。由DRAM（Dynamic Random Access Memory，动态随机访问存储器）芯片组成的内存也是存储器层次结构中重要的一环，其速度虽然慢于SRAM（Static Random Access Memory，静态随机访问存储器），但因为价格更低廉，可以获得更大的容量。\n在当前的生产环境中，内存容量已经达到了TB级别。随着大数据时代的到来，大量数据需要保存在内存中进行处理，才能够获得更低的延迟，从而缩短用户的等待时间，提升用户体验。云时代的到来为计算机内存管理提出了新的挑战：**需要为客户机提供从0开始的、连续的、相互隔离的虚拟“物理内存”，并且使内存访问延迟低，接近宿主机环境下的内存访问延迟。**这就是本文主要介绍的内容，即如何高效地实现内存虚拟化。\n广义的内存虚拟化不仅包括硬件层面的内存虚拟化，还包含更多意义上的内存虚拟化。内存虚拟化即给访存指令提供一个内存空间，或称为地址空间。地址空间必须是从0开始且连续的，可以形象地看作一个大数组，通过从0开始的编号访问其中的元素，每个元素储存了固定大小的数据。由低层向高层、由简单到复杂，各类地址空间可以概括为：单机上的物理地址空间、单/多机上的虚拟地址空间、单机上的“虚拟”物理地址空间、多机上的“虚拟”物理地址空间，下面将依次介绍。\n单机上的物理地址空间\n对于一条访存指令，若系统中没有开启分页模式，在不考虑开启分段模式的情况下，这条指令可以访问全部的物理内存。指令访问的PA（Physical Address，物理地址）是从0开始且连续的。在计算机启动之初，BIOS探测到主板内存插槽上的所有内存条，并给每个内存条赋予一个物理地址范围，最后给CPU提供一个从0开始的物理地址空间。从而每个内存条都被映射到一个物理地址范围内，软件无须知道自己访问的是哪个内存条上的数据，使用物理地址即可访问内存条上存储的数据。物理地址隐藏了内存条的相关信息，给系统提供了从0开始且连续的物理地址空间抽象，是一种虚拟化，如下图(a)所示：\n除了内存条被映射到物理地址空间内，也有一些外围设备 (Peripheral Devices) 的内存和寄存器被映射到物理地址空间内。当CPU发出的访存指令的物理地址落在外围设备对应的物理地址范围内，则会将数据返回给CPU，这称作内存映射I/O MMIO。如果只有一层物理地址空间，则系统中只能运行一个进程，无法对CPU分时复用。\n单/多机上的虚拟地址空间\n为了实现CPU的分时复用，操作系统提供了进程的概念。一个进程即是一串相互关联、完成同一个任务的指令序列。为了使不同进程的访存指令在访问物理内存时不会相互冲突，VA（Virtual Address，虚拟地址）的概念被引入。每个进程都有一个独立的虚拟地址空间，从0开始且连续，VA到PA的映射由操作系统决定。这样，假设操作系统为进程A分配了第0块和第2块的物理内存，为进程B分配了第1块和第3块的物理内存，而两个进程的虚拟地址空间大小均为4。\n本文使用抽象的 “块” 作为虚拟内存和物理内存之间映射的基本单位，即抽象层间映射的基本单位。两个进程都可以使用虚拟地址0访问它们拥有的第0块虚拟内存，而不会引起访问冲突，如上图(b)所示。由于VA到PA的映射由操作系统决定，因此操作系统可以巧妙地设置该映射，使得系统中的多个进程以一种低内存占用的方式运行。假设进程A使用的第2块物理内存和进程B使用的第3块物理内存保存的数据相同，那么操作系统可以选择将进程A、B的第1块虚拟内存同时映射到系统中的第2块物理内存，并释放第3块物理内存，减少物理内存占用。减少物理内存占用的方法还有将虚拟内存块对应的物理内存换出到磁盘上，而不改变虚拟内存的抽象层，这种方式称为页换出。\n这就是抽象层提供的一个好处：给上层应用提供一个不变的 “虚拟” 内存，而灵活地改变其 “后台” 实现。虚拟内存甚至可以建立在多个物理内存硬件上，从而实现内存资源的聚合。如上图(c)所示，这种架构称为DSM（Distributed Shared Memory，分布式共享内存），它可以使单机进程无修改地运行在M0和M1上（M代表Machine），后文将介绍其原理，以及如何被用于实现分布式虚拟机监控器GiantVM。除了横向扩展内存虚拟化的概念，即增加物理内存的量，还可以增加抽象层的层级数。\n单机上的 “虚拟 ”物理地址空间\n如果保持物理地址空间的概念不变，继续更改物理地址空间的后台实现将会产生什么概念？ 一个很容易想到的想法是用虚拟内存代替物理内存条作为物理地址实现的后台。而物理内存可以提供给一整台机器使用，于是产生了内存虚拟化的概念。\n假设进程 A 运行在物理硬件上，它提供 4 块虚拟的物理内存给客户机A使用，分别对应其 0、1、2、3 块虚拟内存。客户机A在此 “虚拟” 物理内存的基础上，继续提供虚拟内存的抽象，将第0、1块物理内存分别给客户机中运行的进程A1、B1使用，分别映射到进程A1的第0块虚拟内存、进程B1的第1块虚拟内存。如下图所示，客户机进程A和普通进程B并无差别。\n1 // 图3-2 这里产生了GVA、GPA、HVA和HPA四层地址空间，其中**HVA到HPA的映射仍然由宿主机操作系统决定，GPA到HVA的映射由Hypervisor决定，而GVA到GPA的映射由客户机操作系统决定。**这样的 “虚拟” 物理地址抽象有什么可利用之处呢？\n首先，这改变了对于访存指令的固有认知，即访存指令不一定访问真实的物理内存。只要保证虚拟硬件的抽象和原有的物理硬件相同，系统软件就可以按需灵活地更改抽象层的后台实现。如果真实的物理硬件需要更换维修，那么虚拟机热迁移可以将客户机迁移到另一个机器上，而不会由于更换硬件停止虚拟机的运行。这是由于 “虚拟” 物理内存的抽象没有改变，客户机将不会感知到它所依赖运行的硬件发生了变化。其次，根据前文对单机虚拟内存的描述，多个进程的虚拟内存总量可以超过系统上装备的物理内存总量。类似的，在内存虚拟化中，“虚拟” 物理内存的总量可以超过真实的物理内存总量，即内存超售。\n多机上的 “虚拟 ”物理地址空间\n大数据环境下的应用都会占用大量的物理内存。单个机器渐渐无法满足大数据处理任务运行过程中所需要的内存空间。于是，人们将关注点从高配置的单机纵向扩展 (Scale-up) 转向了数量较大的单机横向扩展 (Scale-out) 。为了使大数据应用运行在多个机器上，而掩盖掉网络通信、容错等分布式环境下需要额外注意的复杂度，大数据框架被开发出来，如Spark、Hadoop等。但这些框架仍然有陡峭的学习曲线，程序员需要学习其复杂的编程模型才能在分布式框架上编写代码。\n如果存在一个SSI（Single System Image，单一系统镜像），即在多个节点组成的分布式集群上给程序员提供一种单机的编程模型，则会极大地提高分布式应用的开发效率，彻底掩盖分布式系统的复杂性，无须学习分布式框架的编程模型。若把前文DSM的思想用于实现 “虚拟” 物理内存，将获得一个容量巨大的 “虚拟” 物理内存。DSM在多个物理节点之上建立了一个 “虚拟” 物理内存的抽象层，如下图所示，M0、M1分别配备4块物理内存。\n仿照单机上的 “虚拟” 物理内存，此处仍由宿主机的VM进程A、B为跨界点客户机VM提供 “虚拟” 物理内存。于是，VM拥有了8块物理内存，其后台实现是两个物理机的虚拟内存。这被称为“多虚一”，即多个节点共同虚拟化出一个虚拟机。DSM保持了 “虚拟” 物理内存的抽象层不变，任何一个操作系统均可运行在这样的抽象之上，和运行在真实物理硬件上没有差别。DSM抽象层的后台实现将在后文中介绍。\n2 内存虚拟化的实现 下面首先介绍虚拟地址的实现，再介绍内存虚拟化的实现。最后再聊聊建立多机上内存抽象的方法。\n2.1 谈谈页表 系统使能MMU后，所有访存指令提供的地址均为VA，还需要将VA转换为PA，才能从真实的内存硬件中获取数据。如何实现VA到PA的转换？ 一个简单的想法是：将每个VA和PA的对应关系记录在一个表中，使用VA查询该表即可找到对应的PA。这样的映射表会占用大量内存，故在现代操作系统中，虚拟内存和物理内存被分为4KB页，映射表中只记录 VFN（Virtual Frame Number，虚拟页号）对应的 PFN（Physical Frame Number，物理页号），映射表表项数量减少为原来的1/4096。映射表记录了虚拟页与物理页之间的映射，于是得名PT（Page Table，页表），其表项称为 PTE（Page Table Entry，页表项）。为了进一步减少 PTE 数量，也可以使用2MB/1GB大小的页，即大页。\n所有系统软件的设计都追求时间尽可能短、空间占用尽可能少，地址翻译系统的设计也一样。事实上，虚拟地址空间十分庞大，应用程序不可能在短时间内访问大量的虚拟内存，而是多次访问某些范围内的虚拟内存，即存在空间局部性。基于这一观察，**操作系统设计者将页表组织成基数树，或称为多级页表，**可以使未使用的查询表项不出现在内存中，大大减少内存占用。在32位架构（如x86）中，操作系统使用 10+10 形式的二级页表，用前10位索引一级页表，后10位索引二级页表。而在64位架构（如x86-64、ARMv8-A）中，操作系统使用 9+9+9+9 形式的四级页表，如下图所示：\n其中页表的每一级分别用PML4（Page Map Level 4，第4级页映射）、PDPT（Page Directory Pointer Table，页目录指针表）、PD（Page Directory，页目录）、PT表示，查询一次页表需要4次内存访问。在32位系统中，当一个应用程序连续访问了连续的4MB虚拟内存，使用 10+10 的二级页表则仅需要1个一级页表页和1个二级页表页，页表占用内存大小为8KB。而使用一级页表则需要4MB内存，仅仅是查询页表时多了一次内存访问，内存占用就减小为原来的 8KB/4MB=1/512，这是十分划算的。\n查询页表不应该由应用程序负责，因为这对于应用程序完成自己的工作是无意义的，并且由应用程序负责修改页表会产生虚拟内存泄漏的问题，虚拟内存的隔离性将不复存在，造成安全问题。于是，CPU芯片设计者引入了MMU（Memory Management Unit，内存管理单元）负责查询页表。每个CPU核心上都配备了一个独立的MMU，只要CPU将页表的基地址放入一个指定的寄存器中，MMU中的PTW（Page Table Walker，页表爬取器）即可查找页表将CPU产生的VA自动翻译成PFN，左移12位后与VA的低12位相加即得到PA，不需要CPU执行额外的指令。这一指定的寄存器在Intel体系中是CR3，在ARM体系中是TTBRx（Translation Table Base Register x，翻译表基地址寄存器x）。这些架构下的地址翻译原理大致相同，故本文统一使用ptr（pointer，指针）代表页表基地址。\n同时，MMU硬件也通过PTE上的标志位实现了访存合法性的检查，以及内存访问情况的记录。Intel体系下的PTE标志位详见Intel SDM卷3第4章，其他体系有相应的手册。此处列举如下x86架构中常用的标志位:\nP (Present) 位。PTE[0]，置为1时该PTE有效，为0时该PTE无效。任何访问该PTE对应的虚拟地址的指令均引起缺页异常。当物理页未分配、页表项未建立（此时PTE的每一位均为0）或物理页被换出时（此时PTE的每一位不全为0），P位为0，物理页不存在； R/W位和U/S位。PTE[1∶2]，置为1时表示该页是可读可写的，为0时表示该页只读。U/S位置为0时只有管理者（supervisor，即运行在Ring0、Ring1、Ring2的代码）可以访问，为1时用户（user，即运行在Ring3的代码）、管理者均可访问。这两个位用于权限管理； A(Accessed)位和D(Dirty)位。PTE[5∶6]，当CPU写入PTE对应的页时D位被置为1，当CPU读取或写入PTE对应的页时A位被置为1，这两位均需要操作系统置0。 D位仅存在于PTE中，而不存在于PDE（Page Directory Entry，页目录项）中。D位用于标识一个文件映射的页在内存中是否被修改，在将页换出时需要更新对应的磁盘文件； A位用于标识内存页是否最近被访问。当操作系统将A位置为0后一段时间内，若A位不再变为1，则对应的内存页不经常被访问，可以被换出到磁盘。操作系统有一套内存页换出机制，将不经常访问的内存页换出到磁盘，保证内存被充分使用。 **当MMU硬件无法完成地址翻译时，则需要操作系统软件的配合。**在地址翻译的过程中，MMU硬件仅负责页表的查询，而操作系统负责页表的维护。当MMU无法完成地址翻译时，就会向CPU发送一个信号，产生了缺页异常（在x86架构中是14号异常），从而调用操作系统内核的缺页异常处理函数。内核按照其需要为产生缺页异常的VA分配物理内存，建立页表，并重新执行引起缺页异常的访存指令；如果该访存指令访问的虚拟内存被换出到磁盘上，则需要首先分配物理内存页，再对磁盘发起I/O请求，将被换出的页读取到新分配的物理内存页上，最后建立对应的页表项。操作系统可以灵活地利用缺页异常，实现内存换出、COW（Copy-On-Write，写时复制）等功能。\n缩短时间的一种重要方式是缓存。在MMU中，TLB（Translation Lookaside Buffer，翻译后备缓冲器）用于缓存VA到PA的映射，避免查询页表造成的内存访问。然而，内存容量随着技术的进步快速增大，TLB容量的增速却很缓慢，这导致TLB能够覆盖的虚拟内存空间越来越小。研究表明，应用运行时间的 50% 都耗费在查询页表上。**前文提到的大页可以在一定程度上解决这一问题，但不够灵活。**Vasileios等提出了区间式TLB机制，每个区间TLB项可以将任意长度的虚拟内存区间映射到物理内存，该长度由操作系统决定，进一步减少了PTE的数量。这两种方式使得TLB能够覆盖更多的虚拟地址空间，减少了查询页表的次数。\n在进程切换时，由于进程运行在不同的虚拟地址空间中，需要将TLB中的所有项清空，否则地址翻译将出错。x86/ARM硬件提供了PCID（Process Context IDentifier，进程上下文标识符）/ASID（Address Space IDentifier，地址空间标识符），将TLB中的项标上进程ID，于是在进程切换时无须清空TLB。详见Intel SDM卷3第4.10节。\n2.2 内存虚拟化的纯软件实现: 影子页表 在新系统设计的过程中，复用已有设计可以加快设计的流程，降低设计的难度。是否可以重用CPU芯片上的MMU从而实现GVA到HPA的翻译呢？ 客户机操作系统运行在非根模式下，将客户机页表起始地址GPA写入ptr，这样MMU只能完成GVA到GPA的翻译。将页表修改为GVA到HPA的映射，MMU就能将GVA翻译为HPA，并且对客户机完全透明，从而完成问题的转化。使用影子页表翻译客户机虚拟地址的流程如下图所示：\n具体操作过程是：\n当vCPU创建时，Hypervisor为vCPU准备一个新的空页表。当vCPU将GPT的GPA写入ptr时，引起一次VM-Exit，vCPU线程退出到Hypervisor中，调用处理ptr写入的处理函数； Hypervisor中保存了GPA到HVA的映射，处理函数读取客户机试图写入的ptr值，翻译为HVA，即可读取GPT的内容。进而，Hypervisor遍历GPT，并将每个GPT表项中保存的GPA翻译为HVA，再使用宿主机操作系统内核翻译为HPA，最后将HPA填入新页表SPT的GVA处； 完成GPT的遍历后，就建立了对应的新页表SPT，Hypervisor的处理函数最终将该新页表的基地址写入ptr中，并返回到vCPU重新执行引起VM-Exit的指令。见上图中标号①； GPT的所有页表页被标记为只读，客户机写入GPT引发VM-Exit，并调用Hypervisor的相关处理函数，把对GPT的修改翻译为对SPT的修改，完成新页表与GPT之间的同步，见图3-5中标号②。 上述过程中出现的新页表称为影子页表，因为对于每个GPT，都需要对应的SPT作为代替，且SPT与GPT的结构完全相同，其不同仅仅是每个表项中的GPA被修改为了对应的HPA，如同影子一样。影子页表查询的结果是HFN（Host Frame Number，宿主机页号），左移12位后与GVA的低12位相加即得到对应的HPA。\n和非虚拟化场景下的页表相同，MMU硬件可以自动查询SPT，将CPU产生的GVA直接翻译为HPA，也有TLB保存GVA到HPA的映射以加快地址翻译。虚拟内存的优势也可应用在VM进程的内存管理上，如宿主机操作系统可以将VM进程不常用的内存页换出到磁盘，两个客户机之间也可以通过页表的权限实现简单的隔离，还可以通过将两个内存插槽对应的HVA映射到相同的HPA，应用COW技术实现客户机之间的内存共用，从而节约系统的物理内存，实现内存超售。\n为了将GPT翻译为SPT，需要利用宿主机中保存的GPA到HPA的映射。由于HVA到HPA的映射由宿主机页表保存，Hypervisor仅需关注GPA到HVA的映射。事实上，在Hypervisor (如KVM) 中，内存插槽 (kvm_memory_slot) 数据结构将一段GPA一对一映射到HVA，用宿主机的虚拟内存作为虚拟的 “内存插槽” 给客户机使用。内存插槽在HVA中的位置可以不从0开始且不连续，但Hypervisor使用多个内存插槽可以给客户机提供一个从0开始且连续的物理地址空间。客户机内存插槽的设计在后文中有详细介绍。\n影子页表的缺陷：\n由于Hypervisor需要为每个客户机中的每个进程维护一个独立的SPT，系统中GPT的数目等于SPT的数目，这将占用大量内存； 当GPT被修改时，由于需要保持SPT与GPT的同步，vCPU线程需要VM-Exit将控制权转让给Hypervisor，由Hypervisor根据GPT的修改来修改SPT，同时将TLB中缓存的相关项清除。于是，每次客户机对GPT的修改都将引起巨大的性能开销，只有当客户机很少修改GPT时，SPT才会表现出较好的性能。 2.3 内存虚拟化的硬件支持: 扩展页表 仔细回顾虚拟化环境下的地址翻译，可以发现GPA到HPA的转换一步可以从影子页表中剥离出来:\n当客户机创建时，宿主机给客户机使用的内存已经分配完毕，在客户机的运行过程中，很少改变GPA到HPA的映射； 其次，SPT和GPT也包含重复的信息，即等价于包含了两次GVA到GPA的映射； 两个进程的SPT之间也包含重复的信息，即重复多次包含了GPA到HPA的映射，故增加了页表维护的复杂度并增大了内存开销。 于是，系统设计者将GPA到HPA的映射从影子页表中剥离出来，形成一个新的页表，配合GVA到GPA映射的页表(GPT)共同完成地址翻译。\n虚拟环境下的地址翻译依赖于双层页表 (Two Level Paging) 。GPA到HPA映射的页表在 x86 中称为EPT（扩展页表），在ARM中称为第二阶段页表 (Stage-2 Page Table)，而GPT称为第一阶段页表 (Stage-1 Page Table)，一、二阶段页表基地址分别保存在 TTBR0_EL1 和 VTTBR_EL2 中。\n本文使用gptr（guest pointer，客户机指针）表示GPT基地址寄存器，HPT（Host Page Table，宿主机页表）表示被剥离出的页表；hptr（host pointer，宿主机指针）表示HPT基地址寄存器。\n每个客户机有一个私有的HPT，包含与GPT完全不重复的信息。由于HPT与GPT之间没有依赖关系，修改GPT时无须修改HPT，即无须Hypervisor干预，从而减少了客户机退出到Hypervisor的次数。\nARM与Intel VT都提供了类似的双层页表支持，这里只介绍Intel VT中提供的支持，后文统一使用EPT表示保存了GPA到HPA映射的页表。Intel VT提供了具有交叉查询GPT和EPT功能的扩展MMU。当VM-Entry时，EPTP（EPT Pointer，扩展页表指针，即EPT的基地址）将由Hypervisor进行设置，保存在VM-Execution控制域中的EPTP字段中。需要注意的是，每个客户机中的所有vCPU在运行前，其对应的VMCS中的EPTP都会被写入相同的值。这是因为所有的vCPU应该看到相同的客户机物理地址空间，即所有vCPU共享EPT。一个客户机仅需一个EPT，故减小了内存开销。\nEPT表项的构成较为简单，其第0、1、2位分别表示了客户机物理页的可读、可写、可执行权限，并包含指向下一级页表页的指针(HPA)。当EPTP（存在于VMCS中）的第6位为1时，会使能EPT的A/D（Accessed/Dirty，访问/脏）位。EPT中的A/D位和前文所述的进程页表的A/D位类似，D位仅存在于第四级页表项，即PTE中。A/D位由处理器硬件置1，由Hypervisor软件置0。\n每当EPT被使用时，对应的EPT表项的A位被处理器置1； 当客户机物理内存被写入时，对应的EPT表项的D位被置1。 需要注意的是，**对客户机页表GPT的任何访问均被视为写，GPT的页表页对应EPT表项的D位均被处理器硬件置1。**该硬件特性将在本文中多处提及，可用于实现一些软件功能，也可选择关闭该特性，例如可以用此硬件特性实现虚拟机热迁移。详见Intel SDM卷3第28.2.4节。\n和客户机操作系统中的GPT类似，EPT也是在缺页异常中由Hypervisor软件建立的，具体过程如下：\n当刚启动的客户机中的某进程访问了一个虚拟地址，由于此时该进程的一级页表（GPT中的PML4）为空，故触发客户机操作系统中的缺页异常； 客户机操作系统为了分配GPT对应的客户机物理页，需要查询EPT。此时，由于EPT尚未建立，客户机操作系统就退出到了Hypervisor。当客户机操作系统访问了一个缺失的EPT页表项，处理器产生EPT违例(EPT Violation)的VM-Exit，从而Hypervisor分配宿主机内存、建立EPT表项； 触发EPT违例的详细原因会被硬件记录在VMCS的VM-Exit条件 (VM-Exit Qualification) 字段，供Hypervisor使用。宿主机操作系统完成宿主机物理页分配，建立对应的EPT表项，将返回到客户机操作系统； 客户机操作系统继续访问GPT的下一级页表(PDPT)，重复步骤1、2，GPT和EPT的建立方可完成。 这里又出现了软硬件的明确分工: **软件维护页表，硬件查询页表。**如果访问了EPT中的一个配置错误，不符合Intel规范的表项，处理器会触发EPT配置错误 (EPT Misconfiguration) 的VM-Exit。例如访问了一个不可读但可写的表项，此时硬件将不会记录发生EPT配置错误的原因，这类VM-Exit被Hypervisor用于模拟MMIO。有关EPT硬件的详细内容请参阅Intel SDM卷3第28章，Hypervisor软件部分的介绍见本文的第三章。\n当处在非根模式下的CPU访问了一个GVA，MMU将首先查询GPT。gptr包含客户机页表的起始地址GPA，这会触发MMU交叉地查询EPT，将gptr中包含的GPA翻译成HPA，从而获取GPT的第一级表项。同理，为了获取GPT中每个层级的页表项，MMU都会查询EPT。\n在64位的系统中，Hypervisor使用GPA的低48位查询EPT页表，而EPT页表也使用了 9+9+9+9 的四级页表的形式。假设GPT也是四级页表，那么非根模式下的CPU为了读取一个GVA处的数据，需要额外读取24个页表项（图中加粗黑框的灰色长方形），因此需要额外的24次内存访问，如下图所示：\n即使MMU中有TLB缓存GVA到HPA的映射，TLB也无法覆盖越来越大的客户机虚拟地址空间，双层页表的查询将会造成巨大的开销。而在TLB未命中时，一个四级影子页表仅需访问 4 次内存即可得到HPA，相比于双层页表有巨大的优势。是否可以将影子页表与扩展页表相结合呢？\n2.4 //TODO 扩展页表与影子页表的结合: 敏捷页表 在系统设计中，经常存在着各种各样的折中与权衡。虽然影子页表和双层页表（即x86中的扩展页表）在TLB命中时，均可以以最快的时间获得GVA到HPA的映射，但遭遇1次TLB未命中时，查询双层页表需要访问内存的次数增长到了24/4=6倍。然而影子页表会引起大量的VM-Exit，尤其是在频繁分配、释放虚拟内存的内存密集型场景下，这使得影子页表在现在的虚拟化环境下很少使用。由于内存容量快速增大、TLB容量增长缓慢，TLB不命中的次数越来越多，查询双层页表造成的6倍内存访问也造成了不可忽视的开销，影子页表有一些优势。表3-1将影子页表与扩展页表进行了对比。\nJayneel等人观察到[插图]，在2秒的采样时间内，仅有1~5%的地址空间被频繁修改，且被修改的地址空间会比未修改的地址空间修改得更加频繁。例如，保存代码的地址空间极少被修改、写入，称为静态区；而地址空间的堆栈以及映射文件的部分则被频繁修改，称为动态区。若用SPT完成静态区的地址翻译，则能减小TLB不命中时的页表查询开销；而用双层页表完成动态区的地址翻译，则能避免GPT频繁修改造成的VM-Exit。于是，研究人员于2016年提出了敏捷页表[插图]。他们设计了一种影子页表和双层页表混用的机制，还有一种策略决定何时由影子页表转换到双层页表或由双层页表转换到影子页表。这是一种机制与策略的分离：机制是固定的，而程序员可以灵活修改策略，具有更好的灵活性。图3-7展示了影子页表与双层页表的混用机制。**为了实现敏捷页表，需要硬件支持以及软件支持，**下面分开介绍。\n硬件支持\n先介绍影子页表与双层页表的混用机制，这部分功能主要使用硬件实现。首先，影子页表需要提供转换位(Switching Bit)的支持。在影子页表的页表项中，仍有一些标志位被硬件忽略，可以放置转换位。硬件增加了sptr（Shadow Pointer，影子指针）寄存器，用于放置影子页表基地址；同时保留原有的ptr寄存器（更名为gptr寄存器），用于放置客户机页表基地址；hptr放置EPT的基地址，用于查询EPT。当查询SPT时读到一个转换位被置为1的影子页表项，则MMU切换到双层页表的地址翻译模式继续交叉地查询GPT和EPT。在切换前后，MMU中的TLB仍然保存了GVA到HPA的映射，无须刷新TLB。在转换位被置为1的影子页表项中，记录了下一级GPT页表页的起始地址。该地址为HPA，转换位被置为1时由硬件写入。通过该HPA即可查询到下一级GPT的页表项，使得页表查询过程继续进行下去。\n在影子页表中，可以将任何一级页表项的转换位置为1，如图3-7(a)中SPT的第三级页表项的转换位置为1，图3-7(b)中SPT的第三级页表项的转换位置为1。在下图(a)中查询3级影子页表，仅额外读取3+1+4=8次页表项（图中的加粗黑框灰色长方形）；图(b)中仅遍历2级影子页表，但要额外读取2+2+8=12次页表项。\n软件支持\n转换位是否置1由Hypervisor决定，这属于策略设计需要关注的。当MMU进入双层页表查询模式后，其读取的GPT表项不再被标记为只读，可以被客户机操作系统写入而不引起VM-Exit。为了利用这一优势，Hypervisor实现了一套策略维护转换位。当客户机进程被创建且调度运行时，MMU处在影子页表地址翻译的状态，GPT被标记为只读。若一个SPT页表项被修改，将发生VM-Exit，Hypervisor记录1次对该SPT页表项的修改并更新SPT，返回客户机。当Hypervisor记录到2次对该SPT页表项的修改时，则将该SPT页表项的转换位置为1，表明该SPT页表项经常被修改，需要切换到双层页表模式。如图3-7(a)所示，当客户机修改了两次sPDE（shadow Page Directory Entry，影子页目录项）时，将sPDE的转换位置为1，那么对于gPTE（guest Page Table Entry，客户机页表项）的修改将不会引起VM-Exit，代价只是由读取4次内存增加到读取8次。\n还需要一套策略决定转换位置0。当客户机页表很少被修改时，如果将MMU地址翻译模式部分切换回SPT的翻译模式，在TLB不命中时可以减少读取页表项的次数。然而，假设此时有对GPT和EPT频繁的只读访问，客户机将不退出Hypervisor，Hypervisor也无法得知何时应该切换回影子页表模式。为此，前文所述的EPT A/D位特性（见Intel SDM卷3第28.2.4节）应该被关闭，因为当EPT A/D位使能时，所有对GPT的访问无论读/写均视为写（注意，除GPT页表页的只读访问并不被视为写），于是GPT所在的客户机物理页对应的EPT表项中脏位（即D位）将被置1。此时Hypervisor可以通过EPT中的脏位观察是否应该回到SPT模式。在一个检查周期开始时，将所有GPT对应的EPT表项脏位置0；在周期结束时，若脏位仍然没有置1，则视为GPT修改不频繁，对应的转换位置为0，切换回双层页表的翻译模式。\n敏捷页表仅是一个设想，所需的硬件支持尚未实现。研究者用仿真工具模拟了敏捷页表的运行，并测得相比于双层页表更低的TLB不命中开销，以及相比于影子页表更少的VM-Exit次数。然而，敏捷页表对进程切换不友好，由于敏捷页表的一级页表使用了影子页表的页表页，在切换sptr时会引起VM-Exit，但有相应的硬件优化，详见参考文献[插图]。\n2.5 //TODO 内存的半虚拟化: 直接页表映射与内存气球 上文所述的内存虚拟化实现均基于一个前提：客户机无从得知自己使用的是“虚拟”的物理内存。如果客户机知道自己运行在“虚拟”的内存硬件上，内存虚拟化的实现是否会更简单？内存虚拟化实现的性能是否更高？本节介绍两个与内存半虚拟化的相关技术。\n直接页表映射\n半虚拟化可以通过告知客户机运行在虚拟环境下，让客户机协同Hypervisor完成虚拟化任务，从而可以使Hypervisor需要完成的工作更少，Hypervisor的实现更为简单。将半虚拟化的思想应用在内存虚拟化上，则Hypervisor有能力告知客户机操作系统：将页表维护成能够直接安装到真实硬件MMU的版本，Hypervisor将不对客户机页表进行任何修改。GPT中将保存GVA到HPA的映射，而Hypervisor需要做的仅仅是告知客户机操作系统可以使用的真实物理内存范围。这样，只需要增加客户机操作系统的一些复杂性，就不需要降低客户机运行性能的影子页表，也不需要复杂的EPT硬件扩展，内存虚拟化的困难减小，性能也得到提高。这种内存虚拟化的实现方式称作直接页表映射。\n然而，由于页表中的映射对于客户机之间的隔离性、系统安全等至关重要，客户机对页表不可随便更改。在客户机更改页表时，只能调用Hypervisor提供的超级调用，由Hypervisor检查客户机映射的内存范围是否合法，才能返回客户机继续执行。相比于影子页表，Hypervisor需要完成复杂的GPT到SPT的翻译，直接映射大大减轻了Hypervisor的负担。为了减小多次非根模式与根模式切换带来的开销，客户机可以选择将多次对GPT的更改组合起来，合并成一次超级调用进入Hypervisor，从而将多次CPU模式切换替换为1次模式切换。虽然Hypervisor进行了GPT修改的合法性检查，但由于客户机明确地知道真实物理硬件的物理地址(HPA)，仍然可以利用HPA发起行锤(Rowhammer)攻击。该攻击具体原理如下：由于DRAM不断发展，厂商将DRAM的单元做得越来越近，而相邻单元的相互影响也越来越大，不断访问某个地址的物理内存，即可造成相邻位置内存位的翻转。若客户机得知了真实的物理内存地址，则可以对不属于自己的相邻物理内存发起行锤攻击。\n内存气球\n根据对SPT原理的介绍，客户机的“虚拟”物理内存的后台实现其实是宿主机进程的虚拟内存，可以使用宿主机虚拟内存的功能管理客户机物理内存。例如，为了实现内存超售，给所有客户机分配的物理内存总量可以大于物理硬件的内存容量。由于抽象层这一概念的存在，系统软件的设计者可以灵活更改“虚拟”物理内存的后台实现，将“虚拟”物理内存对应的宿主机虚拟内存换出到磁盘，或映射到同一块宿主机物理内存，从而减小宿主机物理内存的压力。然而，Hypervisor在决定换出哪块“虚拟”物理内存时，无法精确地得知哪些部分在未来一段时间内不会被客户机使用。即使开启了EPT的A/D位，Hypervisor也仅仅能够得知在过去一段时间内，客户机访问了哪些页、长时间未访问哪些页，而无法得知这些页之间的关联与意义，即所谓的语义鸿沟。这会导致“虚拟”物理内存换出的太多或太少：“太多”会使客户机不断等待“虚拟”物理内存从磁盘换入内存，降低客户机性能；而“太少”则使Hypervisor没有释放那些完全可以被立即释放的“虚拟”物理内存，造成系统内存资源的浪费。\nHypervisor无法实现高效的客户机内存换出策略的原因是：Hypervisor无法得知客户机内部发生了什么。而半虚拟化可以很好地解决该问题，可以使客户机和Hypervisor更好地沟通。内存气球利用了客户机内核提供的物理内存分配函数，来实现客户机内存的高效释放。其主要工作流程是，Hypervisor调用客户机提供的内存释放接口，请求客户机释放其占用的“虚拟”物理内存。客户机收到该请求后，调用其内核提供的物理内存分配函数（如Linux内核中的alloc_pages函数），并把分配好的“虚拟”物理内存范围返回给Hypervisor。Hypervisor可以将该“虚拟”物理内存对应的虚拟内存释放，减轻系统内存压力。由于客户机内核的物理内存分配函数会“自动”找出未被使用的物理内存，因此这种方式很简易地找出了客户机中不被使用的物理内存，大大简化了内存气球的实现。\n3 QEMU/KVM分析 本节将深入分析QEMU/KVM内存虚拟化相关代码，其中：\nKVM代码来自Linux内核v4.19，QEMU代码版本为4.1.1。\n内存虚拟化的核心是使用虚拟内存代替物理内存条，作为 “虚拟” 物理内存的实现 “后台”，从而给客户机提供从0开始且连续的 “虚拟” 物理内存。客户机访存指令提供的地址是GVA，被宿主机MMU翻译成HPA，再发送到物理内存上读取/写入数据。Hypervisor和操作系统维护页表，将页表装载到MMU中，与MMU硬件协同完成内存虚拟化。\n对应到广泛使用的Type II Hypervisor QEMU/KVM中，两者的职能分别是:\nQEMU负责在宿主机用户态分配虚拟内存，作为客户机 “虚拟” 物理内存的后台实现，即完成所有物理内存硬件的功能； 而KVM负责在内核态维护GVA到HPA的映射，即维护页表，并将页表装载到MMU中完成软硬件的配合。 这属于一种策略和机制的分离，其中KVM提供了地址翻译机制，而QEMU决定如何利用KVM的地址翻译机制完成内存虚拟化，实现一套功能完整的内存虚拟化策略。这种分离的好处在于，Hypervisor的编写者可以灵活地变更策略的实现，而机制无须修改。下面分别对QEMU的物理内存模拟和KVM的页表维护进行分析。\n3.1 QEMU内存数据结构 为了正确地运行客户机，**QEMU需要模拟物理地址空间的所有功能，**包括：\nQEMU作为宿主机上的用户态进程，在宿主机上分配一段虚拟内存提供给客户机作为客户机物理内存使用； QEMU需要**模拟物理地址空间中外围设备对应的MMIO部分，**通过截获对该内存区域的访问，完成对设备功能的模拟，使得客户机像在真实环境中一样完成MMIO。 \u0026ldquo;虚拟\u0026quot;物理内存分配 本节从解决第一个问题开始，即：QEMU进程如何分配虚拟内存作为客户机的物理内存？\n熟悉C语言标准库的读者知道，要分配一段大小不固定的虚拟内存，应该调用 malloc 函数。系统首先分配足够的堆内存给 malloc 函数使用，当分配的堆内存用完时，malloc 函数调用 brk 函数修改内核中的brk指针，增大分配的堆内存。如果 malloc 函数请求的内存大小超过128KB，则会调用 mmap 系统调用在虚拟内存的内存映射区而非堆上分配内存。\n由于QEMU需要给客户机分配较大块的虚拟内存作为 “虚拟” 物理内存，故QEMU选择使用 mmap 函数。mmap 函数建立的虚拟内存映射根据分配的虚拟内存是否关联到磁盘文件分为文件映射和匿名映射，此处只关注匿名映射。**RAMBlock方便了宿主机虚拟内存的管理，**简称RB，其定义如下:\nqemu-4.1.1/include/exec/ram_addr.h\nhost指针保存了 mmap 函数返回的宿主机虚拟地址 max_length保存了 mmap 函数申请的虚拟内存大小 idstr保存了该RB的名称 mr保存了其所属的MemoryRegion next指向该RB在全局变量ram_list中的下一个RB ram_addr_t类型代表了所有内存条组成的GPA空间 ram_list.blocks 将所有 “虚拟” 物理内存块RB组织在一起，根据max_length从大到小排列，RAMBlock组织结构如下图所示：\n其中，offset是在ram_addr_t地址空间中的偏移，used_length是当前使用的长度，即包含有效数据的长度，max_length是 mmap 函数分配的长度，即最大可以使用的长度。和 mmap 函数的映射类型相同，RB也分为匿名文件对应的类型（其fd为-1）以及磁盘文件对应的类型（如果使用QEMU的-mem-path选项)。qemu_ram_alloc_* 函数族负责分配新的RB，它们最终都调用 ram_block_add 函数填充RB数据结构，代码如下。\nqemu-4.1.1/exec.c\n参数 new_block 表示待填充的RB，流程如下:\n首先调用 find_ram_offset 在全局ram_list.blocks中查找能够容纳下max_length大小RB的位置，并将该位置填入RB的offset中; 然后调用 phys_mem_alloc 函数，它最终调用 mmap 函数从而系统调用完成虚拟内存的分配，并将分配的虚拟内存起始地址填入host成员中。 当new_block完成分配后，还需要将new_block加入ram_list.blocks中，通过 QLIST_INSERT_* 函数完成 ram_list.blocks 将整个虚拟机的所有 “内存条” RB管理起来，形成了ram_addr_t类型的地址空间，表示所有“虚拟”物理内存条在客户机物理地址空间中所占的空间。管理RB的接口一般命名为 qemu_ram_*，见exec.c文件。最终，QEMU调用 qemu_madvise 函数建议对该RB对应的虚拟内存使用大页，根据前文分析，使用大页有助于提高TLB命中率。 ramlist 的类型struct RAMList如下:\nqemu-4.1.1/include/exec/ramlist.h\nram_list将ram_list.blocks封装成struct RAMList数据结构从而方便管理，是exec.c文件中的全局变量，保存客户机所有物理内存条的信息。其成员如下：\nmru_block保存了最近使用的RB，作为查找ram_list.blocks的缓存，无须遍历链表; dirty_memory是整个ram_list.blocks中所有RB的脏页位图，每一位代表了一个脏的物理内存页，而RB的bmap位图是其一部分; 为了模拟VGA（Video Graphics Array，显示绘图阵列，可看作一种设备），QEMU需要重绘脏页对应的界面；为了模拟TCG（Tiny Code Generator，微码生成器，支持QEMU的二进制代码翻译，一种基于纯软件的虚拟化方法），QEMU需要重新编译自调整的代码；对于热迁移，QEMU需要重传脏页。QEMU调用ioctl(KVM_GET_DIRTY_LOG) 函数从KVM中读取脏页位图。\n支持\u0026quot;虚拟\u0026quot;物理内存访问回调函数 实体MR 物理地址空间不仅被内存条所占据，也被外围设备的MMIO区域所占据，QEMU需要对客户机访问MMIO进行模拟。CPU使用PIO访问端口地址空间，QEMU也需要对这类访问进行模拟。**对于这些地址空间段，QEMU无须为其分配宿主机虚拟内存，只需设置对应的回调函数。**为此，QEMU在RAMBlock的基础上加了一层包装，形成了MemoryRegion，简称MR，包含MR和回调函数。**MR代表客户机的一块具有特定功能的物理内存区域，**定义如下:\nqemu-4.1.1/include/exec/memory.h\nQEMU实现了MMIO的模拟。为此，将一个RB和包含了MMIO模拟函数的MemoryRegionOps绑定起来，就形成了MR这种表示多个种类物理内存块的数据结构。当KVM中表示内存条的MR其ops域为NULL时，ram_block不为NULL；而对于表示MMIO内存区的MR，其ops注册为一组MMIO模拟函数时，ram_block为NULL。\n当客户机访问了一个MMIO对应的区域，KVM将退出到QEMU，调用ops对应的函数。ops中包含了read、write等函数，其参数包括相对于MR的hwaddr地址addr、写入的数据以及数据的大小，模拟硬件MMIO读写（例如PCI Device ID（Peripheral Component Interconnect Device IDentifier，外设部件互联设备标识符））的read函数应该返回设备ID。\n至此，QEMU将整个物理地址空间用MR占满，这种既包含内存条区域又包含MMIO区域的物理地址空间的类型是hwaddr。MR的addr域即为hwaddr类型，表示该MR的GPA。\nQEMU对象模型为MR提供了构造函数和析构函数，分别在一个MR实例创建和销毁时调用。在MR的parent_object销毁时，就会调用MR的析构函数。具体为：\nMR创建时调用 memory_region_initfn 函数初始化MR，包括将enable置为true和初始化ops、subregions链表等； 调用 memory_region_ref 函数使MR的parent_obj的引用数加1； memory_region_unref 函数则使MR的parent_obj的引用数减1，若引用计数为0，则会调用memory_region_finalize 函数完成MR的析构，如果是RAM类型的MR，还会释放对应的RB。 QEMU给所有种类的MR都提供了进一步封装的构造函数，根据MR的类型填充数据结构。这些函数是memory_region_init_*，其中 * 代表类型，下面举例介绍。\nRAM类型MR需要调用前文的qemu_ram_alloc函数分配一个RB填入ram_block域，ram域为true；ROM类型MR则需要额外将read_only域置为true，表示只读的内存区； MMIO类型MR负责实现MMIO模拟，需要传入ops进行初始化，其中ops是一组回调函数，当QEMU需要模拟MMIO时，会调用ops中的函数进行MMIO模拟； 对于ROM Device（Read Only Memory Device，只读内存设备）类型MR，对它进行读取则等同于RAM类型的MR，而写入则等同于MMIO类型的MR，调用回调函数ops； IOMMU（Input/Output Memory Management Unit，输入输出内存管理单元）类型MR将对该MR的读写转发到另一MR上模拟IOMMU。 所有的MR构造函数 memory_region_init_* 都要调用 memory_region_init 函数填充size、name等成员。这些MR均称为实体MR，terminates为true。前三类实体MR的创建过程如下图所示：\n容器MR/别名MR 所有的MR组成了一棵树，其叶子节点是RAM和MMIO，而中间节点代表总线（容器MR）、内存控制器（别名MR）或被重定向到其他内存区域的内存区，树根是一个容器MR，如下图所示：\n这棵树表示一个地址空间，被AddressSpace数据结构指向。下面介绍容器MR和别名MR：\n容器 (Container) MR\n容器 (Container) MR将其他的MR用subregions链表管理起来，用 memory_region_init 函数初始化。例如PCI BAR（PCI Base Address Register，PCI基地址寄存器）的模拟由RAM和MMIO部分组成，需要容器类型的MR表示PCI BAR，通过memory_region_add_subregion 函数加入容器MR，子MR的container成员指向其容器MR。\n通常情况下，一个容器MR的子MR不会重叠，当在该容器MR内解析一个hwaddr时，只会落入一个子MR。但一些情况下子MR会重合，这时使用 memory_region_add_subregion_overlap 函数将子MR加入容器MR，而解析地址时由优先级决定哪个子MR可见。\n没有自己的ops/ram_block的容器MR称为纯容器MR，容器MR也可以拥有自己的MemoryRegionOps以及RAMBlock。在一个容器MR中，subregions链表上的所有子MR按照各自的优先级从小到大排列。\nmemory_region_add_subregion_overlap函数可以指定子MR的优先级，如果优先级为负，则隐藏在默认子MR之下，反之在上。\n别名 (Alias) MR\n别名 (Alias) MR指向另一个非别名类型的MR，QEMU通过将多个别名MR指向同一个MR的不同偏移处，从而将此MR分为几部分。alias域指向原MR（在代码中多用orig表示），alias_offset是别名MR在实际MR中的位置。\n别名MR用 memory_region_init_alias 函数初始化。别名MR没有自己的内存，没有子MR。别名MR本质上是一个实体MR（或另一个别名MR）的一部分，而容器MR的子MR并非容器MR的一部分，仅被容器MR管理。别名MR并非容器MR的子MR。\n在QEMU中，全局变量system_memory是整个MR树的根。给定MR树的根MR以及要查询的客户机物理地址，QEMU就可以查找该地址落在哪个实体MR中。首先判断该地址是否在根管理的范围内，如果不在则返回，否则进行如下步骤的搜索：\n按照优先级从高到低的顺序遍历该容器MR的subregions链表； 如果子MR是实体MR，且该地址落在子MR的[mr-\u0026gt;addr，mr-\u0026gt;addr+mr-\u0026gt;size）中，则结束查找，返回该实体MR； 如果子MR是容器MR，则递归调用步骤1； 如果子MR是别名MR，则继续查找对应的实际MR； 如果在所有的子MR中都没有查找到，则查询该地址是否在容器MR自己的内存范围内。 想了解更完整的说明，可以查看QEMU源码树中的 docs/devel/memory.rst 文档，或查阅注释，其中包含MemoryRegion的所有概念，以及MR相关接口的详细解释。\n顶层数据结构AddressSpace 在前几节中，QEMU使用 mmap 函数分配了宿主机虚拟内存，作为客户机“虚拟”物理内存的实现后台，又构建了MemoryRegion树，对一个物理地址空间内各个不同区域的功能进行了模拟，包括内存条RAM以及MMIO，使用容器MR对子MR进行管理，完成了QEMU对各段内存功能的模拟。\n前文由底层向上层，介绍了QEMU中抽象程度较高的数据结构MR。进一步地，除了对客户机物理地址空间的模拟，MR树可以用在对任何地址空间的模拟上。计算机硬件中还存在以下地址空间，与内存地址空间类似。\nCPU地址总线传来的地址可以用于访问RAM或MMIO，即形成了物理地址空间，system_memory即表示此空间； 除了MMIO，CPU与外围设备打交道的另一种方式是PIO，CPU使用 in/out 指令访问设备端口，端口号组成了另一种地址空间，在x86上有65536个端口，端口地址空间范围是[0，0xffff）； 除了CPU可以访问内存，外围设备也可以自发地访问内存，这种访问方式称作DMA（Direct Memory Access，直接内存访问），可以绕过CPU，不使用CPU访存指令访问内存。外围设备可以看到的内存地址也组成了一个地址空间。 以上三个场景均需要对一个MR树进行管理。QEMU引入了AddressSpace数据结构（简称为AS），表示CPU、外设或PIO可以访问的地址空间，定义如下：\nqemu-4.1.1/include/exec/memory.h\nAS数据结构主要承担两个任务：一是将MR树（由root成员指向）转换成线性视图current_map，二是在给定一个hwaddr(GPA)时快速查找到一个MemoryRegion，从而快速调用MR对应的回调函数ops或MR对应的RB，从而找到HVA，完成GPA到HVA的翻译。简而言之，就是完成树和线性表之间的相互转换，而线性表和树各有用途：\n线性表在向KVM注册内存时，需要给KVM提供一个线性的内存区域，才可以请求KVM完成这段客户机物理内存的地址翻译以及EPT建立； 而树状结构方便QEMU调用MR对应的回调函数ops，当KVM截获了一个MMIO/PIO的读写操作，且需要返回到QEMU时，应该在对应的AS中查询MR树，得到对应的ops进行模拟；也方便QEMU根据GPA找到RAM类型MR对应的HVA，即完成GPA到HVA的转换，方便内存读写。 除了线性结构和树结构之间的转换，QEMU还需要同步线性结构和树结构。在AS中，struct FlatView类型的current_map是线性结构，而MemoryRegion类型的root是树状结构。由于树状结构和线性结构应该保存相同的内存拓扑结构信息，其中一个更改时，应该同步到另一个数据结构。然而，不会存在对线性结构进行修改的情况，只会对树状结构通过函数族 memory_region_* 对MR树进行修改。\n当MR树被修改时，QEMU需要重新生成线性结构，再遍历AS的listeners链表，调用每个listener中的函数。每当生成了一个新的线性结构FlatView后，都需要重新告知KVM需要翻译的内存区域。显而易见，每次重新告知KVM新的内存拓扑需要进行 ioctl 调用，这是一个系统调用，开销很大，故QEMU将旧的FlatView保存在current_map中，比较新旧FlatView得出更改部分，仅告知KVM要更改的部分即可。\nQEMU代码中创建了**四类AS，**包括：\n全局变量address_space_memory，表示物理地址空间，其MR树根是大小为UINT64_MAX的system_memory； address_space_io，表示PIO空间，其MR树根是大小为65536（即0xffff）的system_io； 每个CPU都有一个名为cpu-memory-n的AS，其中n为从0开始的CPU编号，和address_space_memory一样，使用了system_memory作为MR树根； 外围设备角度的AS，例如VGA、e1000等模拟设备，它们的AS使用一个大小为UINT64_MAX的总线MR作为MR树根，并使用system_memory的别名MR作为MR树根的子MR，即可以将内存读写转发到system_memory对应的区域上。 AS数据结构提供了以下接口，供AS的使用者对AS进行创建、销毁与读写，其含义见注释：\nqemu-4.1.1/include/exec/memory.h\n此处省略了与缓存功能相关的接口，感兴趣的读者可自行查阅源码。其中有关读写的接口说明详见QEMU源码树的docs/devel/loads-stores.rst文档。后续将围绕这些AS管理函数，对AS的树结构与线性结构的同步，以及AS的读写、回调进行讲解。由于一些函数复杂度较高，这里只讲解重要的函数。AS相关操作均围绕着hwaddr(GPA)、void或uint8_t(HVA)之间的转换进行。\n在memory.c文件中，还有几个全局变量在下文出现，总结如下。\nqemu-4.1.1/memory.c\n综上，AddressSpace相关数据结构之间的关系如下图所示，下面将围绕此图的各个部分展开介绍。\n图中流程简述：①修改MR树；②生成扁平视图；③通知所有listeners；④ioctl系统调用进入KVM；⑤AddressSpace读写；⑥定位到MR并完成读写。\n从树状结构到线性结构 这里介绍树状结构到线性结构的同步过程。为了将GPA翻译到HPA，QEMU通过 ioctl 系统调用，将AS对应的线性结构current_map注册到KVM中；当树状结构被修改时，QEMU需要重新生成新的current_map，并与旧的current_map进行比较，将更改的部分重新注册到KVM中。FlatView用于管理线性结构，定义如下。\nqemu-4.1.1/include/exec/memory.h\n每个AS都有一个对应的FlatView，它保存了AS内存拓扑的线性结构，并且承担了hwaddr的分派功能，即通过dispatch成员将hwaddr映射到对应的MR，后文介绍。FlatView中保存了FlatRange数组，是AS对应的线性结构，FlatRange定义如下:\nqemu-4.1.1/memory.c\n如果将FlatView的FlatRange数组按顺序铺开，就得到了一个分布在hwaddr地址空间上的线性结构，由一段段可能互不相邻的FlatRange组成。何时由MemoryRegion树状视图生成该线性视图？ 经过分析，有两个时间节点：\nAS被初始化时，根据传入的MR树根生成线性视图current_map； AS对应的MR树被更改时，需要重新生成线性视图current_map。 首先分析AS初始化时如何生成FlatView，AS初始化函数定义如下。\nqemu-4.1.1/memory.c\n该函数首先初始化各个成员，包括current_map、root指针，并初始化listeners监听者链表。全局链表address_spaces负责将模拟客户机硬件所使用的所有AS链接起来，方便遍历所有的AS\naddress_update_topology 函数较为重要，该函数负责为新的AS生成FlatView，定义如下。\nqemu-4.1.1/memory.c\n调用 memory_region_get_flatview_root 函数找到AS的物理MR根（简称physmr，物理MR，后文将多次用到），其目的是找到实体MR（而非别名MR）的树根。这样可以减少FlatView的个数，使得拥有相同的实体MR的AS共用一个FlatView，减少内存开销。由此可知，AS的FlatView与physmr绑定，而非与根MR绑定； 调用 flatviews_init 函数初始化全局变量flat_views，它是一个全局的哈希表，负责将MR映射到FlatView。在首次调用 flatviews_init 函数时被设置为新的空哈希表； 调用 generate_memory_topology 函数，生成physmr对应的FlatView。这是将树状结构转换为线性结构的核心函数，但由于其复杂程度较高，此处不进行详细分析。它首先初始化view，然后调用render_memory_region 函数生成physmr对应的view，再调用 flatview_simplify 函数简化view。接下来的代码将根据生成的view填充地址分派器dispatch。最终，physmr到view的映射被存储在全局哈希表flat_views中。 至此，QEMU已经将树状结构转换为线性结构，接下来是告知所有监听者新的线性结构。在这里，只有一个KVM监听器，代码如下：\nqemu-4.1.1/include/sysemu/kvm_int.h\nKVMMemoryListener中的KVMSlot是QEMU中KVM的kvm_memory_slot对应的数据结构，负责向KVM注册内存；listener是通用监听器，定义如下：\nqemu-4.1.1/include/exec/memory.h\n通用监听器MemoryListener是一个函数指针的集合，并有一个priority成员表示其优先级，所有的listener在注册时按照优先级顺序连接到AS的监听者链表上。其中KVM监听器注册的代码如下：\nqemu-4.1.1/accel/kvm/kvm-all.c\n在KVM监听者中，region_add 函数指针指向 kvm_region_add 函数，最终调用 ioctl 函数完成内存的注册。\n下面让我们回到AS的初始化函数address_space_init 中，生成线性视图view后继续调用 address_space_set_flatview 函数，将新生成的physmr对应的view告知所有监听者，并且将AS的current_map更新到新生成的view。此处调用的是KVM监听器的回调函数，从 address_space_set_flatview 函数讲起，代码如下：\nqemu-4.1.1/memory.c\naddress_space_to_flatview 函数首先找到旧的current_map，作为old_view，该情况下为NULL； 再通过physmr在全局哈希表flat_views中查找新生成的physmr对应的view，作为new_view，将新旧view比较。在AS初始化时，如果新旧view不相同，则会将新旧view传给 address_space_update_topology_pass函数，从而告知所有的监听者线性结构的变化，最后将current_map更新为new_view。 其中，线性结构变化的单位是MemoryRegionSection，定义如下：\nqemu-4.1.1/include/exec/memory.h\naddress_space_update_topology_pass 函数首先对比新旧FlatView，得出新旧FlatView之间的差别，用FlatRange的形式保存。对此FlatRange调用MEMORY_LISTENER_UPDATE_REGION函数将FlatView转化为MemoryRegionSection，准备好调用所有listener的region_add函数。最终遍历AS的listeners链表，使用MemoryRegionSection调用所有listener的region_add函数。此处只关注KVM的kvm_region_add函数，这在前文介绍监听器数据结构时已经介绍过。具体调用链如下:\nqemu-4.1.1/memory.c\n至此，QEMU已经完成了AS初始化工作。AS初始化时涉及的调用链如下图所示：\n可以看到，初始化一个AS时，首先调用 generate_memory_topology 函数生成其physmr根对应的FlatView，再调用函数 address_space_set_flatview-\u0026gt;address_space_update_topology_pass-\u0026gt;kvm_region_add 通知KVM模块: 线性视图已经更改，需要重新向KVM使用ioctl函数注册内存，最终调用ioctl函数进入内核态的KVM模块中。\n简而言之，重要的是两个函数：\ngenerate_memory_topology 函数，负责生成MR树对应的FlatView； address_space_set_flatview 函数，负责将FlatView的更改通过 address_space_update_topology_pass函数告知所有listener，其中包括KVMMemoryListener。 AS的初始化只是一个需要同步树状结构与线性结构的情况。在AS初始化之后，MR树被更新时也需要同步到FlatView，并通知KVM。\nQEMU提供的多个操作MR的接口会更新MR树，如 memory_region_add_subregion 函数，QEMU都会使用MR事务机制完成FlatView的同步以及KVM监听器的通知，其大致调用链代码如下:\nqemu-4.1.1/memory.c\n可以看到，每次修改MR树都在 flatviews_reset 函数中重新生成了对应的FlatView，并且调用address_space_set_flatview 函数将新的FlatView注册到KVM中。简化的调用链如下图所示，类似于AS创建之后的调用链。\n至此，QEMU完成了树状结构到线性结构的同步，并将线性结构注册到KVM中。QEMU进行了树状结构到线性结构的转化，将较为复杂的“策略”转换成一种简单的形式，可以调用KVM提供的“机制”完成QEMU/KVM的协同工作。\n此时，当客户机访问一个“虚拟”物理地址GPA时，如果该GPA不是用于MMIO，那么MMU都会查询KVM所维护的EPT得到其对应的“真实”物理地址，客户机访问这部分“虚拟”物理内存将不会引起VM-Exit，从而完成高效的内存虚拟化。对于MMIO区域的GPA，QEMU是如何与KVM协作的呢？\n客户机物理内存地址的分派 对于实现MMIO的MR，在KVMMemoryListener对它进行KVM内存注册时，注册函数 kvm_region_add 将其识别为MMIO对应的“虚拟”物理内存区，没有对应的宿主机虚拟地址，就不会将其提交到KVM中。如果客户机访问了这段内存，KVM会识别这是MMIO对应的内存区，最终返回到QEMU的 ioctl(KVM_RUN) 系统调用之后。对于PIO的处理是类似的，也会退出到QEMU的 ioctl(KVM_RUN) 系统调用之后。调用代码如下：\nqemu-4.1.1/accel/kvm/kvm-all.c\n可以看到，在退出到QEMU之后，需要对QEMU模拟MMIO/PIO所使用的AS数据结构进行读写，使用的是前文介绍的AS读写函数address_space_rw。\n所谓客户机物理内存地址的分派是指，将对AS读写的地址分派到对应的MemoryRegion上，调用MR所包含的处理函数进行MMIO/PIO模拟。事实上，如果给定一个hwaddr，可以在MR树中搜索其对应的MR，但这样做无疑是很低效的。为此，QEMU引入了一个类似于页表的结构完成地址转换，即 struct AddressSpaceDispatch，其复杂程度较高，不进行深入分析。数据结构关系如下：\nqemu-4.1.1/include/exec/memory.h\nAS的线性视图（扁平视图）中保存了struct AddressSpaceDispatch的指针，定义如下：\nqemu-4.1.1/exec.c\n每个AS有一个独立的AddressSpaceDispatch，作为AS内存地址分派的页表，其叶子结点的页表项指向MemoryRegionSection，查询该页表的结果是MemoryRegionSection。在AddressSpaceDispatch中，mru_section保存了最近一次的查询结果，作用类似于TLB；map是一个多级页表，即能够快速查找，又不会占用过多内存。phys_map起到了CR3的作用，PhyPageMap中的nodes表示页表的中间节点，sections等同于物理页的作用。\n由于AddressSpaceDispatch实现较为复杂，这里只关注该数据结构提供的接口。正如前文提到的，AS对应的dispatch在生成线性视图时初始化，并填入FlatView的dispatch字段，即在前文提到的核心函数 generate_memory_topology 中初始化，代码如下：\nqemu-4.1.1/memory.c\n在生成AS的FlatRange数组之后，QEMU将FlatRange数组中每个元素转换成其对应的MemoryRegionSection，并调用 flatview_add_to_dispatch 函数填入页表AddressSpaceDispatch中。这意味着，只要生成了FlatRange，QEMU就可以使用AddressSpaceDispatch查找一个AS中hwaddr(GPA)所在的MR，从而得出GPA对应的HVA。\naddress_space_rw 函数使用页表AddressSpaceDispatch完成地址转换，定义如下：\nqemu-4.1.1/exec.c\n这里出现了一个MemTxResult类型，QEMU将对AS的读写视为一次MR事务，其返回结果MemTxResult是一个uint32_t类型的变量，定义在include/exec/memattrs.h中，可以取MEMTX_OK、MEMTX_ERROR等值。对AS进行读写时，首先原子地读取AS的current_map，即当前的线性结构；再调用 flatview_read/write 函数对线性结构fv进行读写。此处用 flatview_read 函数作为例子进行说明，其定义如下：\nqemu-4.1.1/exec.c\n可以看到，flatview_read 函数不断调用 flatview_translate 函数，通过FlatView内部的页表AddressSpaceDispatch得到一个hwaddr地址对应的MemoryRegion，再进行MemoryRegion对应的模拟。对于I/O类型的MR，最终调用 ops-\u0026gt;read 函数完成读取的模拟；对于RAM类型的MR，则找到hwaddr addr对应的HVA，记录在ptr指针中，调用memcpy完成读取。客户机物理内存地址分派的调用链如下图所示：\n至此，已经介绍了QEMU中内存虚拟化相关的大部分数据结构及其操作接口之间的关系。总结如下：\nAddressSpace是顶层数据结构，将MemoryRegion树和FlatView线性结构组织起来，形成一个可供读写的地址空间； MemoryRegion中的容器类型和别名类型分别模拟了真实系统中的总线和内存控制器，将I/O类型的MR和RAM类型的MR通过树的形式组织起来。 I/O类型的MR提供了一组ops回调函数，供QEMU实现物理硬件的模拟； 而RAM类型的MR对应宿主机上的一段虚拟地址HVA，作为客户机的“虚拟”物理地址，QEMU最终将这段地址注册到KVM中完成GPA到HPA的翻译； FlatView保存了MemoryRegion树对应的线性结构FlatRange，供QEMU将其转换为MemoryRegionSection注册到KVM中；还保存了地址分派器AddressSpaceDispatch，负责将GPA翻译为HVA。 下一节将展示运行过程中这些数据结构的组织形式，可以有更加直观的认识。\n实验: 打印MemoryRegion树 QEMU为了模拟MMIO以及物理设备的行为，形成了一套复杂的数据结构，但这些只是静态的代码。本节将QEMU代码运行起来，在动态过程中打印出MemoryRegion树，更形象地展示数据结构之间的关系。\n实验使用从源代码编译的QEMU v4.1.1，以及事先准备好的客户机磁盘镜像作为QEMU的-hda参数传递给QEMU。首先，使用如下命令进入QEMU监视器：\n启动命令的含义为：将QEMU管理器的输入输出重定向到字符设备stdio(-monitor stdio)，即此处的命令行。\n此命令启动了2个vCPU -smp 2，使用NUMA（Non-Uniform Memory Access，非统一内存访问）架构，分为两个NUMA节点 -numa node，分配 4 GB的“虚拟”物理内存 -m 4096 ；开启KVM支持，并使用与宿主机一样的CPU型号 -cpu host--enable-kvm。\n接下来，使用命令 info mtree 打印此客户机的MemoryRegion树，在输出中，QEMU用不同宽度的缩进表示不同树的深度，打印如下：\n此处省略了被标为 [disabled] 的MR，以及一些陌生的MR。可以看到，整个虚拟机有address_space_memory作为物理内存空间的AS，有address_space_io作为PIO端口映射空间的AS。由于本实验启动了 2 个vCPU，所以这里打印出了两个CPU的AS，即 cpu-memory-0/1。其他的AS包括从设备角度可以观察到的AS，如e1000、VGA等设备的AS。\n每个AS下显示了AS的MR树，其中非别名类型的MR只能打印出一条较短的记录，包含其地址范围。如address_space_memory的MR树根system_memory，其地址范围是0x00000000000000000xffffffffffffffff，即0UINT64_MAX；而别名MR会被明确标识为alias，并追加上其alias指针指向的原MR。有关 info mtree 命令的实现函数，请查阅QEMU源码树memory.c文件的 mtree_info -\u0026gt; mtree_print_mr 函数。\n为了与源码相对应，继续在QEMU源码中寻找这些AS和MR被创建的位置，具体方法多种多样。一种直接的方法是在源码中搜索相关的创建函数，如 address_space_init、memory_region_init，更严谨的方法是通过GDB打断点的方式寻找。\n首先，在QEMU的main函数中，cpu_exec_init_all 函数初始化了主要的AS以及MR树根，代码如下：\nqemu-4.1.1/exec.c\n在这里，QEMU初始化了 system_memory/system_io 等静态变量，作为两个全局AS变量 address_space_memory/address_space_io 的MR树根。这里初始化了与体系结构无关的AS，下面进入i386的模拟部分中与初始化架构相关的部分。在不同类型的PC_MACHINE的定义函数中，也会初始化AS/MR等数据结构，以 pc_init1 函数为例，代码如下：\nqemu-4.1.1/hw/i386/pc_piix.c\n可以看到，pc_init1 函数首先初始化了PCI MR，继续调用 pc_memory_init 函数，初始化了真实的全局物理内存pc.ram MR，并将其分为两个别名MR，即 ram_below_4g/ram_above_4g，并作为子MR加入了system_memory。\n在解析QEMU参数时，QEMU读取到 -m 参数后的数字，并将其保存在machine-\u0026gt;ram_size中，作为初始化pc.ram MR的大小，即物理内存的大小。在分配全局pc.ram MR时，QEMU将NUMA和非NUMA的情况分类。非NUMA的情况下，直接分配一个RAM类型的实体MR即可；而NUMA情况下，需要调用 host_memory_backend_get_memory 函数得到每个NUMA节点对应的MR，并作为子MR加入pc.ram MR中。这与之前info mtree打印出来的MR树相符合。\n3.3 KVM内存数据结构 相比于 “策略” 的实现，“机制” 的实现往往更加简单。QEMU内存虚拟化需要完成的功能较多，包括宿主机虚拟内存的分配、MMIO的模拟、HVA到GPA的翻译等，而**KVM内存虚拟化只需要维护好EPT页表，并与硬件配合即可。**同时，由于KVM是Linux内核中的一个内核模块，它可以重用Linux内核的内存管理接口，降低了实现的难度。\n本节不讨论由于性能不佳而不常采用的影子页表的实现，只讨论Intel x86架构下的扩展页表EPT的维护。在Linux源码树的Documentation目录下，描述内核代码的 Documentation/virtual/kvm/mmu.txt 文档有对KVM内存管理模块较为全面规范的描述，但较难理解。下文将抽取主线，使叙述更易懂。KVM中相关的数据结构相比于QEMU更简洁，如下图所示。\nKVM在内存虚拟化方面的工作流程大致为：\n接收QEMU的内存注册； 创建vCPU的虚拟MMU； EPT的建立； 下面对每个步骤展开分析。\n接收QEMU的内存注册 首先，QEMU应该给KVM注册需要其做地址翻译的 “虚拟” 物理内存，否则KVM所维护的EPT页表将无用武之地，因此从KVM接收QEMU的内存注册开始讲起。\n前文提到，当MemoryRegion树被更改后，都会通知所有的listener，其中包括KVM的监听器KVMMemoryListener，调用ioctl完成KVM内存注册。注册的基本单位是如下数据结构，包含GPA、HVA、该段内存的大小等字段，与QEMU中的线性视图相类似。\nlinux-4.19.0/include/uapi/linux/kvm.h\nQEMU进行ioctl系统调用后进入内核的 kvm_vm_ioctl 函数，并根据ioctl的参数调用相应的处理函数。此时ioctl参数是KVM_SET_USER_MEMORY_REGION，代码流程如下：\nlinux-4.19.0/virt/kvm/kvm_main.c\n由于QEMU中的kvm_userspace_memory_region处在用户态，因此内核态的KVM需要使用 copy_from_user 函数将其中的数据复制到内核态。最终进入 __kvm_set_memory_region 函数，将 kvm_userspace_memory_region 注册到KVM中，而KVM中保存客户机内存线性视图的结构是kvm_memory_region，用户态QEMU传来的数据需要转化为该数据结构进行保存，定义如下：\nlinux-4.19.0/include/linux/kvm_host.h\n可以看到，每个KVM虚拟机对应的struct kvm结构体都保存了一个memslots，其中保存了kvm_memory_slot的数组。这是KVM中唯一保存客户机物理页信息的位置，与QEMU不同，KVM仅有一个客户机物理内存的线性视图，保存了所有客户机物理内存的相关信息。\n在进入KVM的内存注册ioctl后，_kvm_set_memory_region 函数将使用用户态传来的结构体kvm_userspace_memory_region更新kvm的memslots，更新类型为enum kvm_mr_change。__kvm_set_memory_region 函数定义如下：\nlinux-4.19.0/virt/kvm/kvm_main.c\n该函数首先得到要插入位置的旧memslot，与用户态传来的新memslot对比，得出change的类型。如果QEMU添加新的memslot，那么就会进入KVM_MR_CREATE的分支，执行架构相关函数 kvm_arch_create_memslot 填充新的memslot。准备好新的memslot后，就调用update_memslots 函数将新memslot填入slots数组，并保持数组的排序（base_gfn从大到小），最终原子地将slots填入kvm中。\nKVM维护x86架构相关的客户机物理页信息，包含 kvm_rmap_head 以及 kvm_lpage_info，代码如下：\n针对x86架构，每个客户机的 “虚拟” 物理页都有相关的信息，保存在memslot的arch成员中。EPT中最后一级页表可以是第3级页表，映射一个1GB的大页；可以是第2级页表，映射一个2MB的大页；也可以是通常情况的第1级页表，映射一个4KB的页。因此在arch结构体中，KVM将memslot管理的这段“虚拟”物理内存按照不同大小的页面分割，共上述3种情况(1GB、2MB、4KB)，形成不同个数的页面（如1GB的内存区域按照1GB分割，则只有1页；按照2MB分割，则有512页）。下面代码填充每个页面的相关信息，分为KVM_NR_PAGE_SIZES种页面大小的情况。\nlinux-4.19.0/arch/x86/kvm/x86.c\n这里，每个“虚拟”物理页都有两个信息：\n映射该gfn的所有spte（EPT页表项），保存在arch.rmap数组中，可以以spte链表的形式存在，每个页面都有对应的链表。该链表负责在某HVA处的页面被换出时，将所有与该HVA对应的spte置为无效。这种反向映射机制在Linux内核中也存在； 保存该gfn处是否可以使用大页，不做深入分析。 可以看到，KVM已经保存了所有客户机 “虚拟” 物理内存页面的信息，存在于memslots成员中，KVM维护EPT页表时将完全基于memslots数据结构。下面分析KVM如何维护EPT页表，与MMU硬件协同完成地址翻译。\n创建vCPU的虚拟MMU 继续介绍EPT页表页的创建与管理，虚拟MMU相关数据结构如下图所示：\n其中需要着重关注的是 struct kvm_mmu_page，它管理了一个EPT页表页的所有信息，vCPU虚拟MMU的主要工作是维护EPT页表中每个页表页的 struct kvm_mmu_page。\n**为了管理EPT页表页，KVM使用了 struct kvm_mmu_page 数据结构保存一个EPT页表页的相关信息，又使用 struct kvm_mmu 数据结构保存与内存管理相关的函数模拟硬件MMU。**每个vCPU都有一个虚拟的MMU，且MMU的模拟依赖架构，因此保存在 struct kvm_vcpu_arch 数据结构中。\n注意区分，struct kvm_arch 保存了整个客户机的架构相关信息，而 struct kvm_vcpu_arch 保存了每个vCPU的架构相关信息。简而言之，虚拟MMU保存在vCPU的架构相关部分中，虚拟MMU的root_hpa指向kvm_mmu_page（后文介绍）组成的页表。\n以下为这些数据结构的代码：\nlinux-4.19.0/arch/x86/include/asm/kvm_host.h\n在KVM中有一个全局变量 enable_ept 决定是否开启EPT模式。如果 enable_ept 为1，则启用EPT模式，最终将全局变量 tdp_enabled（在arch/x86/kvm/mmu.c中）置为true。事实上还需要读取VMCS的相关配置域才能决定是否将 enable_ept 置为1，简洁起见本节省略这部分的介绍。下面是将 tdp_enable 置为true的代码。\nlinux-4.19.0/arch/x86/kvm/vmx.c\n在此之后创建vCPU时，将完成基于EPT（即tdp）的虚拟MMU初始化。虚拟MMU的初始化分为 kvm_mmu_create/kvm_mmu_setup 两步：\n虚拟MMU创建 EPT基地址设置 MMU创建流程 整体代码如下：\n虚拟MMU的初始化与vCPU的初始化绑定，即一个vCPU有一个虚拟MMU。可以看到，虚拟MMU事实上包含了一组与tdp相关的页表管理函数，包括缺页异常处理函数 tdp_page_fault、页表基地址设置函数 set_tdp_cr3（即 vmx_set_cr3），以及虚拟MMU的相关属性的设置。\n完成虚拟MMU的创建后，下文介绍KVM如何维护 EPTP 和客户机 CR3。\nEPT基地址设置流程 EPT页表页 kvm_mmu_page 与普通进程的页表页一样，在缺页异常中创建。但这不是一般的缺页异常，而是客户机引发的VM-Exit，是一个由Intel硬件提供的特性，即 EPT PAGE FAULT。该VM-Exit将使客户机暂停执行并进入KVM，使得KVM有机会完善EPT。为了配合硬件，KVM需要将EPT的基地址写入VMCS，让硬件MMU自动访问该EPT页表，当硬件MMU发现该页表存在不完整的情况时，将产生VM-Exit，并调用虚拟MMU的相关函数。\n接上文对VM文件描述符的 KVM_CREATE_VCPU 调用，QEMU将对vCPU对应的文件描述符进行 KVM_RUN 调用，运行刚刚初始化并填充好的vCPU，流程如下：\n可以看到，vcpu_run 函数中出现了无限for循环，保证在运行vCPU退出后，完成KVM的处理继续运行vCPU。在该循环中，运行vCPU前调用kvm_mmu_reload 函数，如果root_hpa尚未初始化（指向INVALID_PAGE），则调用 kvm_mmu_load 函数初始化root_hpa，其初始化工作主要包括：\n调用 mmu_topup_memory_caches 函数保证arch中的 3 个cache充足； 为root_hpa分配一个kvm_mmu_page，作为EPT的第4级页表页，页表页的分配将在下文介绍； 根据上一步分配的EPT第4级页表页的物理地址，创建EPTP，写入VMCS的EPTP域中，EPTP域的详细文档见Intel SDM的卷3第24.6.11节；还要从arch结构中读取客户机CR3，写入VMCS的GUEST_CR3域中。这样在物理CPU进入非根模式后，就可以使用该EPTP进行GPA到HPA的翻译； 刷新TLB，有三种方式，不详细介绍。 综上，KVM已经准备好进入非根模式，执行 kvm_x86_ops-\u0026gt;run 函数。下面介绍EPT页表页及其创建过程，EPT页表的创建和维护过程均在kvm_x86_ops-\u0026gt;handle_exit 函数中进行。\nEPT的建立 KVM从QEMU得到了需要GPA到HPA翻译的所有 kvm_memory_slots，设置好EPTP后，MMU硬件就可以截获GPA地址的访问，使客户机退出到KVM中，建立GPA相关的EPT页表。这里介绍EPT的建立与管理，首先介绍管理EPT页表页的数据结构 kvm_mmu_page，它定义如下：\nlinux-4.19.0/arch/x86/include/asm/kvm_host.h\n该结构维护了一个 spt 指针关联的信息，spt 是指向页表页的指针，是一个HVA。spt 指向的4KB大小的页面即是EPT页表页，保存了512个页表项。KVM在 struct kvm_arch 中保存了一个active_mmu_pages链表，将所有的kvm_mmu_page链接起来，可以当页表页版本号mmu_valid_gen与arch中的mmu_valid_gen不同时，释放页表页（通过其操作接口 kvm_mmu_free_page 函数）。这样KVM就做到了内存占用尽可能小，这和mmu.txt文档中KVM内存虚拟化设计原则相符合。\n创建页表页的时机在于发生EPT Violation时，CPU进入根模式运行KVM。在这里，KVM执行如下虚拟MMU的缺页异常处理函数：\nlinux-4.19.0/arch/x86/kvm/mmu.c\n退出原因是EXIT_REASON_EPT_VIOLATION，于是调用 handle_ept_violation 函数。VMCS保存了该缺页异常的相关信息，包括造成缺页异常的gpa、缺页异常的类型error_code等。如果error_code表示需要处理MMIO类型的EPT Violation，则调用 handle_mmio_page_fault 函数。在EPT页表缺页的情况下，调用 tdp_page_fault 函数完善EPT。\n这里忽略所有与大页相关的代码，感兴趣的读者可以在介绍的基础上研究大页相关代码，进行“增量式”学习。\ntdp_page_fault 函数主要代码如下：\nlinux-4.19.0/arch/x86/kvm/mmu.c\ntdp_page_fault 函数较为复杂，涉及多个Linux内核子系统，下面分步介绍。\n该函数首先填充三个缓存池，每次预分配都分别增加16、8、4个预备的对象。这三个缓存池分别用于pte链表（反向映射中一个gfn对应的所有pte的链表，以及一个pte的parent_ptes链表）、EPT页表页、EPT页表页头（即描述EPT页表页的struct kvm_mmu_page）的分配，这三类对象的分配在KVM中很频繁，因此缓存池能够通过合并分配提高分配效率。\n这里忽略大页管理系统，假定EPT中不映射大页，mapping_level函数将返回1。接下来，尝试缺页异常处理的快速路径，调用fast_page_fault函数。此函数和KVM脏页管理系统有关，回忆当QEMU注册memslot时，可以使用KVM_MEM_LOG_DIRTY_PAGES标志，它表示需要对这段内存进行脏页跟踪，多用于QEMU虚拟机热迁移的实现。当QEMU使用该标志进行内存注册时，KVM将会把这段内存对应的所有EPT表项置为只读，这涉及内存页的反向映射系统。于是，该页表项指向的物理页已经分配，只需要将该页表项修改为可写就可继续正常执行客户机代码，而不再产生EPT Violation，不需要执行后面的真实缺页异常处理。\ntry_async_pf 函数负责检查客户机访问的GPA是否在KVM的memslots中，如果在，则分配宿主机物理页面，得到pfn。EPT将GPA翻译为HPA，那么为了填充EPT表项，一个客户机物理页(GPA)必将对应一个宿主机物理页(HPA)。但是，如果该宿主机内存页已经换出到磁盘上，则会使客户机vCPU停滞较长的一段时间。为此，KVM实现了异步缺页异常，当一个vCPU访问了被换出的宿主机物理页（这里借助了宿主机Linux内核的内存管理相关接口，不做深入介绍）时，则会将vCPU挂起，并执行另一个vCPU，等待被换出的页加载到内存中。加载完毕后，被挂起的vCPU则会从 try_async_pf 函数中返回，重新执行引起缺页异常的客户机指令。\n该函数的另一个任务是实现MMIO，检查参数gfn是否在KVM的memslots中，如果不在，则向pfn中写入KVM_PFN_NOSLOT，最后进入__direct_map函数的set_mmio_spte函数中，将EPT中这段客户机物理内存对应的部分标为特殊值，以后的读写都会引起EPT Misconfiguration的VM-Exit。至此，KVM获得了pfn（类型为kvm_pfn_t，表示HPA），交由下一步构建EPT表项、填充EPT使用。\n进入修改EPT的代码区，由于多个vCPU线程以及MMU通知器（MMU Notifier，当Linux内核将虚拟内存页换出到磁盘上时将收到通知，KVM也注册了一个这样的通知器，其具体作用是在Linux内核换出客户机内存页时修改EPT）可能会同时修改EPT，需要加kvm的mmu_lock锁。mmu_notifier_retry 函数让MMU通知器线程优先执行，放弃vCPU的EPT填充。接下来，make_mmu_pages_available 函数将无用的EPT页表页释放，与之前填充EPT页的分配缓存池相互照应。这里也用到了反向映射系统。\n__direct_map函数实际填充了EPT四级页表，其定义如下：\ninux-4.19.0/arch/x86/kvm/mmu.c\n该函数根据前面步骤得到的gfn（客户机物理页号）以及pfn（宿主机物理页号）完成EPT中gfn对应部分的建立。for_each_shadow_entry宏负责遍历四级页表，其参数是客户机物理地址 gfn\u0026lt;\u0026lt;PAGE_SHIFT 和遍历光标iterator。在C++11语法中，iterator承载了指针的作用，且方便了对一个数据结构的遍历。而此处的iterator（即读取EPT页表时指向EPT的指针）中存储了读取页表时的相关参数，方便了EPT的操作。详细内容见注释，此处仅关注其中的level、addr和sptep。\nlinux-4.19.0/arch/x86/kvm/mmu.c\n在此处的讨论中，for_each_shadow_entry宏会执行4次，由于忽略了大页，EPT共有四级页表。分两种情况：\n遍历到的页表页是页表的中间节点，如果当前页表项 *iterator.sptep 不存在，则调用 kvm_mmu_get_page 函数获取一个kvm_mmu_page，并调用 link_shadow_page 函数将当前sptep指向新的页表页； 如果遍历到的是最后一级页表，则调用 mmu_set_spte 函数将最后一级页表项指向新的pfn。这样就建立了映射gfn到pfn的EPT。 最终释放mmu_lock，如果更改EPT页表过程中的某一步失败，还需要释放由 try_async_pf 函数分配的pfn处的宿主机物理页。\n至此，KVM完成了gpa对应的EPT四级页表的填充。\n实验: 将GVA翻译为HPA 实验概述 作为内存虚拟化的总结以及KVM内存虚拟化源码分析的拓展，本节进行GVA到HPA的翻译实验。内存虚拟化的核心是地址翻译，即将某一个地址空间的地址转换为下层地址空间的地址。如前文所述，地址翻译由MMU硬件完成，首先使用客户机进程页表GPT将GVA翻译为GPA，再由扩展页表EPT将GPA翻译为HPA。由于无法观察硬件的地址翻译过程，于是本节借助内核提供的页表访问接口，通过编写软件模拟MMU的功能。\n为了证明内存翻译代码运行的正确性，首先在GVA处写入一个int类型的变量，并在最后得到的HPA处对该int类型变量进行读取，如果写入的变量和读到的变量值相同，那么证明地址翻译正确。本实验实现的软件MMU分为两部分：\n客户机中的地址翻译模块作为客户机中运行的内核模块，首先在GVA处写入一个int变量 0xdeadbeef，再通过读取GPT的方式将GVA翻译成GPA，最后通过超级调用将GPA传递给宿主机操作系统； 宿主机中的KVM内核模块将截获到该超级调用，得到客户机传来的GPA，通过读取EPT的方式进一步翻译成HPA。为了读取HPA处的变量，还需要使用内核提供的接口做一次HPA到HVA的转化，这是因为分页模式开启，访存指令中的地址均是HVA，无法使用HPA直接访问物理内存。最终读取HVA处的变量，将读取到的值与客户机写入的值进行比较，如果也是 0xdeadbeef，则能够证明地址翻译的正确性。 下文分别介绍**客户机的地址翻译与宿主机的地址翻译，**形成一个整体，使读者了解地址翻译的流程，流程如下图所示。\n注：①调用kmalloc函数得到GVA；②查询GPT；③打印GPT相关表项；④查询GPT，得到GPA；⑤通过超级调用将GPA传入KVM，查询EPT；⑥打印EPT相关表项；⑦查询EPT，得到HPA。\n首先说明实验的运行环境：宿主机CPU型号是IntelCore i7-6500U，频率2.50GHz，QEMU为客户机提供了与宿主机相同型号的CPU。宿主机物理内存大小为7.5GB，客户机物理内存为4GB。本节宿主机和客户机均使用Linux v4.19.0内核，并使用从源码编译的QEMU v4.1.1。再说明实验准备，读者应该事先制作好一个客户机磁盘镜像，作为QEMU的-hda参数进行读取，使用QEMU启动一个客户机。\n实验相关过程与脚本见代码仓库：https://github.com/GiantVM/book 。\n客户机中的地址转换: GVA到GPA 为了读取Linux操作系统的页表，需要在内核态编写代码。使用Linux内核模块是编写内核代码的一种简单的方式。整体操作流程如下：\n内核模块的源码可以仅由一个.c文件组成，在实验里是gpt-dump.c，只需要编写一个Makefile（编译命令文件）对gpt-dump.c进行编译，得到gpt-dump.ko文件； 再使用命令sudo insmod gpt-dump.ko 即可将内核模块插入内核； 内核模块插入内核后，就会调用gpt-dump.c中定义的 init_module 函数；而使用命令sudo insmod gpt-dump.ko 将内核模块移出内核时，则会调用 cleanup_module 函数。在 init_module 函数中即可编写内核代码在内核态运行，可以访问页表。 虚拟地址和物理地址在Linux内核中均使用64位的变量表示，而在Intel Core i7系统中，虚拟地址空间为48位，共256TB大小，物理地址空间为52位，共4PB(4096TB)大小。一个页表项的大小为64位，即8字节，一个页表页有512个页表项，页表页大小为4KB，故需要虚拟地址中的log2(512)=9 位索引每级的页表页。\nLinux内核使用4级页表，用虚拟地址的第47：39位索引第4级页表，第38：30位索引第3级页表，第29：21位索引第2级页表，第20：12位索引第1级页表。这样就形成了前文所述的 9+9+9+9 形式的四级页表。\n虚拟地址使用第47：12位存储页表的索引，第11：0位存储虚拟地址在页中的偏移，因此查询页表只使用了虚拟地址的前48位，即访问虚拟地址空间仅使用了48位的虚拟地址。此处定义宏 UL_TO_VADDR以/VADDR_PATTERN 打印虚拟地址，包含页表的索引位（共36位），以及偏移（共12位）。部分相关宏定义如下。\ngpt-dump/gpt-dump.c\n由于内核中尚没有将变量转化为二进制表示的函数，此处编写 pr_info 函数，接收不同的 %c 格式化字符串和 0/1 字符的组合来打印虚拟地址、物理地址以及页表项。以虚拟地址的打印为例，首先定义输出3个位的字符组合，即 TBYTE_TO_BINARY，还有对应的打印三个char类型的格式化字符串 TBYTE_TO_BINARY_PATTERN。接下来，需要将虚拟地址的低12位（即偏移的12个位）全部打印出来。继续对代表虚拟地址的ulong（64位的变量）使用 TBYTE_TO_BINARY，得到其低3位；再将ulong右移3位，并使用宏 TBYTE_TO_BINARY，得到其5：3位，以此类推，可以得到所有形式的 0/1 字符组合，包括宏 UL_TO_PTE_OFFSET 负责输出12位，UL_TO_PTE_INDEX 负责输出9位。对于 pr_info 的格式化字符串，实现方式类似，只需在合适的位置加上空格，便于观察。在内核模块初始化函数中，首先调用 print_ptr_vaddr 打印虚拟地址，输出如下：\ngpt-dump/gpt-dump.txt\n可以看到四级页表的四个索引值，以及页内偏移的二进制表示。\n为了获得一个GVA，在模块初始化函数中，首先调用 kmalloc 函数生成一个int类型变量的指针，由于内核模块运行在客户机内核中，所以该指针包含一个GVA。在上面的代码片段中，客户机内核模块在该指针处写入数字：0xdeadbeef，期望在GVA对应的HVA处读到该数字。在输出中可以看到四级页表的索引，得知在页表页中应该读取第几个页表项。接下来，内核模块找到客户机内核线程的 CR3，并将其传入页表打印函数。下面是客户机内核模块的相关代码：\ngpt-dump/gpt-dump.c\ncurrent-\u0026gt;mm-\u0026gt;pgd 是当前进程current的CR3，指向PGD（Page Global Directory，页全局目录）页表页的起始地址。dump_pgd 函数负责遍历pgd页表页的PTRS_PER_PGD个页表项，这里是512个页表项。\npgd是一个pgd_t类型的变量，内核还提供了类似的pud_t、pmd_t、pte_t数据结构表示每级页表的页表项，以及操作这些数据结构的接口。\n此处使用这些接口获取页表项的含义，如 pgd_val 函数返回该页表项的值，即该页表项对应的unsigned long变量；pgd_present 函数检查该页表项的第0位，返回该页表项是否有效；pgd_large 函数返回该页表项是否指向1GB的大页。\n本节忽略大页的情况，我们可以使用内核参数关闭大页。这里定义了全局变量保存的虚拟地址和对应的物理地址，以及各级页表索引。pgd_idx表示从64位虚拟地址中获得的PGD页表索引。接下来，如果PGD页表页的第 pgd_idx 个页表项存在，那么调用 pr_pte 函数打印该页表项，此函数定义如下：\ngpt-dump/gpt-dump.c\n参数 address 表示本级页表页的起始地址，从上一级页表项获得，pte 表示本级页表项。宏PTE_PATTREN用于打印一个页表项。继续查询下一级页表的函数调用链为 dump_pgd -\u0026gt; dump_pud -\u0026gt; dump_pmd -\u0026gt; dump_pte，其中每一步的逻辑大致相同。address和pte的打印结果如下。\ngpt-dump/gpt-dump.txt\n从加粗的部分可以看到，客户机内核模块对页表页的地址调用 _pa 函数获得其物理地址，和上一级页表项中保存的下一级页表页的物理地址完全相同，这符合预期。最终，就可以从PTE中获得物理地址，为了验证正确性，客户机内核模块对vaddr调用 __pa 函数，打印出GPA开头的行，具体在 print_pa_check 函数中实现，代码如下。\ngpt-dump/gpt-dump.c\n可以看到，和之前获得的PTE中的物理地址相同，说明读取页表的结果正确。其中低12位是页内偏移，物理地址和虚拟地址中的页内偏移完全相同。最后，客户机内核模块将调用 kvm_hypercall1(22,paddr) 函数，将paddr传给KVM。\nKVM中的地址转换: GPA到HPA KVM负责维护EPT，具有读取EPT的权限。本实验修改宿主机内核的KVM模块，**增加一个超级调用处理函数，**接收从客户机传来的GPA，模拟GPA到HPA的翻译。\n当客户机执行了一个敏感非特权指令时，会引起CPU的VM-Exit，CPU的执行模式从非根模式转换为根模式，并进入KVM的VM-Exit处理函数。kvm_hypercall1 函数最终执行vmcall指令，陷入KVM，KVM得知VM-Exit的原因是客户机执行了vmcall指令，编号为EXIT_REASON_VMCALL，于是调用如下 handle_vmcall 处理函数。\nlinux-4.19.0/arch/x86/kvm/vmx.c\n函数 handle_vmcall 调用超级调用模拟函数 kvm_emulate_hypercall，代码如下。\n其中，nr是 kvm_hypercall1 函数的第一个参数，a0是第二个参数。nr表示应该调用哪个超级调用模拟函数，定义KVM_HC_DUMP_SPT为22，表示打印客户机内核线程页表对应的EPT。首先调用 print_gpa_from_guest 函数打印客户机传来的GPA，并从GPA中获取每级EPT页表的索引，保存在全局变量 pxx_idx[4] 中，print_gpa_from_guest 函数的打印结果如下。\ngpt-dump/ept-dump.txt\n可以看到，此处的GPA与在客户机中读取GPT得到的GPA相同，说明客户机内核模块的超级调用成功。接下来调用 mmu_spte_walk 函数遍历此vCPU的EPT，在代码中称作spt，这是为了和影子页表共用一套代码。传入 mmu_spte_walk 函数的参数有vcpu，以及遍历到一个页表项时所调用函数的指针pr_spte，负责打印页表项。遍历代码如下。\nlinux-4.19.0/arch/x86/kvm/mmu.c\nvcpu-\u0026gt;arch.mmu.root_hpa 保存了EPT的基地址，初始化时被设为INVALID_PAGE。mmu_spte_walk 函数首先判断vcpu-\u0026gt;arch.mmu.root_hpa是否是无效页INVALID_PAGE，如果是，则说明vCPU对应的EPT尚未建立，无法遍历。如果EPT的级数大于PT64_ROOT_4LEVEL，则调用递归函数__mmu_spte_walk 遍历页表。\npage_header 函数返回一个hpa_t变量所指向页表页的 kvm_mmu_page 结构的指针。于是，KVM将EPT第4级页表页的kvm_mmu_page结构传入_mmu_spte_walk 函数，并且从level=1开始遍历EPT。\n和查询GPT一样，KVM遍历页表页中的每一个页表项，如果页表项的索引等于之前 print_gpa_from_guest 函数中获得的pxx_idx中对应的索引，那么此页表项就是目标页表项，并将它传入fn函数进行打印。如果查询到的页表项不是最后一级，则继续递归调用 __mmu_spte_walk 函数查询下一级页表。在这里，将fn置为打印EPT页表项的函数，如下打印格式与GPT相同。\ngpt-dump/ept-dump.txt\n由于本实验没有关闭宿主机上的大页，KVM在查询到最后一级页表时做了两种处理：如果遍历到大页，则调用 print_huge_pte 函数打印最后获取HPA的过程；否则调用 print_pte 函数。\n具体的打印代码不再赘述，下面只展示最后如何使用代码得到HPA。\nlinux-4.19.0/arch/x86/kvm/mmu.c\n**对于2MB大页的情况，**最后一级页表项是PDE，这类页表项的第51：21位表示大页的起始物理地址，操作如下：\n这里使用PT64_DIR_BASE_ADDR_MASK宏从ent页表项中获取大页的起始物理地址； 接下来，使用 PT64_LVL_OFFSET_MASK(2)|(PAGE_SIZE-1) 获取GPA中的大页偏移的部分，即20：0位； 最终，结合大页起始地址和大页偏移得到HPA，最后调用 __va 函数获取对应的HVA，并解引用该HVA，读取到数据0xdeadbeef，符合预期。 **对于普通4KB页的情况，**PTE中的第51：12位表示其指向的物理页的起始地址，操作如下：\n使用PT64_BASE_ADDR_MASK宏从PTE中获取页的物理地址； 再使用PAGE_SIZE-1从GPA中获取页偏移，即11：0位； 最后结合页的物理地址和页偏移得到GPA，最后调用 __va 函数得到HVA，并解引用该HVA，读取到数据0xdeadbeef，符合预期。 综上所述，客户机内核模块在一个GVA处写入了0xdeadbeef，读取客户机页表得到GVA对应的GPA，通过超级调用传递GPA到KVM模块，KVM读取EPT将GPA翻译成HPA，最后通过_va函数找到HPA对应的HVA，并读取到0xdeadbeef，表明HPA处确实存储了GVA处的数据，地址翻译成功。在翻译过程中，实验代码打印了地址翻译所涉及的页表项、GPA、HPA等，环环相扣。其中，ept-dump.txt文件存储了完整的输出信息。\n4 //TODO: GiantVM内存虚拟化 ","date":"2024-10-17T10:06:35+08:00","permalink":"https://zcxggmu.github.io/p/kvmbook-mem_3/","title":"Kvmbook Mem_3"},{"content":"[TOC]\n1 概述 本章将深入介绍CPU虚拟化部分。早期由于缺乏相应硬件的支持，只能采用陷入-模拟、扫描与修补、二进制翻译等软件模拟的方式解决“虚拟化漏洞”问题，效率较低。而随着Intel VT-x等虚拟化硬件技术的出现，硬件辅助CPU虚拟化技术逐渐成为主流。本文将按照如下顺序进行介绍：\n简要概述CPU虚拟化 介绍Intel VT-x提供的CPU虚拟化硬件支持 QEMU/KVM CPU虚拟化实现 QEMU/KVM 中断虚拟化实现 介绍新型“多虚一”开源项目GiantVM中CPU虚拟化和中断虚拟化的实现。 首先回顾物理环境中CPU的主要功能。作为运算单元，CPU从主存中取出指令并执行，在此过程中CPU需要从寄存器或主存中获取操作数，并将结果写回寄存器或主存。此外，CPU还需要响应一些发生的系统事件，这些系统事件可能是由指令执行触发，如除零错误、段错误等；也有可能是由外部事件触发，如网卡收到了一个网络包、磁盘数据传输完成等。在这些事件中，最受关注的就是中断和异常事件。简而言之，**CPU应当能高效正确地执行所有指令并响应一些发生的系统事件，这也是虚拟环境中vCPU应当完成的工作。**但在虚拟环境中，要实现上述功能却面临一些挑战。\n1.1 敏感非特权指令的处理 敏感指令、特权指令与敏感非特权指令 在现代计算机架构中，CPU通常拥有两个或两个以上的特权级，其中操作系统运行在最高特权级，其余程序则运行在较低的特权级。而一些指令必须运行在最高特权级中，若在非最高特权级中执行这些指令将会触发特权级切换，陷入最高特权级中，这类指令称为**特权指令。在虚拟化环境中，还有另一类指令称为敏感指令，**即操作敏感物理资源的指令，如I/O指令、页表基地址切换指令等。\n虚拟化系统的三个基本要求：资源控制、等价与高效。资源控制要求Hypervisor能够控制所有的物理资源，虚拟机对敏感物理资源（部分寄存器、I/O设备等）的访问都应在Hypervisor的监控下进行。这意味着在虚拟化环境中，Hypervisor应当替代操作系统运行在最高特权级，管理物理资源并向上提供服务，当虚拟机执行敏感指令时必须陷入Hypervisor（通常称为虚拟机下陷）中进行模拟，这种敏感指令的处理方式称为“陷入-模拟”方式。\n“陷入-模拟” 方式要求所有的敏感指令都能触发特权级切换，从而能够陷入Hypervisor中处理，通常将所有敏感指令都是特权指令的架构称为可虚拟化架构，反之存在敏感非特权指令的架构称为不可虚拟化架构。遗憾的是，大多数计算机架构在设计之初并未将虚拟化技术考虑在内。\n以早期的x86架构为例，其SGDT（Store Global Descriptor Table，存储全局描述符表）指令将GDTR（Global Descriptor Table Register，全局描述符表寄存器）的值存储到某个内存区域中，其中全局描述符表用于寻址，属于敏感物理资源，但是在x86架构中，SGDT指令并非特权指令，无法触发特权级切换。\n在x86架构中类似SGDT的敏感非特权指令多达17条，Intel将这些指令称为“虚拟化漏洞”。**在不可虚拟化架构下，为了使Hypervisor截获并模拟上述敏感非特权指令，一系列软件方案应运而生，**下面介绍这些软件解决方案。\n敏感非特权指令的软件解决方案 敏感非特权指令的软件解决方案主要包括解释执行、二进制翻译、扫描与修补以及半虚拟化技术。\n**解释执行技术。**解释执行技术采用软件模拟的方式逐条模拟虚拟机指令的执行。解释器将程序二进制解码后调用指令相应的模拟函数，对寄存器的更改则变为修改保存在内存中的虚拟寄存器的值。 **二进制翻译技术。**区别于解释执行技术不加区分地翻译所有指令，二进制翻译技术则以基本块为单位，将虚拟机指令批量翻译后保存在代码缓存中，基本块中的敏感指令会被替换为一系列其他指令。 **扫描与修补技术。**扫描与修补技术是在执行每段代码前对其进行扫描，找到其中的敏感指令，将其替换为特权指令，当CPU执行翻译后的代码时，遇到替换后的特权指令便会陷入Hypervisor中进行模拟，执行对应的补丁代码。 **半虚拟化技术。**上述三种方式都是通过扫描二进制代码找到其中敏感指令，半虚拟化则允许虚拟机在执行敏感指令时通过超调用主动陷入Hypervisor中，避免了扫描程序二进制代码引入的开销。 上述解决方案的优缺点如下表所示：\n这几种方案通过软件模拟解决了敏感非特权指令问题，但却产生了巨大的软件开销。**敏感非特权指令究其本质是硬件架构缺乏对于敏感指令下陷的支持，**近年来各主流架构都从架构层面弥补了“虚拟化漏洞”，解决了敏感非特权指令的陷入-模拟问题，下面简要介绍这些硬件解决方案。\n敏感非特权指令的硬件解决方案 前面提到，敏感非特权指令存在的根本原因是硬件架构缺乏对敏感指令下陷的支持。因此最简单的一种办法是更改现有的硬件架构，将所有的敏感指令都变为特权指令，使之能触发特权级切换，但是这将改变现有指令的语义，现有系统也必须更改来适配上述改动。\n另一种办法是**引入虚拟化模式。**未开启虚拟化模式时，操作系统与应用程序运行在原有的特权级，一切行为如常，兼容原有系统；开启虚拟化模式后，Hypervisor运行在最高特权级，虚拟机操作系统与应用程序运行在较低特权级，虚拟机执行敏感指令将会触发特权级切换陷入Hypervisor中进行模拟。虚拟化模式与非虚拟化模式架构如图2-1所示。\n注：①陷入；②恢复；③开启虚拟化模式；④关闭虚拟化模式。\n非虚拟化模式通常只需要两个特权级，而虚拟化模式需要至少三个特权级用于区分虚拟机应用程序、虚拟机操作系统与Hypervisor的控制权限，此外还需要引入相应的指令开启和关闭虚拟化模式。虚拟化模式对现有软件影响较小，Hypervisor能够作为独立的抽象层运行于系统中，因此当下大多数虚拟化硬件都采用该方式：\nIntel VT-x为CPU引入了根模式与非根模式，分别供Hypervisor和虚拟机运行； ARM v8在原有EL0与EL1的基础上引入了新的异常级EL2供Hypervisor运行； RISC-V Hypervisor Extension则添加了两个额外的特权级，即VS/VU供虚拟机操作系统和虚拟机应用程序运行，原本的S特权级变为HS，Hypervisor运行在该特权级下。 虚拟化模式的引入解决了敏感非特权指令的陷入以及系统兼容性问题，但是**特权级的增加也带来了上下文切换问题。**下面介绍虚拟化环境中的上下文切换。\n1.2 虚拟机上下文切换 在操作系统的进程上下文切换中，操作系统与用户态程序运行在不同的特权级中，当用户态程序发起系统调用时，需要将部分程序状态保存在内存中，待系统调用完成后再从内存中恢复程序状态。\n而在虚拟化环境下，当虚拟机执行敏感指令时，需要陷入Hypervisor进行处理，Hypervisor与虚拟机同样运行在不同的特权级中，因此硬件应当提供一种机制在发生虚拟机下陷时保存虚拟机的上下文。等到敏感指令模拟完成后，当虚拟机恢复运行时重新加载虚拟机上下文。\n此处，“虚拟机上下文”表述可能有些不准确，更准确的说法应当是“vCPU上下文”。\n一个虚拟机中可能包含多个vCPU，虚拟机中指令执行单元是vCPU，Hypervisor调度虚拟机运行的基本单位也是vCPU。当vCPU A执行敏感指令陷入Hypervisor时，vCPU B将会继续运行。在大部分Hypervisor中，vCPU对应一个线程，通过分时复用的方式共享物理CPU。\n以vCPU切换为例说明上下文切换的流程，vCPU切换流程如下图所示，其中实线表示控制流，虚线表示数据流。\n注：①保存vCPU寄存器；②加载Hypervisor寄存器；③保存Hypervisor寄存器；④加载vCPU寄存器；⑤指令执行顺序。\n可以看到：当vCPU 1时间片用尽时，Hypervisor将会中断vCPU 1执行，vCPU 1陷入Hypervisor中（见图中标号I）。在此过程中，硬件将vCPU 1的寄存器状态保存至固定区域（见图中标号①），并从中加载Hypervisor的寄存器状态（见图中标号②）。Hypervisor进行vCPU调度，选择下一个运行的vCPU，保存Hypervisor的寄存器状态（见图中标号③），并加载选定的vCPU 2的寄存器状态（见图中标号④），而后恢复vCPU 2运行（见图中标号II）。\n上述固定区域与系统架构实现密切相关，以Intel VT-x/ARMv8为例：\n在Intel VT-x中，虚拟机与Hypervisor寄存器状态保存在VMCS（Virtual Machine Control Structure，虚拟机控制结构）中，VMCS是内存中的一块固定区域，通过 VMREAD/VMWRITE 指令进行读写； 而ARM v8则为EL1和EL2提供了两套系统寄存器，因此单纯发生虚拟机下陷时，无须保存寄存器状态；但是虚拟机下陷后，若要运行其他vCPU，则需要将上一个vCPU状态保存至内存中。 后续章节将以Intel VT-x为例介绍上下文切换过程中VMCS的具体用法。\n1.3 中断虚拟化 前面提到，vCPU不仅要能高效地执行所有指令，还要能正确处理系统中出现的中断和异常事件。大部分异常（如除零错误、非法指令等）无须虚拟化，直接交给虚拟机操作系统处理即可。而对于部分需要虚拟化的异常，则需要陷入Hypervisor中进行相应的处理，没有固定的解决方案。\n内存虚拟化文档中将会介绍Hypervisor如何处理虚拟机缺页异常。本节主要关注中断虚拟化的相关内容。\n中断是外部设备请求操作系统服务的一种方式，通常由外部设备发起，经由中断控制器发送给CPU。以磁盘为例，在物理环境下，操作系统发起一个读磁盘请求，磁盘将操作系统请求的数据放置在指定位置后给中断控制器发送一个中断请求，中断控制器接收后设置好内部相应寄存器，CPU每次执行指令前都会检查中断控制器中是否存在未处理的中断，若有则调用相应的ISR（Interrupt Service Routine，中断服务例程）进行处理。\n**而在虚拟环境下，可能存在多个虚拟机同时运行的情况，它们通过虚拟设备共用物理磁盘，此时若磁盘产生一个物理中断，该中断应该交给哪一个虚拟机的操作系统处理呢？**即如何将该中断注入发起读磁盘操作的虚拟机中，使该虚拟机操作系统执行相应的ISR。\n在虚拟化环境下，设备与中断控制器均由Hypervisor模拟，相应寄存器的状态对应于内存中某些数据结构的值。当虚拟机执行I/O指令时，会陷入Hypervisor中进行处理，Hypervisor调用相应的设备驱动完成I/O操作。在上述过程中，Hypervisor不仅知道发起I/O操作的虚拟机的详细信息，还能通过设置虚拟设备和虚拟中断控制器的寄存器状态从而完成中断注入。因此一个理想的方案是将该物理中断交给Hypervisor处理，再由Hypervisor设置虚拟中断控制器，注入一个虚拟中断到虚拟机中。\n仍以读磁盘为例，中断虚拟化流程如下图所示：\n注：①向虚拟磁盘发起读请求；②虚拟机下陷；③Hypervisor向物理磁盘发起读请求；④向中断控制器提交中断；⑤执行中断服务例程；⑥Hypervisor设置虚拟磁盘与虚拟中断控制器；⑦恢复虚拟机执行；⑧虚拟机执行中断服务例程。\n虚拟机发起一个读磁盘请求（见图中标号①）触发虚拟机下陷进入Hypervisor进行处理（见图中标号②），Hypervisor向物理磁盘发起读磁盘请求（见图中标号③），物理磁盘完成数据读请求后产生一个中断并递交给物理中断控制器（见图中标号④），Hypervisor执行相应的中断服务例程完成数据读取（见图中标号⑤），并设置虚拟磁盘和虚拟中断控制器相应寄存器的状态（见图中标号⑥），而后Hypervisor恢复虚拟机运行（见图中标号⑦），虚拟机发现有待处理的中断，调用相应的中断服务例程（见图中标号⑧）。\n上述流程对硬件有如下要求：\n物理中断将会触发虚拟机下陷进入Hypervisor中处理； 虚拟机恢复运行时要先检查虚拟中断控制器是否有待处理中断。 后续章节将以Intel VT-x为例说明虚拟化硬件如何完成这两点要求。\n**通过虚拟机下陷将物理中断转换为虚拟中断解决了多虚拟机系统中物理中断的路由问题，**但是对于直通设备却很不友好。直通设备是指通过硬件支持将一个物理设备直通给某个虚拟机使用，该设备由这个虚拟机独占，故该设备产生的中断也应当由这个虚拟机处理，无须经过Hypervisor路由。\n为了解决这个问题，ELI（Exitless Interrupt，不退出中断）引入了Shadow IDT（Shadow Interrupt Descriptor Table，影子中断描述符表），区分直通设备产生的中断与其他物理中断，直通设备产生的中断直接递交给虚拟机处理，无须Hypervisor介入。 DID（Direct Interrupt Delivery，直接中断交付）则进一步通过将虚拟设备产生的中断转换为物理IPI（Inter-Processor Interrupt，处理器间中断）直接递交给虚拟机进行处理。 相较于上述软件方案，硬件厂商提供了新的硬件机制，**直接将中断注入正在运行的虚拟机且不会引发虚拟机下陷，**如Intel公司的发布-中断(Posted-Interrupt)机制和ARM公司的ITS（Interrupt Translation Service，中断翻译服务）机制。Directvisor便使用Posted-Interrupt机制将直通设备产生的中断注入虚拟机中。\n“多虚一”环境下的中断虚拟化则更为复杂，节点A上I/O设备产生的中断可能需要注入节点B上运行的vCPU中。为了解决上述问题，GiantVM在每一个物理节点上都创建一个虚拟中断控制器，并选定一个节点作为主节点，其他节点上的虚拟中断控制器接收到中断信号时，会将该信号转发给主节点上的虚拟中断控制器，设置相应寄存器的值，而后由主节点虚拟中断控制器决定将该中断注入哪个vCPU中。\n2 Intel VT-x硬件辅助虚拟化 前文介绍了CPU虚拟化面临的一些挑战和可能的软硬件解决方案。本节将以Intel VT-x为例介绍上述问题在x86架构下是如何解决的。自2005年首次公布硬件辅助虚拟化技术以来，Intel公司陆续推出了针对处理器虚拟化的Intel VT-x技术、针对I/O虚拟化的Intel VT-d技术和针对网络虚拟化的Intel VT-c技术，它们统称为Intel VT（Intel Virtualization Technology，英特尔虚拟化技术）。\n本节将着眼于Intel VT-x中与CPU虚拟化相关的部分，Intel EPT（Extended Page Table，扩展页表）内存虚拟化技术和Intel VT-d I/O虚拟化技术将在其他文档中介绍。\n2.1 VMX操作模式 为了弥补 “虚拟化漏洞”，Intel VT-x引入了VMX（Virtual Machine eXtension，虚拟机扩展）操作模式，CPU可以通过 VMXON/VMXOFF 指令打开或关闭VMX操作模式。VMX操作模式类似于前述虚拟化模式，包含根模式与非根模式，**其中Hypervisor运行在根模式，虚拟机则运行在非根模式，**VMX模式下敏感指令处理示意图如下图所示：\nCPU从根模式切换为非根模式称为VM-Entry，从非根模式切换为根模式则称为VM-Exit。值得注意的是，根模式与非根模式都有各自的特权级(Ring0~Ring3)，虚拟机操作系统和应用程序分别运行在非根模式的Ring0和Ring3特权级中，而Hypervisor通常运行在根模式的Ring0特权级，解决了三者的特权级的划分问题。\nIntel VT-x改变了非根模式下敏感非特权指令的语义，使它们能够触发VM-Exit，而根模式指令的语义保持不变。处于非根模式的虚拟机执行敏感指令将会触发VM-Exit，陷入Hypervisor进行处理。Hypervisor读取VM-Exit相关信息，造成VM-Exit的原因（如I/O指令触发、外部中断触发等）并进行相应处理。处理完成后，Hypervisor调用 VMLAUNCH/VMRESUME 指令从根模式切换到非根模式，恢复虚拟机运行。\n2.2 VMCS 为了解决1.2节提到的虚拟机上下文切换问题，Intel VT-x引入了VMCS。\nVMCS是内存中的一块区域，用于在 VM-Entry 和 VM-Exit 过程中保存和加载Hypervisor和虚拟机的寄存器状态； 此外，VMCS还包含一些控制域，用于控制CPU的行为。 本节将简要介绍VMCS的组成与使用方式。\nVMCS操作指令 在多处理器虚拟机中，VMCS与vCPU一一对应，每个vCPU都拥有一个VMCS。当vCPU被调度到物理CPU上运行时，首先要将其VMCS与物理CPU绑定，物理CPU才能在 VM-Entry/VM-Exit 过程中将vCPU的寄存器状态保存到VMCS中。Intel VT-x提供了两条指令，分别用于绑定VMCS和解除VMCS绑定：\nVMPTRLD \u0026lt;VMCS地址\u0026gt;：将指定VMCS与当前CPU绑定。 VMCLEAR \u0026lt;VMCS地址\u0026gt;：同样以VMCS地址为操作数，将VMCS与当前CPU解除绑定，该指令确保CPU缓存中的VMCS数据被写入内存中。 在发生vCPU迁移时，需要先在原物理CPU上执行 VMCLEAR 指令，而后在目的物理CPU上执行 VMPTRLD 指令。\n此外，VMCS虽然是内存区域，但是**Intel软件开发手册指出通过读写内存的方式读写VMCS数据是不可靠的，因为VMCS数据域格式与架构实现是相关的，而且部分VMCS数据可能位于CPU缓存中，尚未同步到内存中。Intel VT-x提供 VMREAD 与 VMWRITE 指令用于读写VMCS数据域，**指令格式如下：\nVMREAD \u0026lt;索引\u0026gt; ：读取索引指定的VMCS数据域。 VMWRITE \u0026lt;索引\u0026gt; \u0026lt;数据\u0026gt;：将数据写入索引指定的VMCS数据域。 VMCS结构 VMCS区域大小不固定，最多占用4KB，VMCS的具体大小可以通过查询MSR（Model Specific Register，特殊模块寄存器）IA32_VMX_BASIC[32∶44] 得知。VMCS结构如下图所示：\nVMCS版本标识符 (VMCS revision identifier) 指明了VMCS数据域 (VMCS data) 的格式，VMX中止指示符 (VMX-abort indicator) 则记录了VMX的中止原因。VMCS数据域则包括以下六部分：\n**客户机状态域(Guest-state area)。**保存虚拟机寄存器状态的区域，主要包括CR0和CR3等控制寄存器、栈指针寄存器RSP、PC寄存器RIP等重要寄存器的状态。\n**宿主机状态域(Host-state area)。**与客户机状态域类似，是保存Hypervisor寄存器状态的区域，它包含的寄存器与客户机状态域大致相同。\n**VM-Execution控制域(VM-Execution control fields)。**控制客户机在非根模式下运行时的行为，如哪些指令会触发VM-Exit、外部中断是否引发VM-Exit等。\n在1.3节提到，将物理中断转化为虚拟中断注入虚拟机要求物理中断能够触发虚拟机下陷，这一特性由VM-Execution控制域中的外部中断退出 (External-Interrupt Exiting) 字段控制，当该位置为1时，外部中断将会触发VM-Exit，若为0则不会触发VM-Exit，直接由Guest处理。ELI 和 DID 都利用了该特性直接递交中断。\n**VM-Exit控制域(VM-Exit control fields)。**控制VM-Exit过程中的某些行为，如VM-Exit过程中需要加载哪些MSR。\n**VM-Entry控制域(VM-Entry control fields)。**控制VM-Entry过程中的某些行为，如VM-Entry后CPU的运行模式。\n**VM-Exit信息域(VM-Exit information fields)。**用以保存VM-Exit的基本原因及其他详细信息。\n从以上描述不难发现，VMCS对硬件辅助虚拟化具有极其重要的意义，几乎影响了虚拟化的方方面面。下图主要展示了VMCS在VM-Entry和VM-Exit过程中的作用，后续章节还会涉及部分VMCS域的具体功能。\n注：①保存客户机状态；②加载宿主机状态；③Hypervisor获取退出原因；④保存宿主机状态；⑤加载客户机状态；⑥指令执行顺序。\n2.3 PIC/APIC 在多虚拟机环境下，可以通过虚拟机下陷将物理中断转换为虚拟中断，解决物理中断的路由问题。在此过程中，Hypervisor通过虚拟中断控制器模拟真实环境下中断的分发和传送，完成虚拟中断的注入，因此在介绍中断虚拟化之前有必要先了解物理中断控制器及其工作方式。\n本节将简要介绍Intel x86架构下的两种中断控制器——PIC（Programmable Interrupt Controller，可编程中断控制器）和APIC（Advanced Programmable Interrupt Controller，高级可编程中断控制器）及它们各自的中断处理流程。\nPIC PIC，即Intel 8259A芯片，是单处理器 (Uni-processor) 时代广泛使用的中断控制器。它主要包含8个中断引脚 IR0~IR7，用于连接外部设备以及三个内部寄存器：IMR（Interrupt Mask Register，中断屏蔽寄存器）、IRR（Interrupt Request Register，中断请求寄存器）和 ISR（Interrupt Service Register，中断服务寄存器）。\nIMR：共8位，对应于IR0~IR7，置1表示相应引脚中断被屏蔽； IRR：共8位，对应于IR0~IR7，置1表示收到相应引脚的中断信号； ISR：共8位，对应于IR0~IR7，置1表示收到相应引脚的中断信号正在被CPU处理。 其中 IR0~IR7 用于连接外部设备，连接到不同的中断引脚对应的中断优先级不同，其中 IR0 优先级最高，IR7 优先级最低。PIC逻辑如下：\n每当外设需要发送中断时，便会拉高相连中断引脚的电平； 若相关中断未被屏蔽，就设置 IRR 中相应位并拉高 INT 引脚电平通知CPU有中断到达； CPU给 INTA 引脚发送一个脉冲确认收到中断，8259A芯片收到上述 INTA 脉冲信号后，将 IRR 最高优先级位清零并将 ISR 对应位置1； 而后CPU发送第二次脉冲，8259A芯片收到后将最高优先级的中断向量号发送给到数据线上，CPU接收中断向量号并执行相应的中断服务例程； 中断处理完成后，CPU向8259A的命令寄存器写入指定值并发送一个 EOI（End of Interrupt，中断结束）信号； 8259A芯片收到 EOI 后，将 ISR 中的最高优先级位清零。 根据上述描述，8259A芯片最多支持8个中断源，而为了支持更多的外设，通常将若干个8259A芯片级联，最多可以支持64个中断。\nAPIC APIC是20世纪90年代Intel公司为了应对多处理器 (Multi-Processor) 架构提出的一整套中断处理方案，用于取代老旧的8259A PIC。APIC适用于多处理器机器，每个CPU拥有一个LAPIC（Local APIC，本地APIC），整个机器拥有一个或多个IOAPIC（I/O APIC，输入/输出APIC），设备的中断信号先经由IOAPIC汇总，再分发给一个或多个CPU的LAPIC。\nLAPIC中也存在 IRR 和 ISR 以及 EOI 寄存器，IRR 和 ISR 的功能与PIC中相应寄存器类似，大小为256位，对应x86平台下的256个中断向量号。在APIC中断架构下，LAPIC的中断来源可分为以下3类：\n**本地中断。**本地中断包括 LINT0 和 LINT1 引脚接收的中断、APIC Timer产生的中断、性能计数器产生的中断、温度传感器产生的中断以及APIC内部错误引发的中断。\n**通过IOAPIC接收的外部中断。**当I/O设备通过IOAPIC中断引脚产生中断信号时，IOAPIC从内部的PRT（Programmable Redirection Table，可编程重定向表）中找到相应的RTE（Redirection Table Entry，重定向表项）。PRT共有24条PTE，与IOAPIC的24个中断引脚对应，每条PTE长度为64位。IOAPIC根据PTE中存储的信息（如触发方式、中断向量号、目的处理器等）格式化出一条中断消息发送到系统总线上。\n在PIC中，中断优先级由所连接的8259A芯片引脚决定，而在APIC中，中断优先级由中断向量号（也称为vector）决定，范围为0~255，ISR 和 IRR 每位对应一个中断向量号，置1表示收到该中断向量号的中断。中断向量号越大，中断优先级越高。IRR 和 ISR 的最大中断向量号记为 IRRV 和 ISRV。\n**处理器间中断(IPI)。**CPU可以通过写入APIC的 ICR（Interrupt Control Register，中断控制寄存器）发送一条IPI消息到系统总线上，从而发送给CPU。\nLAPIC收到中断消息后，确认自己是否是中断消息的目标，如果是，则对该中断消息进一步处理，这一步称为中断路由(Interrupt Routing)。LAPIC接收中断消息后，处理过程如下：\n如果收到的中断是NMI、SMI、INIT、ExtINT或SIPI，则直接交给CPU处理，否则设置 IRR 寄存器中适当的位，将中断加入等待队列，这一步称为中断接受(Interrupt Acceptance)。 对于在 IRR 中阻塞的中断，APIC取出其中优先级最高的中断（要求IRRV[7∶4]\u0026gt;PPR[7∶4]）交给CPU处理，并根据中断向量号设置 ISR 寄存器中相应的位。这一步称为中断确认(Interrupt Acknowledgement)。 CPU通过中断向量号索引 IDT（Interrupt Descriptor Table，中断描述符表）执行相应的中断处理例程，这一步称为中断交付(Interrupt Delivery)。 中断处理例程执行完毕时，应写入 EOI 寄存器，使得APIC从 ISR 队列中删除中断对应的项，结束该中断的处理（NMI、SMI、INIT、ExtINT及SIPI不需要写入EOI）。 MSI MSI（Message Signaled Interrupt，消息告知中断）是PCI总线发展出的新型中断传送方式，它允许设备直接发送中断到LAPIC而无须经过IOAPIC。MSI本质上就是在中断发生时，不通过带外(out-band)的中断信号线，而是通过带内(in-band)的PCI写入事务来通知LAPIC中断的发生。\n从原理上来说，MSI产生的事务与一般的DMA事务并无本质区别，需要依赖特定平台的特殊机制从总线事务中区分出MSI并赋予其中断的语义。\n在x86平台上，是由Host Bridge/Root Complex负责这一职责，将MSI事务翻译成系统上的中断消息，但凡目标地址落在 [0xfee00000，0xfeefffff] 的写入事务都会被视为MSI中断请求并翻译成中断消息。\n一个MSI事务由地址和数据构成，每个设备可以配置其发生中断时产生MSI事务的地址和数据，并且可以在不同事件发生时产生不同的MSI事务（即不同的地址-数据对）。MSI中断消息到达LAPIC后的处理流程与上述过程一致。\nIRQ \u0026amp; GSI \u0026amp; Vector \u0026amp; Pin 在正式介绍中断虚拟化之前，还需要介绍几个重要的概念：IRQ（Interrupt Request，中断请求）、GSI（Global System Interrupt，全局系统中断号）、Vector（中断向量号）和 Pin（中断引脚号）。这几个概念在后续章节将会反复提及，故在此进行区分。\n**IRQ：**IRQ是PIC时代的产物，但是如今可能还会见到IRQ线或者IRQ号等各种说法。在PIC中断架构下，通常将两块8259A芯片级联，支持16个中断，由于ISA设备通常连接到固定的8259A中断引脚，因此设备的IRQ号通常是指它所连接的8259A引脚号。\n如前所述，8259A有8个中断引脚( IR0~IR7 )，那么连接到这些主8259A芯片 IR0~IR7 引脚的设备中断号分别对应 IRQ0~IRQ7，连接到从8259A芯片 IR0~IR7 引脚的设备对应的中断号为 IRQ8~IRQ15。而IRQ线可以理解为设备与这些引脚的连线，命名方式通常也为 IRQx，它们通常与中断控制器引脚一一对应，故使用IRQ时应当注意相应的情境。\n**GSI：GSI是ACPI（Advanced Configuration and Power Interface，高级配置和电源管理接口）引入的概念，它为系统中每个中断控制器的输入引脚指定了一个全局统一的编号。**例如系统中有多个IOAPIC，每个IOAPIC都会被BIOS分配一个基础GSI (GSI Base)，每个IOAPIC中断引脚对应的GSI为基础GSI+Pin。\n比如IOAPIC 0的基础GSI为0，有24个引脚，则它们分别对应 GSI0~GSI23。在APIC系统中，IRQ和GSI通常会被混用，15号以上的IRQ号与GSI相等；而15号以下的IRQ号与ISA设备高度耦合，只有当相应的ISA设备按照对应的IRQ号连接到IOAPIC 0的1~15引脚时，IRQ才和GSI相等，这种情况称为一致性映射。而若IRQ与GSI引脚不一一对应，ACPI将会维护一个ISO（Interrupt Source Override，中断源覆盖）结构描述IRQ与GSI的映射。如PIT（Programmable Interrupt Timer，可编程中断时钟）接PIC的IR0引脚，因此其IRQ为0；但当接IOAPIC时，它通常接在2号中断引脚，所以其GSI为2。而在QEMU/KVM中，GSI和IRQ完全等价，但是不符合前述基础GSI+Pin的映射关系。\n**Vector：**中断向量号是操作系统中的概念，是中断在 IDT 中的索引。每个GSI都对应一个Vector，它们的映射关系由操作系统决定。x86中通常包含256个中断向量号，031号中断向量号是x86预定义的，32255号则由软件定义。\n**Pin：**中断控制器的中断引脚号，对于8259A而言，其Pin取值为 0~7；对于IOAPIC，其Pin取值为 0~23。\n根据以上描述，IRQ、GSI和Vector都可以唯一标识系统中的中断来源，IRQ和GSI的映射关系以及GSI和Pin的映射关系由ACPI设置，IRQ和Vector的映射关系由操作系统设置。\n2.4 Intel VT-x中断虚拟化 中断虚拟化的一般思路中，最为关键的两点便是：\n中断设备的模拟（外部设备和中断控制器的模拟） 中断处理流程的模拟 这也是早期Intel VT-x中断虚拟化采用的方法，后来Intel公司推出了 APICv（APIC Virtualization，APIC虚拟化）技术对上述两个功能进行了优化。本节将分别介绍传统中断虚拟化和APICv支持的中断虚拟化。\n传统中断虚拟化 在传统中断虚拟化中，Hypervisor会创建虚拟 IOAPIC 和虚拟 LAPIC，以下统称为虚拟APIC。它们通常表现为存储在内存中的数据结构，而其中的寄存器则作为结构体中的若干域 fields。\n默认情况下，虚拟机通过MMIO（Memory-Mapped I/O，内存映射I/O）的方式访问虚拟APIC，但是由于底层并没有相应的物理硬件供其访问，故Hypervisor需要截获这些MMIO请求进而访问存储在内存中的虚拟寄存器的值。\nHypervisor通常会将虚拟机中APIC内存映射区域相应的EPT项状态设为不存在，这样虚拟机访问这一段地址空间时，便会触发缺页错误从而触发VM-Exit陷入Hypervisor中进行模拟。\n此外，**虚拟APIC仍需要接收和传递中断。**在物理环境下，外部设备产生的中断会通过连接线传输至APIC进而发送到CPU，在此过程中，硬件会设置相应寄存器的值。而在虚拟环境中，这些连接线则会被替换为一系列的函数调用，并在调用过程中设置内存中相应虚拟寄存器的值。\nAPIC中断处理流程包括**中断产生、中断路由、中断接受、中断确认和中断交付。**其中，中断产生到中断确认部分都是通过设置特定APIC寄存器完成，对于虚拟APIC而言，可以通过设置内存中相应的虚拟寄存器实现同样的功能。而对于中断交付而言，**需要硬件提供某种机制，使得虚拟机恢复运行时发现待处理的虚拟中断从而执行相应的ISR。**在Intel VT-x中，这是通过VMCS VM-Entry控制域中的VM-Entry中断信息字段（32位）实现的，该字段保存了待注入事件的类型（中断或异常等）和向量号等信息，其格式如下图所示：\n每次触发VM-Entry时，CPU会检查该域，发现是否有待处理的事件并用向量号索引执行相应的处理函数。\n当Hypervisor需要向正在运行的vCPU注入中断时，需要给vCPU发送一个信号，使其触发VM-Exit，从而在VM-Entry时注入中断。如果vCPU正处于屏蔽外部中断的状态，如vCPU的 RFLAGS.IF=0，将不允许在VM-Entry时进行中断注入。此时可以将VM-Execution控制域中的中断窗口退出(Interrupt-Window Exiting)字段置为1，这样一旦vCPU进入能够接收中断的状态，便会产生一个VM-Exit，Hypervisor就可以注入刚才无法注入的中断，并将中断窗口退出字段置为0。\nAPICv支持的中断虚拟化 APICv是Intel公司针对APIC虚拟化提出的优化技术，它主要包括两方面内容：虚拟APIC访问优化和虚拟中断递交。\n虚拟APIC访问 前面提到，当虚拟机通过MMIO的方式访问APIC寄存器时，需要VM-Exit陷入Hypervisor中设置内存中相应虚拟寄存器的值，这将会触发大量的VM-Exit，严重影响虚拟机的性能。于是APICv引入虚拟APIC页 (Virtual APIC Page) 的概念，它相当于一个影子APIC，虚拟机对APIC的部分甚至全部访问都可以被硬件重定向为对虚拟APIC页的访问，这样就不必频繁触发VM-Exit了。要启用虚拟APIC页，需要使能如下三个VM-Execution控制域字段：\n**影子TPR(Use TPR Shadow)字段。**该字段置1后，虚拟机访问CR8时将会自动访问虚拟APIC页中的TPR寄存器。否则，虚拟机访问CR8时将会触发VM-Exit。 **虚拟APIC访问(Virtualize APIC Accesses)字段。**该字段置1后，虚拟机对于APIC访问页(APIC Access Page)的访问将会触发APIC访问(APIC Access)异常类型的VM-Exit。单独设置该域时，与以前通过设置EPT页表项触发VM-Exit相比，仅仅只是将VM-Exit类型从EPT违例(EPT Violation)变为了APIC访问。需要进一步设置APIC寄存器虚拟化(APIC-Register Virtualization)字段消除VM-Exit。 **APIC寄存器虚拟化字段。**该字段置1后，通常虚拟机对于APIC访问页的访问将被重定向到虚拟APIC页而不会触发VM-Exit，但部分情况下仍需要VM-Exit到Hypervisor中处理。 使用APICv前后，虚拟APIC访问方式如下图所示：\n虚拟中断递交 而虚拟中断递交 (Virtual-Interrupt Delivery) 通过VM-Execution控制域中的虚拟中断递交字段开启。在引入该机制前，Hypervisor需要设置 VIRR和 VISR 相应位（虚拟APIC中的 IRR 和 ISR），然后通过上文提到的事件注入机制在下一次VM-Entry时注入一个虚拟中断，调用客户机中相应的中断处理例程。\n开启虚拟中断递交后，虚拟中断注入通过VMCS客户机状态域中的客户机中断状态 (Guest Interrupt Status) 字段完成：\n其低8位为 RVI（Requesting Virtual Interrupt，待处理虚拟中断），表示虚拟机待处理中断中优先级最高的中断向量号，相当于 IRRV ； 而高8位为 SVI（Servicing Virtual Interrupt，处理中虚拟中断），表示虚拟机正在处理中断中优先级最高的中断向量号，相当于 ISRV。 开启虚拟中断传送后，Hypervisor只需要设置 RVI 的值，在VM-Entry时，CPU将会根据RVI的值进行虚拟中断提交，过程如下：\n若VM-Execution控制域中断窗口退出 (Interrupt-Window Exiting) 字段为0且 RVI[7∶4] \u0026gt; VPPR[7∶4]，则确认存在待处理的虚拟中断，其中VPPR指的是vCPU的PPR寄存器； 根据 RVI，清除 VIRR 中对应位，设置 VISR 中对应位，并设置 SVI=RVI； 设置 VPPR = RVI \u0026amp; 0xf0； 若 VIRR 中还有非零位，则设置 RVI=VIRRV，即 VIRR 中优先级最高的中断向量号，否则设置 RVI=0； 根据 RVI 提供的中断向量号，调用虚拟机中注册的中断处理例程。 在上述流程中，中断确认和中断交付工作将由硬件自动完成，Hypervisor无须手动设置虚拟APIC中 VIRR 和 VISR 寄存器的值。此外，设置 RVI后，即使当前vCPU处于屏蔽中断的状态也无妨，硬件会持续检查vCPU是否能够接收中断。一旦vCPU能接收中断，则立即进行虚拟中断交付，无须再通过前述中断窗口产生VM-Exit注入中断。\n虚拟中断虽然省略了中断确认和中断递交的过程，但是中断接受仍需要Hypervisor完成。当Hypervisor需要将虚拟中断注入vCPU时，必须使其发生VM-Exit并设置好 RVI 的值，才能顺利进行后续操作。而发布-中断 (Posted-Interrupt) 机制可以省略中断接受的过程，直接让正在运行的vCPU收到一个虚拟中断，而不产生VM-Exit。\n发布-中断机制还可以配合VT-d的发布-中断功能使用，实现直通设备的中断直接发给vCPU而不引起VM-Exit。\n发布-中断机制通过VM-Execution控制域中的发布-中断处理 (Process Posted-Interrupt) 字段开启，它引入了发布-中断通知向量(Posted-Interrupt Notification Vector)和发布-中断描述符(Posted-Interrupt Descriptor)。其中发布-中断描述符保存在内存中，而发布-中断描述符的地址保存在VMCS的发布-中断描述符地址(Posted-Interrupt Descriptor Address)字段中。发布-中断描述符的格式如下图所示：\n其中，各字段作用：\nPIR（Posted-Interrupt Requests，发布-中断请求）是一个256位的位图，与 IRR 类似，相应位置1表示有待处理中断； ON（Outstanding Notification，通知已完成）表示是否已经向CPU发送通知事件（ON本质上是一个物理中断），向CPU通知 PIR 中有待处理中断。 发布-中断处理字段置1后，当处于非根模式的CPU收到一个外部中断时，它首先完成中断接受和中断确认，并取得中断向量号。然后，若中断向量号与发布-中断通知向量相等，则进入发布-中断处理流程，否则照常产生VM-Exit。发布-中断处理流程如下：\n清除发布-中断描述符的 ON 位； 向CPU的 EOI 寄存器写入0并执行，至此在硬件APIC上该中断已经处理完毕； 令 VIRR |= PIR ，并清空 PIR； 设置 RVI = max(RVI,PIRV)，其中 PIRV 为 PIR 中优先级最高的中断向量号； CPU根据 RVI 按照前述流程递交虚拟中断。 从上述流程不难发现，只需要将待注入的中断放置在发布-中断描述符的 PIR 中，并向CPU发送一个发布-中断通知，CPU就会自动将 PIR 中存储的虚拟中断同步到 RVI 中，无须Hypervisor手动设置RVI的值。\n通过发布-中断机制便可以在不发生VM-Exit的情况下向vCPU中注入一个或者多个虚拟中断。\n3 QEMU/KVM CPU虚拟化实现 本节以x86架构下的QEMU/KVM实现为例，介绍前述硬件虚拟化技术如何被应用到实践中。\nQEMU原本是纯软件实现的一套完整的虚拟化方案，支持CPU虚拟化、内存虚拟化以及设备模拟等，但是性能不太理想。随着硬件辅助虚拟化技术逐渐兴起，Qumranet公司基于新兴的虚拟化硬件实现了KVM。KVM遵循Linux的设计原则，以内核模块的形式动态加载到Linux内核中，利用虚拟化硬件加速CPU虚拟化和内存虚拟化流程，I/O虚拟化则交给用户态的QEMU完成，QEMU/KVM架构如下图所示：\nCPU虚拟化主要关心上图的左侧部分，即vCPU是如何创建并运行的，以及当vCPU执行敏感指令触发VM-Exit时，QEMU/KVM又是如何处理这些VM-Exit的。整体流程如下：\n当QEMU启动时，首先会解析用户传入的命令行参数，确定创建的虚拟机类型（通过QEMU-machine参数指定）与CPU类型（通过QEMU-cpu参数指定），并创建相应的机器模型和CPU模型； 而后QEMU打开KVM模块设备文件并发起 ioctl(KVM_CREATE_VM)，**请求KVM创建一个虚拟机。**KVM创建虚拟机相应的结构体并为QEMU返回一个虚拟机文件描述符； QEMU通过虚拟机文件描述符发起 ioctl(KVM_CREATE_VCPU) ，**请求KVM创建vCPU。**与创建虚拟机流程类似，KVM创建vCPU相应的结构体并初始化，返回一个vCPU文件描述符； QEMU通过vCPU文件描述符发起 ioctl(KVM_RUN)，vCPU线程执行 VMLAUNCH 指令**进入非根模式，**执行虚拟机代码直至发生VM-Exit； KVM根据VM-Exit的原因进行相应处理，如果与I/O有关，则需要进一步返回到QEMU中进行处理。 以上就是QEMU/KVM CPU虚拟化的主要流程。\n本节将从KVM模块初始化、虚拟机创建、vCPU创建和vCPU运行四个方面进行介绍，最后给出CPU虚拟化实例。\n在没有特殊说明的情况下，后续章节所列出的示例代码对应的**Linux内核版本为4.19.0，QEMU版本为4.1.1，**由于篇幅有限，示例代码只摘取了源码中的一部分。\n3.1 KVM模块初始化 前面提到QEMU通过设备文件 /dev/kvm 发起ioctl系统调用请求KVM创建虚拟机，该设备文件便是在KVM模块初始化时创建的。\nkvm-intel.ko 模块初始化函数为 vmx_init，该函数将会调用 kvm_init 函数进而调用 misc_register 函数**注册kvm_dev这一misc设备，**代码如下：\nlinux-4.19.0/arch/x86/kvm/vmx.c\nlinux-4.19.0/virt/kvm/kvm_main.c\n注册成功后会在 /dev/ 目录下产生名为kvm的设备节点，即 /dev/kvm。该设备文件对应的fd（文件描述符）的file_operations为kvm_chardev_ops。该设备仅支持ioctl系统调用，其中最重要的便是 ioctl(KVM_CREATE_VM) 。KVM模块接收该系统调用时将会创建虚拟机。kvm_chardev_ops 定义及具体代码如下：\nlinux-4.19.0/virt/kvm/kvm_main.c\n实际上，KVM模块初始化时，除了创建设备文件，还做了许多与架构相关的初始化工作，例如：\nkvm_init 接收的第一个参数vmx_x86_ops是一个函数指针集合，封装了Intel VT-x相关虚拟化操作。对于AMD-V而言，KVM则提供了另一个函数指针集合svm_x86_ops。KVM会根据当前所处平台选择将vmx_x86_ops或svm_x86_ops赋给 arch/x86/kvm/x86.c 中的全局变量kvm_x86_ops，这样后续通用x86虚拟化操作可以通过这一变量的相应成员调用 arch/x86/kvm/vmx.c 中的相应函数。kvm_x86_ops的设置工作由 kvm_arch_init 函数完成。 此外，kvm_init 函数还调用了 kvm_hardware_setup 函数，该函数最终会调用 arch/86/kvm/vmx.c 中的 hardware_setup 函数完成虚拟化硬件的初始化工作，这与Intel VT-x虚拟化硬件支持密切相关。部分 hardware_setup 代码如下： linux-4.19.0/arch/x86/kvm/vmx.c\nsetup_vmcs_config 函数读取物理CPU中与VMX相关的MSR（特殊模块寄存器），检测物理CPU对VMX的支持能力，生成一个 vmcs_config 结构，后续将根据 vmcs_config 设置VMCS控制域； 而后 hardware_setup 函数调用 cpu_has_vmx_ept 等函数判断CPU是否支持EPT，若不支持，则将全局变量 enable_ept 置0。 alloc_kvm_area 函数则会调用 alloc_vmcs_cpu 函数**为每一个物理CPU分配一个4KB大小的区域作为VMXON区域。**Intel SDM指出执行VMXON指令需要提供一个4KB对齐的内存区间，即VMXON区域。VMXON区域的物理地址后续将作为VMXON指令的操作数。 3.2 虚拟机创建 虚拟机的创建始于 kvm_init 函数，该函数由QEMU调用 TYPE_KVM_ACCEL 类型QOM（QEMU Object Model，QEMU对象模型）对象的init_machine 成员来调用。由于篇幅有限，这里不再详述，仅画出相关函数调用流程，如下图所示。\n注：①QEMU进行ioctl系统调用。\nKVM模块初始化后，kvm_init 函数便可以通过前述 /dev/kvm 设备文件发起 ioctl(KVM_CREATE_VM) ，请求创建虚拟机，相关代码如下：\nqemu-4.1.1/accel/kvm/kvm-all.c\nkvm_init 函数首先打开 /dev/kvm 设备文件，然后通过相应的文件描述符发起 ioctl(KVM_GET_API_VERSION) 获取KVM模块的接口版本，检验与QEMU支持的KVM版本是否相等。随后 kvm_init 函数发起 ioctl(KVM_CRETAE_VM) 便会陷入前述KVM模块中的 kvm_dev_ioctl 函数进行处理。该函数根据ioctl类型调用 kvm_dev_ioctl_create_vm 函数创建虚拟机，相关代码如下：\nlinux-4.19.0/virt/kvm/kvm-main.c\nkvm_dev_ioctl_create_vm 首先调用 kvm_create_vm 函数创建虚拟机对应的结构体，然后调用 anon_inode_getfd 函数为虚拟机创建一个匿名文件，对应的file_operations为 kvm_vm_fops，其定义如下：\nlinux-4.19.0/virt/kvm/kvm_main.c\n这个文件为QEMU提供了虚拟机层级的API，如创建vCPU、创建设备等。KVM最终会将该文件的文件句柄作为返回值返回给QEMU供其使用。\nkvm_create_vm 函数除了创建虚拟机对应的结构体以外，还会调用 hardware_enable_all 函数，该函数最终会在所有的物理CPU上调用hardware_enable_nolock 函数，该函数最终会通过前述kvm_x86_ops的 hardware_enable 成员调用vmx.c中的 hardware_enable 函数使能VMX操作模式。\nhardware_enable 函数将获取为每个CPU分配的VMXON区域的物理地址作为参数传入 kvm_vcpu_vmxon 函数。kvm_vcpu_vmxon 函数会**设置CR4寄存器中的 VMXE(VMX Enabled) 位使能VMX操作模式，并执行VMXON指令进入VMX操作模式。**代码如下：\nlinux-4.19.0/virt/kvm/kvm_main.c\nlinux-4.19.0/arch/x86/kvm/vmx.c\n3.3 vCPU创建 虚拟机创建完成后，QEMU便可以通过前述虚拟机文件描述符发起系统调用，请求KVM创建vCPU，完整的创建流程如下图所示：\n注：①QEMU进行ioctl系统调用。\n在QEMU/KVM中，每个vCPU对应宿主机操作系统中的一个线程，由QEMU创建，其执行函数为 qemu_kvm_cpu_thread_fn，该函数将调用kvm_init_vcpu 函数进而调用 kvm_get_vcpu 函数发起 ioctl(KVM_CREATE_VCPU)，其代码如下：\nqemu-4.1.1/accel/kvm/kvm_all.c\n同虚拟机创建一样，KVM会为每个vCPU创建一个vCPU文件描述符返回给QEMU，然后QEMU发起 ioctl(KVM_GET_VCPU_MMAP_SIZE) 查看QEMU与KVM共享内存空间的大小。\n其中共享内存空间的第一个页将被映射到KVM vCPU结构体 struct kvm_vcpu 的run成员，该成员将保存一些VM-Exit相关信息，如VM-Exit的原因以及部分虚拟机寄存器状态等，便于QEMU处理VM-Exit。kvm_init_vcpu 函数然后调用 mmap 函数将其映射到QEMU的虚拟地址空间中。\n接下来，我们重点看KVM中的vCPU创建流程。\nQEMU发起 ioctl(KVM_CREATE_VCPU) 后将陷入KVM模块进行处理，处理函数为 kvm_vm_ioctl_create_vcpu。kvm_vm_ioctl_create_vcpu函数首先调用 kvm_arch_vcpu_create 函数创建vCPU对应的结构体，然后调用 create_vcpu_fd 函数进而调用 anon_inode_getfd 为vCPU创建对应的文件描述符，对应的file_operations为 kvm_vcpu_fops。\nkvm_vcpu_fops 定义了vCPU文件描述符对应的ioctl系统调用的处理函数，该函数为 kvm_vcpu_ioctl。主要代码如下。\nlinux-4.19.0/virt/kvm/kvm_main.c\n值得注意的是，kvm_arch_vcpu_create 函数除了创建vCPU对应的结构体以外，还完成了vCPU相应的VMCS初始化工作。前面提到每个vCPU都有一个VMCS与其对应，使用时需要执行 VMPTRLD 指令将其与物理CPU绑定。\nkvm_arch_vcpu_create 函数通过前述kvm_x86_ops的 vcpu_create 成员调用vmx.c中的 vmx_create_vcpu 函数。vmx_create_vcpu 函数调用 alloc_loaded_vmcs 函数进而调用 alloc_vmcs_cpu 函数为vCPU分配VMCS结构，然后调用 vmx_vcpu_load 函数进而调用 vmcs_load 函数执行 VMPTRLD 指令将VMCS与当前CPU绑定，相关代码如下：\nlinux-4.19.0/arch/x86/kvm/vmx.c\n绑定之后，vmx_create_vcpu 函数调用 vmx_vcpu_setup 函数通过若干 vmcs_write16/vmcs_write32/vmcs_write64 函数进而调用__vmcs_writel 函数设置VMCS中相关域的值，如前所述，vmcs_writexx 函数最终会执行 VMWRITE 指令写VMCS，相关代码如下：\nlinux-4.19.0/arch/x86/kvm/vmx.c\nvmcs_setup_config 函数主要设置VMCS中控制域的值，客户机状态域和宿主机状态域的初始化工作则由 kvm_vm_ioctl_create_vcpu 函数调用 kvm_arch_vcpu_setup 函数进而调用 kvm_vcpu_reset 函数完成。kvm_vcpu_reset 函数通过kvm_x86_ops的 vcpu_reset 成员调用vmx.c中的 vmx_vcpu_reset 函数完成，部分代码如下：\nlinux-4.19.0/arch/x86/kvm/vmx.c\n根据上述代码，KVM会调用 vmcs_writel 函数将VMCS中客户机的代码段寄存器 GUEST_CS_SELECTOR 设置为0xf000，将代码段基地址GUEST_CS_BASE 设置为0xffff0000。而 kvm_rip_write 函数则会将KVM模拟的虚拟 RIP（Return Instruction Pointer，返回指令指针）寄存器设为0xfff0，当后续调用 vmx_vcpu_run 函数运行vCPU时，该函数会将模拟的 RIP 寄存器值写入VMCS中，部分代码如下：\nlinux-4.19.0/arch/x86/kvm/vmx.c\n这与Intel x86架构硬件要求相符。在Intel x86架构下，计算机加电后会将 CS（Code Segment，代码段）寄存器设置为0xf000，RIP 寄存器设置为0xfff0，而CS寄存器中隐含的代码段基地址则设置为 0xffff0000，这样当程序启动时，执行的第一条指令位于0xfffffff0处 CS_BASE+RIP。\n值得注意的是，这里KVM仅仅是对VMCS中的客户机状态域进行初始化，QEMU仍可以通过 KVM API 设置VMCS客户机状态域。QEMU在调用kvm_cpu_exec 函数运行虚拟机代码前，首先调用 kvm_arch_put_registers 函数，该函数将会调用 kvm_getput_regs 函数和 kvm_put_sregs 函数，通过前述vCPU设备文件描述符发起 ioctl(KVM_SET_REGS) 和 ioctl(KVM_SET_SREGS) ，分别设置vCPU通用寄存器和段寄存器的值，KVM会进而将QEMU传入的寄存器值写入VMCS中。具体寄存器的值则由 x86_cpu_reset 函数指定，相关代码如下：\nqemu-4.1.1/target/i386/cpu.c\nqemu-4.1.1/target/i386/kvm.c\n根据上述代码，QEMU传入的 CS 和 RIP 寄存器的值与默认VMCS相应域的设置相同：CS=0xf000，RIP=0xfff0，CS_BASE=0xffff0000。\n这里通过一个小实验验证上述代码的有效性。QEMU提供了- s 和 -S 选项允许GDB远程连接调试内核，其中 -s 选项使得QEMU等待来自1234端口的TCP连接，-S 选项则使得QEMU阻塞客户机执行，直到远程连接的GDB（Linux下常用的程序调试器）允许它继续执行，这允许用户方便地在GDB中查看虚拟机运行过程中物理寄存器的状态。\n可以打开两个终端，终端1 Terminal 1 启动QEMU等待GDB连接，终端2 Terminal 2 则运行GDB通过1234端口远程连接至QEMU，命令如下：\nTerminal 1\nTerminal 2\n根据上述实验，vCPU启动后，CS 寄存器和 RIP 寄存器的值与QEMU设置的一致，说明虚拟机启动时硬件会将VMCS中相应域加载到寄存器中。\n而为了兼容早期8086架构，程序启动后，0xffff0 和 0xfffffff0 内存处指令一致，都是跳转至BIOS进行初始化，加载bootloader。在0xfffffff0 处设置断点，运行虚拟机后发现断点被触发；而删去断点后，虚拟机正常启动，说明程序的入口位于 0xfffffff0 处。改写后的x86_cpu_reset 函数代码如下：\nqemu-4.1.1/target/i386/cpu.c\n如上所述，将CS寄存器设置为 0x1234，将 RIP 寄存器设置为 0x5678，重新编译QEMU并运行上述代码，实验结果如下。\nTerminal 3\n从GDB输出可以发现上述寄存器设置生效，感兴趣的读者可以自行修改QEMU target/i386/cpu.c 中的 x86_cpu_reset 函数，重复上述实验。\n以上便是QEMU/KVM vCPU创建与初始化流程，与虚拟机创建类似，vCPU创建的起点实际上位于 TYPE_X86_CPU 类型的QOM对象的realize_fn成员中。该成员在CPU对应的QOM对象初始化时被设置为 x86_cpu_realizefn 函数，该函数调用 qemu_init_vcpu 函数创建了vCPU线程，线程执行函数为前述的 qemu_kvm_cpu_thread_fn。\n3.4 vCPU运行 在QEMU中，vCPU线程创建完成后，便会进入循环状态，等到条件满足时开始运行虚拟机代码。虚拟机执行敏感指令或收到外部中断时，便会触发VM-Exit，进入KVM模块进行处理；部分虚拟机退出时，则需要进一步退出到QEMU中进行处理，如I/O模拟。处理完成后，恢复虚拟机运行。vCPU整体运行流程如下图所示：\nQEMU/KVM vCPU运行函数调用流程如下图所示：\n注：①QEMU进行ioctl系统调用，进入KVM；②从ioctl系统调用返回。\n接下来遵循上述流程查看QEMU/KVM中vCPU运行是如何实现的，如下：\n首先 qemu_kvm_cpu_thread_fn 函数调用 kvm_init_vcpu 函数在KVM中创建vCPU对应的结构体，得到相应的vCPU文件描述符后便会进入循环； 在循环中，先判断CPU能否运行，若不能运行便调用 qemu_kvm_wait_io_event 函数进而调用 qemu_cond_wait 函数等待 cpu-\u0026gt;halt_cond； 而QEMU后续在 main 函数中将会调用 vm_start 函数直至最终调用 qemu_cond_broadcast 函数唤醒所有的vCPU线程，相关代码如下： qemu-4.1.1/cpus.c\nvCPU线程被唤醒后将执行 kvm_cpu_exec 函数运行vCPU。kvm_cpu_exec 函数首先调用前述 kvm_arch_put_registers 函数设置vCPU通用寄存器和段寄存器的值，然后调用 kvm_vcpu_ioctl 通过vCPU文件描述符发起 ioctl(KVM_RUN) 调用陷入KVM中。\nKVM中相应的处理函数为前述 kvm_arch_vcpu_ioctl_run，该函数首先调用 vcpu_load 函数，该函数最终通过kvm_x86_ops的 vcpu_load 成员调用前述 vmx_vcpu_load 函数执行VMPTRLD指令绑定该vCPU对应的VMCS，然后调用 vcpu_run 函数运行vCPU。\nvcpu_run 函数首先调用 kvm_vcpu_running 函数判断该vCPU能否运行，若能运行则调用 vcpu_enter_guest 函数准备进入非根模式。具体代码如下：\nlinux-4.19.0/arch/x86/kvm/x86.c\nvcpu_enter_guest 函数首先判断vCPU是否存在待处理的请求，并调用一系列 kvm_check_request 函数对这些请求进行处理。这些请求可能在各个地方发生，以中断事件注入为例，当虚拟IOAPIC需要注入一个虚拟中断时，会调用 kvm_make_request 函数发起一个KVM_REQ_EVENT类型的请求，设置 kvm_vcpu 结构中的requests成员对应的位，并在 vcpu_enter_guest 函数中进行处理。vcpu_enter_guest 函数检查requests发现有KVM_REQ_EVENT类型的请求，则调用 inject_pending_event 函数注入相应事件，后续中断虚拟化部分将详述该流程。具体代码如下。\nlinux-4.19.0/arch/x86/kvm/x86.c\n阻塞请求处理完成后，vcpu_enter_guest 函数通过kvm_x86_ops的 prepare_guest_switch 成员调用vmx.c中的vmx_prepare_switch_to_guest 函数，该函数将部分宿主机的状态保存到VMCS与 host_state 中，如 FS/GS 相应的段选择子和段地址等。具体代码如下：\nlinux-4.19.0/arch/x86/kvm/vmx.c\n然后 vcpu_enter_guest 函数通过kvm_x86_ops的run成员调用vmx.c中的 vmx_vcpu_run 函数，该函数通过一系列内联汇编指令保存宿主机通用寄存器的值，并加载客户机通用寄存器的值，然后**调用 VMLAUNCH/VMRESUME 指令进入非根模式执行虚拟机代码。**具体代码如下：\nlinux-4.19.0/arch/x86/kvm/vmx.c\n当vCPU发生VM-Exit时，vmx_vcpu_run 函数保存虚拟机通用寄存器的值并从VMCS中读取虚拟机退出原因保存至 vmx-\u0026gt;exit_reason 中，然后返回至 vcpu_enter_guest 函数。\nvcpu_enter_guest 函数通过kvm_x86_ops的 handle_exit 成员调用vmx.c中的 vmx_handle_exit 函数，该函数根据前面的 vmx-\u0026gt;exit_reason 调用全局数组 kvm_vmx_exit_handlers 中对应的虚拟机退出处理函数。\n对于由I/O指令触发的EXIT_REASON_IO_INSTRUCTION类型的VM-Exit，它对应的处理函数 handle_io 最终调用 emulator_pio_in_out 函数进行处理。\nemulator_pio_in_out 函数首先调用 kernel_pio 函数尝试在KVM中处理该PIO请求，若KVM无法处理，它将 vcpu-\u0026gt;run-\u0026gt;exit_reason 设置为KVM_EXIT_IO并最终返回0，这导致 vcpu_run 退出循环并返回至QEMU中进行处理。前面提到vCPU中的run成员主要保存VM-Exit相关信息并通过 mmap 与QEMU共享，故退出至QEMU后，QEMU将读取 kvm_run 结构中的exit_reason成员，根据其退出原因进行进一步处理。具体代码如下：\nlinux-4.19.0/arch/x86/kvm/vmx.c\nlinux-4.19.0/arch/x86/kvm/x86.c\n3.5 实验: CPU虚拟化实例 前几节从源码层级分析了QEMU/KVM CPU虚拟化的完整流程，不难发现大部分工作都由KVM完成，QEMU主要负责维护虚拟机和vCPU模型，并通过KVM模块文件描述符、虚拟机文件描述符和vCPU文件描述符调用KVM API接口，本节实现了一个类似于QEMU的小程序，执行指定二进制代码输出 Hello，World!。主要代码如下：\nsample-qemu.c\n代码流程如下：\n通过前面所述的若干ioctl调用创建并运行vCPU，将vCPU起始指令地址设置为 0x1000(GPA)，并将指定的二进制代码复制到相应的内存位置(HVA)； 二进制代码将依次调用OUT指令向 0x3f8 端口写入 Hello，World! 包含的各个字符，触发 EXIT_REASON_IO_INSTRUCTION 类型的VM-Exit，这使得程序退回到用户态进行处理，用户态程序调用 putchar 函数输出对应字符； 二进制代码最终执行 hlt 指令触发VM-Exit，系统回到用户态并退出应用程序。 4 QEMU/KVM 中断虚拟化实现 本节将结合QEMU/KVM源码介绍中断虚拟化的具体实现，包括：\nPIC与APIC等中断控制器的模拟 虚拟中断递交流程的模拟 本节在最后将通过GDB查看中断从产生到注入的完整流程。\n4.1 PIC/IOAPIC模拟 QEMU和KVM都实现了对中断芯片的模拟，这是由于历史原因造成的。早在KVM诞生之前，QEMU就提供对一整套设备的模拟，包括中断芯片。而KVM诞生之后，为了进一步提高中断性能，又在KVM中模拟了一套中断芯片。\nQEMU提供 kernel-irqchip 参数决定中断芯片由谁模拟，kernel-irqchip 参数可取值为：\non：PIC和APIC中断芯片均由KVM模拟； off：PIC和APIC中断芯片均由QEMU模拟； split：PIC和IOAPIC由QEMU模拟，LAPIC由KVM模拟。 由于KVM模拟中断芯片性能更高，故本节以KVM模拟中断芯片为例。\nPIC模拟 当QEMU设置 kernel-irqchip 参数为on时，会在前述 kvm_init 函数中调用 kvm_irqchip_create 函数，通过虚拟机文件描述符发起ioctl(KVM_CREATE_IRQCHIP) 陷入KVM中。KVM中相应处理函数 kvm_arch_vm_ioctl 调用 kvm_pic_init 函数创建并初始化PIC相应的结构体 struct kvm_pic，并将指针保存在 kvm-\u0026gt;arch.vpic 中，相关代码如下：\nlinux-4.19.0/arch/x86/kvm/irq.h\nlinux-4.19.0/arch/x86/kvm/x86.c\nlinux-4.19.0/arch/x86/kvm/i8259.c\n根据上述代码，PIC寄存器状态保存在结构体 struct kvm_kpic_state 中，该结构体包含了前述 IRR、ISR 以及 IMR 等的状态。\n此外，如前所述，通常将两块8259A芯片级联以支持更多的中断来源，kvm_pic 将主PIC和从PIC的状态保存在 pics 成员中，kvm_pic 对应的内存空间由 kvm_pic_init 函数分配。\n除了模拟PIC寄存器状态，kvm_pic_init 函数还提供CPU对PIC访问的支持。CPU通常通过PIO的方式访问PIC，于是 kvm_pic_init 函数首先调用 kvm_io_device_init 函数初始化主PIC、从PIC和eclr三个设备对应的读写操作函数，并调用 kvm_io_bus_register_dev 函数在KVM_IO_BUS上注册三个设备相应的I/O端口。\n当CPU通过PIO相关指令（IN、OUT等）通过注册端口对设备进行读写时，将会触发 EXIT_REASON_IO_INSTRUCTION 类型的VM-Exit，其对应的处理函数为 handle_io。handle_io 函数首先会判断I/O指令类型是否为string I/O指令，若是则调用 kvm_emulate_instruction 函数模拟I/O指令，否则调用 kvm_fast_pio 函数处理，二者最后都会调用 kernel_pio 函数。\n对于读/写端口指令，kernel_pio 函数分别调用 kvm_io_bus_read/kvm_io_bus_write 函数。以写端口为例，kvm_io_bus_write 函数将会调用 _kvm_io_bus_write 函数，该函数根据I/O端口从KVM_IO_BUS总线上找到相应的设备并调用 kvm_iodevice_write 函数写入设备端口，这会触发前述设备对应的读写操作函数。以主PIC为例，其对应的读/写操作函数代码如下：\nlinux-4.19.0/arch/x86/kvm/i8259.c\nlinux-4.19.0/include/kvm/iodev.h\nlinux-4.19.0/virt/kvm/kvm_main.c\nIOAPIC模拟 同PIC创建类似，IOAPIC相应的结构体 struct kvm_ioapic 由 kvm_arch_vm_ioctl 函数调用 kvm_ioapic_init 函数创建并初始化，同时将kvm_ioapic 指针赋给 kvm-\u0026gt;arch.vioapic 。相关代码如下：\nlinux-4.19.0/arch/x86/kvm/ioapic.c\n对于 kvm_ioapic 结构体而言，其中最重要的成员为 redirtbl，该成员对应于IOAPIC中的 PRT（Programmable Redirection Table，可编程重定向表）。如前所述，当IOAPIC某个引脚收到中断时，将根据该引脚对应的RTE格式化出一条中断消息发送给指定LAPIC，后续的中断路由章节将详述这一成员的作用。\nkvm_ioapic 结构体定义如下。\nlinux-4.19.0/arch/x86/kvm/ioapic.h\n与PIC不同的是，CPU通常通过MMIO的方式访问IOAPIC，故 kvm_ioapic_init 函数先调用 kvm_iodevice_init 函数，将IOAPIC MMIO区域访问回调函数设置为 ioapic_mmio_ops，然后调用 kvm_io_bus_register_dev 函数在KVM_MMIO_BUS上注册IOAPIC MMIO区域。IOAPIC MMIO区域的起始地址为 0xfec00000（由 kvm_ioapic_reset 函数指定），长度为 0x100。相关代码如下：\nlinux-4.19.0/arch/x86/kvm/ioapic.c\n当KVM首次访问MMIO区域时，由于EPT中缺乏相应的映射，会触发EPT Violation类型的VM-Exit，KVM发现该区域为MMIO区域，在为其建立相应的页表项时会将该页表项设置为可写可执行但不可读，这样在后续访问该MMIO区域时会触发EPT配置错误 (EPT Misconfig) 类型的VM-Exit，在kvm_vmx_exit_handlers 函数中相应的处理函数为 handle_ept_misconfig。与PIC访问类似，handle_ept_misconfig 函数最终会调用x86_emulate_instruction 函数对该指令进行模拟。\n在大部分情况下，由于外部设备由QEMU模拟，故 x86_emulate_instruction 函数将会返回EMULATE_USER_EXIT，导致 vcpu_enter_guest 函数返回至QEMU中处理。但因为IOAPIC由KVM模拟，故 x86_emulate_instruction 函数最终会调用 vcpu_mmio_read/vcpu_mmio_write 函数处理对于IOAPIC MMIO区域的读写。以 vcpu_mmio_write 函数为例，它同样调用前述 kvm_io_bus_write 函数，根据访问的内存地址在KVM_MMIO_BUS找到IOAPIC对应的设备，并调用前面注册的写回调函数 ioapic_mmio_write。vcpu_mmio_write 函数代码如下。\nlinux-4.19.0/arch/x86/kvm/x86.c\nLAPIC模拟 LAPIC创建路径与PIC和IOAPIC不同，由于LAPIC与vCPU一一对应，故LAPIC将在创建vCPU时构造，这一工作由前述 vmx_create_vcpu 函数最终调用 kvm_create_lapic 函数完成。kvm_create_lapic 函数首先分配LAPIC在KVM中对应的结构体 struct kvm_lapic，并将其保存在 vcpu-\u0026gt;arch.apic 中，然后分配LAPIC寄存器页保存LAPIC寄存器状态，寄存器页的起始地址保存在 kvm_lapic 的regs成员中。kvm_lapic 定义如下：\nlinux-4.19.0/arch/x86/kvm/lapic.h\nlinux-4.19.0/arch/x86/kvm/lapic.c\n与IOAPIC类似，CPU也通过MMIO的方式访问LAPIC。但是**根据不同的VM-Execution控制域设置，KVM对于LAPIC MMIO有三种处理方式，**这里仍以写MMIO区域为例：\n虚拟APIC访问字段为0\n此时LAPIC MMIO处理流程与IOAPIC相同，但是LAPIC MMIO区域并未在KVM_MMIO_BUS上注册。vcpu_mmio_write 函数调用 lapic_in_kernel 函数判断LAPIC是否由KVM模拟：\n若是，则直接调用 kvm_io_device_write 函数尝试写入LAPIC，这将会触发 apic_mmio_ops 函数中的 apic_mmio_write 回调函数，apic_mmio_write 函数将会调用 apic_mmio_in_range 函数判断访问的内存地址是否落在LAPIC MMIO区域中； 若不是，将访问地址减去 base_address 得到 offset（偏移地址），然后通过 kvm_lapic_reg_write 函数写入LAPIC相应的寄存器中。 apic_mmio_write 函数代码如下：\nlinux-4.19.0/arch/x86/kvm/lapic.c\n虚拟APIC访问字段为1且虚拟APIC寄存器虚拟化字段为0\n此时通过MMIO方式访问LAPIC时，会触发APIC访问类型的VM-Exit。LAPIC MMIO区域的地址通过APIC访问地址(APIC Access Address)字段指定。\n在前述vCPU创建流程中，将调用 vcpu_vmx_reset 函数，该函数会调用 kvm_make_request 函数发起一个 KVM_REQ_APIC_PAGE_RELOAD 类型的请求。后续vCPU调用 vcpu_enter_guest 函数运行前，会检查是否有待处理的请求。\n若有未处理的 KVM_REQ_APIC_PAGE_RELOAD 请求，vcpu_enter_guest 函数会调用 kvm_vcpu_reload_apic_access_page 函数处理，该函数通过 kvm_x86_ops 函数的 set_apic_access_page_addr 成员调用vmx.c中的 vmx_set_apic_access_page_addr 函数，将VMCS APIC访问地址字段设置为APIC MMIO区域起始地址对应的HPA。\n而APIC Access类型VM-Exit的处理函数为 handle_apic_access，该函数最终会调用 kvm_emulate_instruction 函数进而调用x86_emulate_instruction 函数，实际处理函数与上一种方式相同，只是VM-Exit类型变为APIC访问。相关代码如下。\nlinux-4.19.0/arch/x86/kvm/x86.c\nlinux-4.19.0/arch/x86/kvm/vmx.c\n虚拟APIC访问字段为1且APIC寄存器虚拟化字段为1\n此时CPU通过MMIO访问LAPIC时会被重定向到虚拟APIC页，而不会触发VM-Exit。在这种情况下，只有特定寄存器访问可能会触发APIC访问或APIC写(APIC Write)异常类型的VM-Exit，此处不再详述，读者可以参考Intel SDM相关资料了解哪些APIC寄存器访问会触发VM-Exit。\n而虚拟APIC页地址由VMCS中的虚拟APIC页地址 (Virtual-APIC Address) 字段指定，这一工作由前述vCPU创建流程中的 vmx_vcpu_reset 函数完成，虚拟APIC地址字段被设置为 kvm_lapic 中的regs成员，即寄存器页的物理地址。vmx_vcpu_reset 函数代码如下：\nlinux-4.19.0/arch/x86/kvm/vmx.c\n4.2 PCI设备中断 在APIC中断架构下，CPU中断来源主要有三种：本地中断、通过IOAPIC接收的外部中断以及处理器间中断。\n本节侧重于介绍通过IOAPIC接收的外部中断。但是由于芯片组的差异以及外部设备的差异，设备产生和传递中断的流程也不尽相同。为了与后续的实验部分相照应，本节主要介绍在 i440FX + PIIX 芯片组中PCI设备中断模拟。\n4.3节将会介绍QEMU模拟设备产生中断是如何传递给KVM模拟的APIC的。\nPCI设备中断原理 计算机主板上通常有多个PCI插槽供PCI设备使用，插入PCI槽的PCI卡通常称为物理PCI设备，每个物理PCI设备上可能存在多个独立的功能单元，这些功能单元也称为逻辑PCI设备。因此逻辑PCI设备的标识符通常包括三部分：设备所在的总线(Bus)号、设备所在的物理PCI设备(Device)号、设备的功能(Function)号，简称为BDF。\n每个物理PCI设备有4个中断引脚：INTA#、INTB#、INTC# 和 INTD#，单功能物理PCI设备只会使用 INTA# 引脚，多功能物理PCI设备可能会用到其余中断引脚。由于一个物理PCI设备最多包含8个逻辑PCI设备，故存在多个逻辑PCI设备共享同一个中断引脚的情况。\n逻辑PCI设备配置空间中的中断引脚 (Interrupt Pin:0x3D) 寄存器记录了该设备使用哪个中断引脚，1~4 分别对应 INTA#~INTD# 引脚。PCI总线通常提供4条或8条中断请求线供PCI设备使用，物理PCI设备中断引脚将连接到这些中断请求线上。假设PCI总线提供了4条中断请求线：LNKA、LNKB、LNKC 和 LNKD ，并非所有的PCI设备 INTA# 引脚都连接至固定 LNKA 中断请求线，因为通常大部分PCI设备都使用 INTA# 引脚，这样会造成各中断请求线负载不均衡的情况。PCI设备中断路由连接如下图所示：\n此时PCI总线中断请求线与PCI设备引脚映射关系为 LNK = (D+I) mod 4，其中 LNK 表示PCI总线中的中断请求线号，D 表示物理PCI设备号，I 表示物理PCI设备的中断引脚号。\n上述连接方式使得各物理PCI设备中断引脚交错连接至PCI总线中断请求线。而PCI总线中断请求线还需要连接至中断控制器，它们与中断控制器引脚之间的映射关系保存在BIOS中的中断路由表中。这样每个PCI逻辑设备都连接至中断控制器的一个中断引脚，中断控制器引脚号记录在PCI配置空间的中断线 (Interrupt Line, 0x3C) 寄存器中。当PCI设备发起中断请求时，便会通过映射的中断控制器引脚将中断递交给CPU。\nPCI设备中断模拟 在QEMU中，虚拟PCI设备调用 pci_set_irq 函数来**触发中断，**具体流程如下：\npci_set_irq 函数首先调用 pci_intx 函数获取该PCI设备使用的中断引脚，pci_intx 函数将会从该设备的PCI配置空间中读取中断引脚寄存器的值并减1； 然后 pci_set_irq 函数将调用 pci_irq_handler 处理PCI设备中断。pci_irq_handler 函数首先判断当前中断引脚的状态是否发生改变，若改变，则调用 pci_set_irq_state 函数设置设备中断引脚的状态，并调用 pci_update_irq_status 函数更新PCI设备的状态。若当前PCI设备中断未被禁用，则 pci_irq_handler 函数最终会调用 pci_change_irq_level 函数触发中断。相关代码如下： qemu-4.1.1/hw/pci/pci.c\npci_change_irq_level 函数首先获取设备所在的PCI总线，然后调用其 map_irq 回调函数获取该设备中断引脚所连接的中断请求线，最后调用所在PCI总线的 set_irq 回调函数触发中断。\n以QEMU i440FX+PIIX 架构为例，其PCI总线 map_irq 相应的回调函数为 pci_slot_get_pirq，set_irq 相应的回调函数为 piix3_set_irq，这两个回调函数在i440FX芯片组初始化函数 i440fx_init 中注册。i440fx_init 函数首先调用 pci_root_bus_new 函数创建PCI根总线，然后调用 pci_bus_irqs 函数设置PCI根总线对应QOM对象的 map_irq 成员和 set_irq 成员。相关代码如下：\nqemu-4.1.1/hw/pci-host/piix.c\nqemu-4.1.1/hw/pci/pci.c\npci_slot_get_pirq 函数首先获取PCI设备的设备号，然后通过设备号和它使用的PCI设备中断引脚获取该PCI设备连接至PCI总线中的哪一条中断请求线。QEMU规定i440FX芯片组中PCI设备与PCI总线中断线的映射关系为 (slot+pin)\u0026amp;3-\u0026gt;\u0026quot;LNK[D|A|B|C]\u0026quot;，而非 (slot+pin)\u0026amp;3-\u0026gt;\u0026quot;LNK[A|B|C|D]\u0026quot;。代码如下：\nqemu-4.1.1/hw/pci-host/piix.c\npiix3_set_irq 回调函数则主要调用 piix3_set_irq_level 函数，该函数首先访问PIIX3设备配置空间，获取PCI总线中断请求线对应的中断控制器引脚号。对于 PIIX3/PIIX4 来说，其配置空间中的中断请求线路由控制寄存器 ( PIRQRC[A:D],0x60~0x63 ) 分别记录了 LNKA~LNKD 中断请求线对应的中断控制器引脚号，这几个寄存器的值由BIOS设置。相关代码如下：\nqemu-4.1.1/hw/pci-host/piix.c\npiix3_set_irq_level 函数获得PCI设备对应的中断控制器引脚号后调用 piix3_set_irq_pic 函数。piix3_set_irq_pic 函数根据传入的中断控制器引脚号索引PIIX3State结构中的 pic 成员，该成员在 i440fx_init 函数中被设置为 pcms-\u0026gt;gsi，故 piix_set_irq_pic 函数本质上索引的是 pcms-\u0026gt;gsi。\npcms-\u0026gt;gsi 是**QEMU中断路由的起点，**其用法将在4.3节介绍。\npiix3_set_irq_pic 函数从 pcms-\u0026gt;gsi 中获得一个 qemu_irq 后将其传递给 qemu_set_irq 函数。\nqemu_irq 是一个指向IRQState结构体的指针，在QEMU中，IRQState通常用于表示设备或中断控制器的中断引脚，n 表示中断引脚号，opaque 由创建者指定，handler 表示该中断引脚收到信号时的处理函数。 qemu_set_irq 函数所做的工作其实就是调用传入 qemu_irq 的handler函数，相关代码如下： qemu-4.1.1/hw/core/irq.c\nqemu-4.1.1/hw/pci-host/piix.c\n4.3 QEMU/KVM中断路由 4.2节介绍了PCI设备中断的模拟，而外部设备产生的中断需要经由中断控制器才能传递给CPU，故本节将介绍**外部设备中断路由至中断控制器的完整流程。**本节仍以KVM模拟中断控制器为例。\nQEMU中断路由 QEMU中断路由的起点位于 pcms-\u0026gt;gsi，它本质上是一个 qemu_irq 数组，为所有的虚拟设备定义了统一的中断传递入口。QEMU根据传入的中断控制器引脚号以及系统当前使用的中断控制器类型，决定该中断如何进行传递。完整的QEMU中断路由流程如下图所示。\npcms 是PCMachineState类型的结构体，可以理解为整个虚拟机的设备模型，其 gsi 成员在虚拟机设备模型初始化函数 pc_init1 中创建：\npc_init1 函数首先调用 kvm_ioapic_in_kernel 函数判断IOAPIC芯片是否由KVM模拟； 若是，则调用 qemu_allocate_irqs 函数； 进而调用 qemu_extend_irqs 函数创建若干个IRQState结构体，其handler为kvm_pc_gsi_handler，opaque则指向预先分配的GSIState结构体。 相关代码如下：\nqemu-4.1.1/hw/i386/pc_piix.c\nqemu-4.1.1/hw/core/irq.c\npcms-\u0026gt;gsi 创建完成后，pc_init1 函数调用 i440fx_init 函数将 pcms-\u0026gt;gsi 赋给PIIX3State中的 pic 成员，相关代码如下：\nqemu-4.1.1/hw/pci-host/piix.c\n前面提到PCI设备中断模拟最终通过 qemu_set_irq 调用 PIIX3State pic 成员中的某个 qemu_irq 数组的handler，而其 pic 成员被设置为 pcms-\u0026gt;gsi。\n实际上对于ISA总线，pc_init1 函数也会调用 isa_bus_irqs 函数将 pcms-\u0026gt;irq 赋值给isabus的 irqs 成员。因此最终ISA设备和PCI设备中断都会调用 pcms-\u0026gt;gsi 中某个 qemu_irq 数组的handler，即前面注册的kvm_pc_gsi_handler 函数。kvm_pc_gsi_handler 函数接收三个参数：第一个参数 opaque 指向前述分配的GSIState，第二个参数为 IRQ 号，第三个参数为电平信号。\nGSIState包含两个成员：i8259_irq 和 ioapic_irq：\n**i8259_irq 对应PIC，**包含16个中断引脚（qemu_irq结构体），handler为 kvm_pic_set_irq； **ioapic_irq 成员对应IOAPIC，**包含24个中断引脚，handler为 kvm_ioapic_set_irq。 二者分别由 pc_init1 函数调用 kvm_i8259_init 和 ioapic_init_gsi 函数进行初始化。当某条IRQ线有中断信号时，QEMU将根据当前系统使用的中断控制器调用不同的handler。\n值得思考的是，当相应的IRQ号小于16时，kvm_pc_gsi_handler 便会调用 GSIState i8259_irq 中 qemu_irq 相应的回调函数，这并非意味着当前系统使用的就是PIC中断控制器。实际上当IRQ号小于16时，KVM会将中断信号同时发送给虚拟PIC和虚拟IOAPIC，虚拟机通过当前真正使用的中断控制器接收该中断信号。相关代码如下：\nqemu-4.1.1/include/hw/i386/pc.h\nqemu-4.1.1/hw/i386/pc_piix.c\nqemu-4.1.1/hw/i386/kvm/ioapic.c\n因此当虚拟设备调用 qemu_set_irq 函数触发中断时，最终会调用 kvm_pic_set_irq 函数或kvm_ioapic_set_irq 函数，而这两个函数最终都会调用 kvm_set_irq 函数通过ioctl将中断信号传入KVM进行处理。代码如下：\nqemu-4.1.1/hw/i386/kvm/i8259.c\nqemu-4.1.1/hw/i386/kvm/ioapic.c\nqemu-4.1.1/accel/kvm/kvm-all.c\nKVM中断路由 KVM中断路由由 struct kvm_irq_routing_table 完成，这一结构体功能类似于QEMU中的GSIState，根据传入的 IRQ 号调用不同的回调函数：\n当 IRQ \u0026lt; 16 时，KVM会同时将中断信号发送给虚拟PIC和虚拟IOAPIC，虚拟机会通过当前使用的中断控制器接收该中断信号，其余中断信号将被忽略； 当 IRQ \u0026gt; 16 时，中断信号只会发送给虚拟IOAPIC。 完整的KVM中断路由流程如下图所示：\n当中断芯片全部由内核模拟时，**KVM对应的中断路由信息由QEMU设置并传入KVM中，**这一工作由 pc_init1 函数调用 kvm_pc_setup_irq_routing 函数完成：\n该函数调用 kvm_irqchip_add_irq_route 函数为每个中断控制器的每个中断引脚都添加一条 kvm_irq_routing_entry，记录所属中断芯片编号、中断引脚号和对应的GSI号； 最后调用 kvm_irqchip_commit_routes 函数将中断路由信息提交给KVM。 值得注意的是，这里IOAPIC的2号中断引脚并不满足前述基础 GSI + Pin 映射关系，在QEMU/KVM中，GSI等同于IRQ。相应的，前述 kvm_set_irq 传入KVM的其实也是IRQ号。相关代码如下：\nlinux-4.19.0/include/uapi/linux/kvm.h\nqemu-4.1.1/hw/i386/kvm/ioapic.c\nKVM接收到QEMU传入的中断路由信息后，调用 kvm_set_irq_routing 函数构建 kvm_irq_routing_table。kvm_irq_routing_table 的组织方式与QEMU中的GSIState不同，其定义如下：\nlinux-4.19.0/include/linux/kvm_host.h\nchip 成员是一个二维数组，记录了每个中断芯片中断引脚对应的GSI号。map 成员是一个链表数组，nr_rt_entries 记录了它的长度。 对于每个GSI，map 成员都会为其保存一个链表，链表的每一项是 kvm_kernel_irq_routing_entry，该结构体类似于上述QEMU中的 kvm_irq_routing_entry，记录了GSI在某个中断芯片中对应的引脚号和中断回调函数。 对于PIC和IOAPIC，它们的某个引脚可能会映射到同一个GSI，map 成员为它们各自维护一条kvm_kernel_irq_routing_entry 保存在链表中。 当收到某个GSI信号后，KVM将遍历GSI相应的链表，调用每条 kvm_kernel_irq_routing_entry 中的set回调函数，将中断信号传递给各个中断芯片。 对于传入KVM中的每条 kvm_irq_routing_entry，kvm_set_irq_routing 函数都会调用 setup_routing_entry 函数将其转换为 kvm_kernel_irq_routing_entry，设置其set回调函数，并填充kvm_irq_routing_table 中的map成员。\nPIC路由表项对应的set函数为 kvm_set_pic_irq，IOAPIC路由表项对应的set函数为 kvm_ioapic_set_irq。相关代码如下：\nlinux-4.19.0/virt/kvm/irqchip.c\nlinux-4.19.0/arch/x86/kvm/irq_comm.c\n当QEMU调用前述 kvm_set_irq 函数陷入KVM时，调用流程如下：\nKVM将调用 kvm_vm_ioctl_irq_line 函数进而调用kvm_set_irq 函数进行处理； kvm_set_irq 函数首先调用 kvm_irq_map_gsi 函数，根据传入的GSI从 kvm_irq_routing_table 的map成员中取出该GSI对应的所有 kvm_kernel_irq_routing_entry 并调用其set函数。 至此，中断信号被传递到KVM模拟的中断芯片中。相关代码如下：\nlinux-4.19.0/virt/kvm/irqchip.c\n考虑到后续实验部分主要涉及IOAPIC，这里**以IOAPIC为例介绍中断芯片接收到中断信号后的处理流程。**前面提到IOAPIC对应的set函数为 kvm_set_ioapic_irq，该函数首先获取前述虚拟IOAPIC对应的 kvm_ioapic 结构体，然后调用 kvm_ioapic_set_irq 函数，其代码如下：\nlinux-4.19.0/arch/x86/kvm/irq_comm.c\nkvm_ioapic_set_irq 函数最终调用 ioapic_service 函数处理该中断请求，它首先以传入的IOAPIC中断引脚号为索引，查询 IOAPIC PRT以获得对应的RTE，PRT记录在前述 kvm_ioapic 的 redirtbl 成员中，它本质上是一个 kvm_ioapic_redirect_entry 数组，每一项记录了相应的中断引脚RTE，kvm_ioapic_redirect_entry 定义如下：\nlinux-4.19.0/arch/x86/kvm/ioapic.h\n获取相应的RTE后，即 kvm_ioapic_redirect_entry 结构体，ioapic_service 函数根据RTE创建一个中断请求保存在 kvm_lapic_irq 结构体中，并调用 kvm_irq_delivery_to_apic 函数将其传递给LAPIC。相关代码如下。\nlinux-4.19.0/arch/x86/kvm/ioapic.c\nkvm_irq_delivery_to_apic 函数最主要的工作是找到中断请求的目标LAPIC集合，可能包含一个或多个LAPIC，这与前述RTE中的 dest_mode 和 dest_id 成员密切相关，这里不再详述该流程。\n对于找到的每一个目标LAPIC，kvm_irq_delivery_to_apic 函数都会调用 kvm_apic_set_irq 函数将中断请求发送给它：\nkvm_apic_set_irq 函数首先从 vcpu-\u0026gt;arch.apic 中获得虚拟LAPIC对应的 kvm_lapic 结构体； 然后调用__apic_accept_irq 函数接收该中断请求。 至此，中断信号传递给LAPIC，与此同时 **GSI/IRQ 号也被转换为相应的中断向量号。**相关代码如下：\nlinux-4.19.0/arch/x86/kvm/lapic.c\n中断传送方式不同，即RTE中的 delivery_mode 不同，__apic_accept_irq 的处理也不同。以Fixed模式为例，其对应的宏为 APIC_DM_FIXED。代码如下：\nlinux-4.19.0/arch/x86/kvm/lapic.c\nLAPIC在设置 IRR 前，首先要设置 TMR（Trigger Mode Register，触发模式寄存器）。TMR 与 IRR、ISR 类似，都为256位，每一位与一个中断向量号相对应。对于边沿触发的中断，TMR 中对应位置0，对于水平触发的中断，TMR 中对应位置1。 然后 _apic_accept_irq 函数检查是否通过发布-中断机制递交虚拟中断？ 若是，则通过 kvm_x86_ops 的deliver_posted_interrupt 成员调用vmx.c中的 vmx_deliver_posted_interrupt 函数； 否则先调用 kvm_lapic_set_irr 函数设置虚拟 LAPIC IRR 的值。kvm_lapic_set_irr 函数将通过前述 kvm_lapic 的regs成员设置虚拟 IRR 并将其 kvm_lapic irr_pending 成员设为true； 接下来，__apic_set_irq 函数调用 kvm_make_request 函数发起一个KVM_REQ_EVENT类型的请求，并调用 kvm_vcpu_kick 函数通知vCPU。 如果当前vCPU线程正处于睡眠状态，kvm_vcpu_kick 函数将会调用 kvm_vcpu_wake_up 函数唤醒vCPU线程； 如果vCPU线程当前正在运行，kvm_vcpu_kick 函数将会调用 smp_send_reschedule 函数给其所在物理CPU发送一个IPI使vCPU发生VM-Exit。 无论是哪种情况，vCPU最终都会调用前述 vcpu_enter_guest 函数重新进入非根模式，而在此之前，vcpu_enter_guest 函数会调用 kvm_check_request 函数发现当前vCPU有待注入的中断请求从而调用inject_pending_event 函数注入该虚拟中断。 相关代码如下：\nlinux-4.19.0/arch/x86/kvm/lapic.h\nlinux-4.19.0/virt/kvm/kvm_main.c\n至此，KVM中断路由结束，4.4节将会介绍虚拟中断的注入流程。\n4.4 虚拟中断注入 前文提到虚拟中断注入由 vcpu_enter_guest 函数调用 inject_pending_event 函数完成。实际上，inject_pending_event 函数不仅可以注入中断事件，还能注入异常事件，但本节只关心中断注入的部分。\ninject_pending_event 函数首先会检查 vcpu-\u0026gt;arch.interrupt.injected 判断系统中是否已经存在待注入的虚拟中断。如在注入上一个虚拟中断过程中发生了VM-Exit，硬件将会把当前正在注入的中断信息保存在VMCS中断向量号信息 (IDT-Vectoring Information) 字段中，然后KVM将会读取该信息并调用 kvm_queue_interrupt 函数将中断信息保存到 vcpu-\u0026gt;arch.interrupt 成员中，并将 vcpu-\u0026gt;arch.interrupt.injected 设为true便于在下一次调用 vcpu_enter_guest 函数时重新注入该事件，这一工作由前述 vmx_vcpu_run 函数调用vmx_complete_interrupts 函数完成。相关代码如下：\nlinux-4.19.0/arch/x86/kvm/vmx.c\n若当前系统中不存在待注入中断，inject_pending_event 函数将会调用 kvm_cpu_has_injectable_intr 函数检查虚拟LAPIC中是否有待处理的中断。若有，则同样调用 kvm_queue_interrupt 函数保存该中断信息，并通过kvm_x86_ops 的set_irq成员调用vmx.c中 vmx_inject_irq 函数注入中断。相关代码如下：\nlinux-4.19.0/arch/x86/kvm/x86.h\nlinux-4.19.0/arch/x86/kvm/x86.c\nvmx_inject_irq 函数的主要工作是根据 vcpu.arch 中保存的中断信息设置前述VM-Entry控制域中断信息字段。在VM-Entry时，CPU会检查该字段发现有待处理的中断，并用中断向量号索引IDT以执行相应的处理函数。相关代码如下：\nlinux-4.19.0/arch/x86/kvm/x86.h\nlinux-4.19.0/arch/x86/kvm/vmx.c\n至此，**虚拟中断注入完成。**4.5节将以e1000网卡中断为例，介绍中断虚拟化的完整流程。\n4.5 实验: e1000网卡中断虚拟化 前几节介绍了当中断芯片全部由KVM模拟时，外部设备中断传送的完整流程。本节将以e1000网卡为例，通过GDB调试工具回顾前述流程。\n实验概述 为了使用GDB调试虚拟中断在KVM模块中的传送流程，本次实验需要在嵌套虚拟化的环境下进行，物理机和虚拟机使用的内核版本均为4.19.0。本节使用前述QEMU提供的 -s 和 -S 选项启动一个虚拟机。启动命令如下：\nPhysical Machine Terminal 1\n在终端2启动GDB加载Linux内核调试信息并连接至1234端口，然后开始运行虚拟机。运行命令如下：\nPhysical Machine Terminal 2\n为了加载KVM模块的调试信息，读者需要在虚拟机中通过 /sys/module/module_name/sections 查看 kvm.ko 和 kvm-intel.ko 模块所在的GPA，并在GDB中手动引入KVM模块的调试信息。运行命令如下。\nVirtual Machine Terminal 1\nPhysical Machine Terminal 2\n接着在虚拟机中启动嵌套虚拟机并运行，-monitor 选项指定了QEMU监视器 (QEMU monitor) 的运行端口为 4444。读者可以另外启动一个终端连接至QEMU监视器。QEMU监视器提供了各种命令用于查看虚拟机的当前状态。这里可以通过 info qtree 查看当前虚拟机的架构。可以发现虚拟机使用的中断控制器为APIC。虚拟IOAPIC直接挂载在 main-system-bus 上，而e1000网卡挂载名为 pic.0 的PCI总线上。启动命令如下：\nVirtual Machine Terminal 2\nVirtual Machine Terminal 3\n在嵌套虚拟机中启动一个终端并执行 lspci-v 指令，可以查看当前虚拟机中的PCI设备，e1000网卡具体信息如下。\nNested Virtual Machine Terminal 1\n上述信息表明e1000网卡的BDF为 00:03.0，即24，而e1000网卡使用的中断IRQ号为11，介绍中断传递时提到在QEMU/KVM中GSI与IRQ等价，除了0号中断引脚外，其余IOAPIC引脚与GSI满足 GSI = GSI_base + Pin 映射关系，故e1000网卡对应的中断引脚号为11。然后使用QEMU监视器输入 info pic 查看虚拟机IOAPIC信息。\nVirtual Machine Terminal 3\n以上输出表明虚拟IOAPIC的11号中断引脚对应的中断向量号为38，即e1000网卡使用 38 号中断向量。下面将通过GDB查看e1000网卡中断传送过程中的关键函数调用以及中断信息。\ne1000网卡中断传送流程 网卡在收发包时都会产生中断，而对于e1000网卡，无论是收包中断还是发包中断，最后都会调用 set_interrupt_cause 函数。读者可以通过前述 Virtual Machine Terminal 2 中运行的GDB在 set_interrupt_cause 函数处设置断点并继续运行该程序。当触发该断点后，打印出e1000网卡的设备号为24，与 lspci-v 指令结果相符。\nVirtual Machine Terminal 2\nset_interrupt_cause 函数最终会调用PCI设备共用的中断接口 pci_set_irq 函数触发中断，为了区别e1000网卡与其他PCI设备，我们可以使用GDB条件断点，使得只有设备号为24时才命中 pci_set_irq 中的断点。终端输出表明e1000网卡使用的中断引脚号（intx 变量）为0，即e1000网卡使用 INTA# 中断引脚。\nVirtual Machine Terminal 2\n如前所述，pci_irq_handler 函数最终会调用 pci_change_irq_level 函数获得PCI设备中断引脚所连接的PCI总线中断请求线。pci_change_irq_level 函数通过调用所属总线的 map_irq 回调函数 pci_slot_get_irq 完成上述转换。在该行设置断点并打印出对应的PCI链接设备号（对应 irq_num）为2，故e1000网卡的 INTA# 中断引脚连接至 LNKC 中断请求线。\nVirtual Machine Terminal 2\n而 pci_change_irq_level 函数将会调用总线成员的set_irq回调函数 piix3_set_irq，进而调用 piix3_set_irq_level 函数。该函数通过PIIX3设备PCI配置空间中的 PIRQRC[A:D] 寄存器**获取PCI总线某条中断请求线对应的IOAPIC IRQ线。**在该函数中断点打印e1000网卡对应的IRQ线(pci_irq)，其值为11。\nVirtual Machine Terminal 2\npiix3_set_irq_level 函数获得PCI设备对应的IRQ线后会调用 piix3_set_irq_pic 函数，该函数进而调用 qemu_set_irq 函数经由4.4节介绍的QEMU中断路由过程后，最终调用 kvm_set_irq 函数将中断信号传递至KVM模拟的中断芯片。调用GDB backtrace 命令打印出当前函数调用栈帧，与QEMU中断路由流程相符。如下：\nVirtual Machine Terminal 2\n**中断信号传入KVM后，**经由4.4节介绍的内核中断路由表，将中断信号发送至所有可能的虚拟中断控制器。对于本实验来说，虚拟机使用的中断控制器为IOAPIC，对应的回调函数为 kvm_set_ioapic_irq，该函数将调用 kvm_ioapic_set_irq 函数处理指定中断引脚传来的中断信号。通过GDB在该函数处设置断点，可以发现传入 kvm_ioapic_set_irq 函数的中断引脚号为11，即e1000中断对应的中断引脚号为11。\nPhysical Machine Terminal 1\n如前所述，kvm_ioapic_set_irq 函数最终调用 ioapic_service 函数处理指定引脚的中断请求。ioapic_service 函数根据传入的中断引脚号在 PRT(ioapic-\u0026gt;redirtbl) 中找到对应的RTE并格式化出一条中断消息 irqe 发送给所有的目标LAPIC。\nirqe 中包含供CPU最终使用的中断向量号。在 ioapic_service 函数中设置断点打印中断消息 irqe，可以发现e1000网卡对应的中断向量号为38，触发方式为水平触发 (trig_mode=1)，与通过QEMU监视器执行 info pic 命令得到的信息完全一致。\nPhysical Machine Terminal 1\n后续流程为：\n虚拟LAPIC收到中断消息后，将设置 IRR 并调用 vcpu_kick 函数通知vCPU； 当vCPU再次调用 vcpu_enter_guest 函数准备进入非根模式时，发现当前有待注入的虚拟中断。最终 vcpu_enter_guest 函数会调用 vmx_inject_irq 函数设置VMCS VM-Entry控制域中的中断信息字段； 当虚拟机恢复运行时，CPU会检查该字段发现有待处理的e1000网卡中断，则CPU将执行相应的中断服务例程。 至此，e1000网卡产生的虚拟中断处理流程完成。\n本节通过GDB展示了e1000网卡虚拟中断处理流程，着重展示了PCI设备中断引脚号与IRQ号的映射以及IRQ号与中断向量号的映射关系。\n5 //TODO: GiantVM CPU虚拟化 ","date":"2024-10-17T10:06:27+08:00","permalink":"https://zcxggmu.github.io/p/kvmbook-cpu_2/","title":"Kvmbook Cpu_2"},{"content":"[TOC]\n0 前言 虚拟化技术是一门“古老”的新技术。早在1959年，牛津大学Christopher Strachey就提出了具有虚拟化概念的高效分时复用方案，意在解决当时大型机器使用效率低下的问题。到20世纪60至70年代，虚拟化研究进入了第一个高速发展时期，出现了以IBM CP-67/CMS为代表的大型机虚拟化技术，并提出了硬件架构可虚拟化的理论和准则（如敏感指令应属于特权指令）。但到了20世纪80年代，随着操作系统的成熟，资源管理不再以虚拟化为中心，MIPS和x86等CPU厂商出于成本和商业考虑，设计的硬件架构不再满足可虚拟化准则，形成了所谓的“虚拟化漏洞”（例如，x86有5类共17条敏感指令，但不属于特权指令）。随着个人计算机的普及，研究者提出了一系列弥补x86架构“虚拟化漏洞”的方法，代表性的技术有斯坦福大学Mendel Rosenblum等在SOSP 1997国际学术会议发表的DISCO系统，虚拟化技术重新兴起。随后，InteI和AMD等硬件厂商提出了硬件辅助虚拟化，使得x86硬件平台满足可虚拟化准则。从2006年亚马逊以虚拟机形式向企业提供IaaS（Infrastructure as a Service，基础设施即服务）平台开始，虚拟化技术成为当前支撑云计算、大数据、移动互联网和工业互联网等新型计算和应用模型的关键“根技术”。\n回顾虚拟化的发展历史，可得出一些重要启示。一是基础研究对于计算机系统非常重要，不少关键技术的突破首先来自学术界和工业界的前沿研究；二是最新的技术未必用于产品，成本和市场也是重要考量（ARM和RISC-V初期也不是可虚拟化硬件架构，如ARMv6有4大类24条敏感非特权指令）。因此，深入理解虚拟化技术，把握其内在的发展规律，对于虚拟化的创新发展有重要作用。\n系统虚拟化作为物理硬件层的虚拟化，在计算机硬件和操作系统之间引入一个系统软件抽象层，向下管理硬件资源，向上对操作系统提供虚拟机接口，其目标可概括为“功能不缺失、性能不损失”。因此，系统虚拟化涉及操作系统和硬件接口，技术体系比较复杂，对初学者来说是一个很大的挑战。如果不了解虚拟化技术的基本原理而去直接翻阅源代码，例如开源的QEMU/KVM，很容易抓不住主线，迷失在庞大的代码中（QEMU源代码已超过150万行）。因此，本文将对系统虚拟化的理论体系进行一个整体性概述，在后续将对各部分逐个分析。\n1 系统虚拟化基本概念 系统虚拟化 (System Virtualization) 已成为当前支撑云计算、大数据、移动互联网和工业互联网等新型计算和应用模型的关键技术，应用于从云数据中心到边缘智能终端等不同硬件尺度的“云-网-边-端”全场景中。\n虚拟化 (Virtualization) 泛指将物理资源抽象成虚拟资源，并在功能和性能等方面接近物理资源的技术，即“功能不缺失、性能不损失”。虚拟化技术广泛应用于从硬件到软件的不同计算机系统层次，例如物理内存抽象成虚拟内存，设备抽象成文件，物理显示器抽象成窗口(Window)，Java应用运行于JVM（Java Virtual Machine，Java虚拟机）。一般而言，计算机系统自下而上被划分为多个层次（见下图）。\n系统/用户ISA\n硬件向上对软件提供的指令集抽象ISA（Instruction Set Architecture，指令集架构）通常分为系统ISA (System ISA)和用户ISA (User ISA)。系统ISA提供特权操作（如切换进程页表），一般在操作系统内核态运行；用户ISA提供给普通应用程序使用（如进行加法操作），一般在用户态运行。例如，RISC-V的指令集手册提供第一卷——用户态ISA(User-Level ISA)和第二卷——特权（系统）架构(Privileged Architecture)，分别对应于用户ISA和系统ISA。因此，两者合起来构成完整的硬件编程接口，即 ISA = System_ISA + User_ISA。\n操作系统OS\nOS（Operating System，操作系统）运行于硬件之上，向下管理硬件资源、向上提供系统调用(System Calls)接口。操作系统提供的系统调用（上层应用可以通过系统调用请求操作系统执行特权操作，即执行系统ISA）以及用户ISA共同组成了应用程序的ABI（Application Binary Interface，应用程序二进制接口）。如果两个系统的ABI相同，意味着提供了一个可移植的基础运行环境，即 ABI = System_Calls + User_ISA。\n用户库Lib\n库程序调用 (Library Calls) 提供系统运行时 (Runtime)、系统公共服务 (Services) 和功能丰富的第三方程序（如数学库、图形库等），运行于用户态，以函数库的形式供应用程序调用。同时，应用程序一般通过库函数调用操作系统的系统调用接口。因此，对于应用程序而言，底层的系统ISA和操作系统的系统调用是透明的，它主要关心运行环境（包括运行时、服务和库等）提供的API（Application Program Interface，应用程序编程接口）。也就是说，确定了应用程序所使用的用户ISA和所调用的库函数（包括动态库和静态库），也就确定了应用程序的用户态编程接口，即 API = Library_Calls + User_ISA。\n因此，如下图所示，硬件和软件资源在不同层次的抽象对应不同的虚拟化方法。\n物理硬件层的虚拟化\n虚拟机监控器Hypervisor通过直接对硬件资源进行抽象和模拟，提供了虚拟硬件ISA(Virtual ISA)接口，包括虚拟系统ISA(Virtual System ISA)和虚拟用户ISA(Virtual User ISA)，即Virtual ISA=Virtual System ISA+Virtual User ISA。Hypervisor有时也称为VMM（Virtual Machine Monitor，虚拟机监控器），除非专门列出（例如Rust-VMM是用Rust语言编写的用户态虚拟机监控程序），本书统一采用Hypervisor表述VMM。由于虚拟化技术的多样性，操作系统还可以进一步细分为宿主机操作系统(Host OS)和客户机操作系统(Guest OS)，这将在后文中详细介绍。虚拟硬件ISA提供了完整的硬件资源抽象，从而使多个操作系统能够运行其上并共享硬件资源，例如VMware虚拟化产品VMware Workstation可以在Windows系统上运行一个Linux操作系统。\n操作系统层的虚拟化\n操作系统本身就是虚拟化技术的一种体现，它将CPU、内存和I/O等资源进行了抽象，最终以进程为单位使用这些资源，并进一步由命名空间 (Namespace) 和控制组 (Cgroups) 等容器 (Container) 技术实现对资源的隔离和限制。因此这一层次的虚拟化也称为轻量级虚拟化、进程虚拟化。有些文献把基于二进制翻译的方法运行相同或不同ISA的二进制程序称为进程虚拟化，例如在基于x86的Windows系统上运行ARM架构的程序。本书采用的术语容器虚拟化主要指基于同构指令集的轻量级虚拟化方法，它提供了虚拟ABI (Virtual ABI) 接口，即 Virtual ABI=虚拟系统调用(Virtual System Calls)+Virtual User ISA。近年比较流行的Docker技术便属于容器虚拟化方法。\n应用层程序运行环境的虚拟化\n应用程序本身作为虚拟机（也称为沙箱，Sandbox），提供用户态虚拟运行时 (Virtual Runtime) 支撑应用程序的运行，即虚拟API接口=虚拟库程序调用(Virtual Library Calls)+Virtual User ISA。比较常见的是高级编程语言的虚拟运行环境，例如Java虚拟机和Python虚拟机。注意，这里的用户态ISA有时采用高级编程语言自定义的字节码(Bytecode)抽象，用于屏蔽各个硬件指令集的差异，能够跨平台运行（最终会将字节码编译到某个具体硬件架构的用户态ISA）。\n以上是三种基本的虚拟化抽象方法，根据指令集、操作系统是否同构等不同存在多种变体。例如库函数虚拟化：通过拦截用户态程序的系统调用，随后对调用进行仿真和模拟，能够在同一个硬件指令集上运行不同操作系统的程序。例如，Wine基于Linux等操作系统运行Windows的应用软件（通过提供兼容层将Windows的系统调用转换成与Linux对应的系统调用）。从ABI角度分析Wine，用户ISA同构（均为x86指令集），但系统调用异构（分别为Windows与Linux）。\n近年日益流行的轻量级Unikernel是一种定制的库操作系统(Library OS)。Unikernel和应用程序一起编译打包，构成单地址空间的可执行二进制镜像，不再严格区分用户态和内核态。因此，Unikernel可以直接运行在硬件或Hypervisor上，就硬件接口而言，Unikernel的CPU和内存部分仍为（高度简化的）虚拟硬件ISA，而I/O部分则基本使用virtio或其他半虚拟化方案，降低了传统虚拟化方法由于逐层软件抽象而引入的性能开销，提供了轻量、高效的虚拟化抽象。\n**本文主要介绍系统虚拟化，也就是上述物理硬件层的虚拟化，**特指在计算机硬件和操作系统之间引入一个系统软件抽象层，由其实际控制硬件，并向操作系统提供虚拟硬件ISA接口，从而可以同时运行多个“虚拟机”(Virtual Machine)。目前指令集同构的系统虚拟化是主流方法，因此本文着重介绍同构虚拟化方法。此外，容器虚拟化与操作系统和硬件密切相关，也是目前大规模使用的云计算的核心技术，本文将在后续简要介绍容器虚拟化方面的内容。应用层程序运行环境的虚拟化涉及与硬件平台无关的字节码和沙箱等技术，限于篇幅，不在本文讨论范围之内。\n综上所述，系统虚拟化将物理机器抽象成虚拟机器，其抽象粒度包括CPU、内存和I/O在内的完整机器。从本质上来说，系统虚拟化抽象了硬件指令集架构，向上对操作系统提供了硬件特性的等效抽象。对操作系统而言，运行于虚拟机上与运行于物理机上没有区别，即虚拟机可视为真实物理机的高效并且隔离的复制品 (A virtual machine is taken to be an efficient,isolated duplicate of the real machine)。因此，系统虚拟化的目标是使虚拟机的功能和性能等与物理机接近，即“功能不缺失、性能不损失”。\n此外，根据抽象类型的不同，系统虚拟化可以分为“一虚多”（见下图(a)）和“多虚一”（见下图(b)）。早期的系统虚拟化主要研究“一虚多”，即把单物理机抽象成若干虚拟机（如一个物理机可以抽象成10个虚拟机）。之后，跨越数量级的硬件性能提升为“多虚一”架构开辟了道路，即把多物理机抽象成单一虚拟机。\n虚拟化技术通过对物理硬件在数量、功能和效果上进行逻辑化虚拟，能够提供高层次的硬件抽象、硬件仿真、服务器整合(Server Consolidation)、资源按需调配、资源聚合、柔性管理、在线迁移、系统高可用、安全隔离等功能，因此虚拟机也成为当前各类硬件平台上的主要载体。虚拟化能大幅提升资源利用率，因为信息系统是按照最大使用峰值设计的，利用率一般不到20%。采用虚拟化技术可以整合资源、削峰填谷和节能降耗，例如电信业信息系统虚拟化后，利用率能够提升到70%以上。根据VMware的报告，VMware vSphere可实现10∶1（10台虚拟机运行于1台物理机之上）或更高的整合率，将硬件利用率从5%~15%提高到80%以上。另外一个典型案例是，2007年《纽约时报》想把从1851年创刊开始到1922年的文章上传到网络，以供免费阅读和搜索，原预计费时数月及花费上百万美元，但租用亚马逊虚拟机，在24小时内将1851—1922年1100万篇文章的报纸扫描件整理成PDF，仅花费240美元。\n综上所述，**系统虚拟化是把物理硬件资源通过 “一虚多/多虚一” 抽象成虚拟资源，并使得性能尽可能接近物理资源。**同时，与物理资源类似，虚拟资源的四大特性（可靠性、可用性、可维护性和安全性）也是系统虚拟化的重要技术指标，在后文中将会逐步对此展开详细介绍。自2009年起，全球新增的虚拟机数量已超过新增的物理机。因此，系统虚拟化技术成为当前支撑云计算、大数据、互联网和机器学习等新型计算和应用模型的核心技术之一。\n2 系统虚拟化的发展历史和趋势展望 系统虚拟化与计算机硬件的发展息息相关，并且随着计算机软硬件技术的发展而不断完善。至今，虚拟化技术已经成为系统软件的核心技术。为了更好理解虚拟化的基本概念和技术演变，本节将简要叙述虚拟化技术的发展历史，并展望未来的发展趋势。\n2.1 发展历史 早期虚拟化探索 虚拟化技术的发展历史与计算机技术一脉相承。早在1959年，牛津大学的计算机教授Christopher Strachey在信息处理国际大会(International Conference on Information Processing)发表了论文Time sharing in large fast computers，针对当时大型机器使用效率低下的问题（资源利用率低到现在也没有得到很好的解决，如何提升资源利用率贯穿整个计算机的发展历史），提出了具有虚拟化概念的高效分时复用方案，原文为：\n“各种程序和外设竞争获得控制权（分时共享），并竞争获得存储的使用权（空间共享）”（There would be various programs and pieces of peripheral equipment competing both for the use of control(time sharing)and also for the use of the store(space sharing)），这被认为是虚拟化技术的最早表述。\n论文中提出的Director监控程序被放置在一个可快速访问并且不可删除的存储介质上（保证了自身运行的高效性和完整性）。Director提供了一个隔离的运行环境，控制各个程序的分时运行，并确保各个程序无法相互干扰。这里的Director可以看作 “一虚多” Hypervisor的雏形。同时，论文提出了虚拟机的概念（包括CPU和存储的完整状态），原文为：\n“多个操作（程序）同时在物理机上运行，似乎跑在单独的机器上（虚拟机），但每个虚拟机比实际的物理机小一些、慢一些”（In this way，during the normal running of the machine several operators are using the machine during the same time. To each of these operators the machine appears to behave as a separate machine(smaller,of course,and slower than the real machine)）。\n由此可见，虚拟化技术提供的“弹性”和“隔离”概念已经孕育于早期的系统设计中。\n因此早期的资源虚拟化主要研究 “一虚多”，即把单物理资源抽象成若干虚拟资源（如1个物理CPU可抽象成512个虚拟CPU）。在计算机技术发展的早期阶段，尚在探索对于计算资源的时分复用，以便多人能通过终端同时使用一台计算机，分享昂贵的计算资源，提高计算机的资源利用率。\n1965年IBM M44/M44X实验项目实现了在同一台主机M44上模拟多个7044系统，突破了部分硬件共享(Partial Hardware Sharing)、分时共享(Time Sharing)和内存分页(Memory Paging)等关键技术，并首次使用术语“虚拟机”。这里模拟的虚拟机M44X并非完全与主机相同，与现在Xen采用的半虚拟化技术异曲同工。在此基础上，IBM研发了一系列虚拟化技术，如CP-40/CMS、CP-67/CMS和CP370/CMS等，构成了完整的CP/CMS大型机虚拟化系统软件，一直延续到后来的IBM虚拟化软件z/VM。\n20世纪60年代中期，IBM的CSC（Cambridge Scientific Center，剑桥科学中心）开发了CP-40/CMS，CP-40是虚拟机控制程序(Virtual Machine Control Program)，其开发目标主要有4个：\n研究资源的分时共享； 评估硬件对分时共享的需求； 开发内部使用的分时共享系统； 评估操作系统与硬件的交互行为。 CP-40提供的虚拟机数量最多为14个，是第一个支持全硬件虚拟化 (Full Hardware Virtualization) 的系统。CP-40的后继系统CP-67于1967年开发，其大小为80KB，可以运行于IBM的System/360-67大型机，提供多个虚拟的System/360机器实例，并可以通过CPREMOTE服务访问远程硬件资源，支持OS/360、DOS和RAX等客户机操作系统。此外，CP-67还有配套开发的专用客户机操作系统CMS（Cambridge Monitor System，剑桥监控系统），是一个不支持并发的单用户系统，以减小开销，其思想可以与今日的Unikernel类似。\nCP-67/CMS这套系统组合甚至比大家熟知的Multics和UNIX系统出现更早，但以虚拟内存和抢占式调度为首要特点的多用户操作系统很快占据了学术界和工业界的主流，而在很长一段时间内，虚拟化技术都没有引起人们足够的重视。因此早期虚拟化技术主要用于大型机，以至于后来设计x86指令集时就没有考虑虚拟化的需求。面向嵌入式和PC（Personal Computer，个人计算机）市场，虚拟化的性能开销也是其重要限制因素。\n虚拟化技术的普及 20世纪60至80年代，虚拟化技术主要用于大型主机。随着x86平台日益流行，特别是微软与英特尔的Wintel联盟主导全球PC市场，基于Linux的x86服务器也逐步侵蚀大型主机和小型机的市场份额，由此产生了对x86平台虚拟化的迫切需求，也推动了虚拟化技术的广泛普及。\n20世纪80年代中期，Rod MacGregor创建的Insignia Solutions公司开发了x86平台的软件模拟器SoftPC，能在UNIX工作站上运行MS-DOS操作系统，1987年SoftPC被移植到苹果的Macintosh平台，并可以运行Windows操作系统。此后，一系列以软件模拟为主的PC虚拟化系统陆续发布，例如苹果开发的Virtual PC和开源的Bochs。\n1997年，斯坦福大学Mendel Rosenblum教授等在计算机系统领域的重要国际会议SOSP（Symposium on Operating Systems Principles，操作系统原理研讨会）上发表了DISCO。DISCO运行在具有共享内存的可扩展多处理器系统上，可以同时运行多个客户机操作系统。不同于SoftPC这样的软件模拟器，DISCO基于全虚拟化(Full Virtualization)技术，大部分用户态指令是直接运行在物理CPU上，使得虚拟化的开销大为降低。基于该原型系统，1998年Mendel Rosenblum教授等成立了著名的VMware公司，该公司也是目前x86虚拟化软件的主流厂商之一。1999年，VMware推出了广受欢迎的桌面虚拟化产品VMware工作站 (VMware Workstation)，也间接推动了x86虚拟化技术的普及。\n2003年，剑桥大学Ian Pratt等学者在SOSP上发表了基于PV（Para-Virtualization，半虚拟化）技术的代表性开源系统Xen。与全虚拟化技术不同，半虚拟化技术需要修改客户机操作系统。通过客户机操作系统主动调用超调用(HyperCall) 的虚拟化管理接口，能够进一步降低虚拟化的开销，并且由于Xen是开源系统，与Linux内核密切配合（半虚拟化技术需要修改操作系统，开源内核修改比较方便），Xen得到了广泛的应用，例如早期的亚马逊云计算平台EC2（Elastic Compute Cloud，弹性计算云）就是基于Xen构建的（EC2第一个采用半虚拟化的实例类型是m1.small）。\n同年，法国天才程序员Fabrice Bellard发布了至今仍然使用的开源虚拟化软件QEMU（Quick EMUlator，快速仿真器），采用动态二进制翻译(Binary Translation)技术，能够支持多源多目标的跨平台仿真执行异构指令集，例如可以在x86平台上仿真运行ARM进程。QEMU是运行在用户态的仿真器，它不仅可以仿真异构指令集，还可以仿真整个机器，通过开源社区的不断改进，QEMU已成为目前使用最为广泛的开源软件仿真器。\n自2005年，x86硬件厂商开始从体系结构层面解决早期x86架构不符合虚拟化准则的问题（也称为“虚拟化漏洞”），避免纯软件全虚拟化方式（如二进制翻译）带来的性能、安全和可靠性的缺陷。Intel和AMD相继发布了基于硬件辅助的全虚拟化技术（Hardware-assisted Full Virtualization）。Intel的硬件辅助VT（Virtualization Technology，虚拟化技术）和AMD的类似技术SVM（Secure Virtual Machine，安全虚拟机）扩展了传统的x86处理器架构，客户机操作系统不用修改源码就可以直接运行在支持虚拟化技术的x86平台上，原来由软件模拟的大量特权操作直接由硬件执行，基本达到了物理CPU和内存的性能，虚拟化开销大为减少。\n2006年，AWS（Amazon Web Services，亚马逊云服务）发布了S3（Simple Storage Service，简单存储服务）和EC2，开始以虚拟机形式向企业提供IT（Information Technology，信息技术）基础设施服务（基于开源Xen搭建），开启了以IaaS（Infrastructure as a Service，基础设施即服务）为代表性技术的云计算时代。\n2007年2月，Linux Kernel 2.6.20中加入了以色列公司Qumranet开发的基于硬件辅助虚拟化的内核模块KVM（Kernel-based Virtual Machine，基于内核的虚拟机）。KVM由虚拟化领域另外一个天才程序员Avi Kivity带领开发，并于2006年10月19日首次在Linux内核社区发布，发布不到半年时间，Linux就将其正式纳入内核官方版本，由此也可以看出KVM对Linux开源社区的重要性和迫切性。Avi Kivity等提出的KVM充分遵循UNIX系统的设计原则，仅实现Linux内核态的虚拟化模块，用户态部分由上述的开源QEMU实现。由此，QEMU/KVM的虚拟化技术组合日益流行，成为目前主流的开源系统虚拟化方案，被各大虚拟化和云计算厂商采用。\n基于KVM/QEMU的系列年度技术论坛（例如KVM Forum）也成为虚拟化技术的重要权威论坛，从2007年开始，KVM/QEMU的重要技术进展和年度汇报均发布于KVM Forum，是Linux虚拟化技术发展的一个窗口。近年来，国内华为、阿里和腾讯等虚拟化技术方面的专家也积极参加KVM Forum，凸显国内对虚拟化技术的贡献日益增加。\n虚拟化技术蓬勃发展 从2010年开始，虚拟化技术不断扩宽应用场景，在移动终端、嵌入式和车载设备等资源受限的平台也开始被引入。同时，新型硬件虚拟化、“多虚一”虚拟化和轻量级容器虚拟化也取得了长足的进展，下面分别展开介绍。\n新型硬件虚拟化\n近年来大量新型硬件得到迅速普及，例如拥有数千个核的GPU（Graphics Processing Unit，图形处理单元）、具有RDMA（Remote Direct Memory Access，远程内存直接访问）功能的高速网络、支持硬件事务内存和FPGA（Field Programmable Gate Array，现场可编程门阵列）加速的CPU处理器等。以计算能力为例，CPU/GPU/FPGA不断延续摩尔定律。自2005年Intel首次提供了针对CPU的硬件辅助VT-x（Virtualization Technology for x86，x86虚拟化技术）后，硬件辅助虚拟化成为主流的“一虚多”虚拟化方法。目前，基于硬件辅助的虚拟化方法在CPU、内存和网络等传统的硬件资源上获得了成功，特别是CPU和内存虚拟化性能已经接近物理性能。新型异构设备（如FPGA、GPU）逐步成为大数据和人工智能等专用计算系统的核心要素，是算力输出的重要甚至主要部分。但这些新型设备要么没有虚拟化，要么处于虚拟化的早期水平，导致云无法享受“新硬件红利”。直到2014年，各大厂商才提出了以Intel gVirt、GPUvm为代表的硬件辅助虚拟化方案，但该方案远未成熟。\n下面以GPU为典型代表介绍新型硬件虚拟化的发展历史。\n现代GPU的功能已经从原来的图形图像计算扩展到了视频编解码、高性能计算，甚至是GPGPU（General-Purpose Graphics Processing Unit，通用图形处理单元）。但GPU这类重要资源虚拟化的高性能、可扩展性和可用性相对于CPU仍处于滞后的阶段，例如2014年Intel的GPU虚拟化解决方案gVirt中，单个物理GPU仅支持3个vGPU（Virtual GPU，虚拟GPU），而同年发布的Xen 4.4已支持512个vCPU（Virtual CPU，虚拟CPU）。直到2016年，亚马逊等各大云服务提供商才陆续推出了商业化的GPU实例。\n**传统GPU虚拟化通过API转发 (API Forwarding) 的方式将GPU操作由虚拟机发送到Hypervisor代理执行，**该方法被大量主流虚拟化产品所采用来支持图形处理，但并非真正意义上的完整硬件虚拟化技术，其性能和可扩展性均无法满足GPGPU等应用（如机器学习）的需要。\nGPU虚拟化的软件模拟采用类似于CPU虚拟化中二进制转换方法。但相对于CPU，GPU的特性复杂，不同的设备提供商之间的GPU规格区别很大，GPU的资源很难被拆分，模拟的效率低。因此，典型的QEMU软件仅模拟了VGA设备的基本功能，它通过一个半虚拟化的图像缓冲区加速特定的2D图像访问，不符合高效、共享的虚拟化要求。\nGPU虚拟化的设备直通 (Passthrough) 方法将物理GPU指定给虚拟机独占访问。设备直通有时也称为设备透传技术，直接将物理设备，通常是PCI（Peripheral Component Interconnect，外设组件互连）设备，配置给虚拟机独占使用（其他虚拟机无法访问该物理设备）。与API转发提供的良好GPU共享能力相比，设备直通方法通过独占使用提供了优异的性能。例如，基于Intel的VT-d/GVT-d技术，通过翻译DMA（Direct Memory Access，直接内存访问）所访问内存地址的方法将GPU分配给一个虚拟机使用，能够达到与原生物理GPU相近的性能，但牺牲了共享特性。NVIDIA的Tesla GPU也提供了类似的虚拟化方案Grid，虚拟机可以通过硬件直通的方式直接访问物理GPU。\n在GPU虚拟化方面，由于架构复杂，既要支持常规显卡，又要支持GPGPU，直到2014年才发表了硬件支持的GPU全虚拟化方案（Intel早在2005年已增加对CPU虚拟化的硬件支持），即Intel提出的产品级开源方案gVirt和学术界提出的方案GPUvm（均发表于USENIX ATC 2014）。gVirt是第一个针对Intel平台的GPU全虚拟化开源方案，为每个虚拟机都提供了一个虚拟的GPU，并且不需要更改虚拟机的原生驱动。此后，在高性能、可扩展和可用性三个重要方面提出了一系列改进（如gHyvi、gScale和gMig等），为GPU虚拟化的广泛应用打下了良好基础。\n“多虚一” 虚拟化\n单台物理主机已经能够拥有超过数百个CPU核、数千个GPU核、TB级内存以及超过100Gbps的网络带宽的硬件环境，由此产生了在单机上构建巨大规模/巨型虚拟机的迫切需求。资源扩展主要有资源横向扩展(Scale out)和资源纵向扩展(Scale up)两种方式。虚拟化资源横向扩展的好处是弹性分配、资源灵活使用且利用率高，但编程模型复杂；纵向扩展的好处是编程模型简单，避免了由于分布式系统和数据分区产生的软件复杂性，但硬件昂贵、灵活性差。针对内存计算等大规模计算需求，是否可以综合利用资源横向和纵向扩展的优势？通过虚拟化技术，可以在虚拟化层面聚合资源，使底层资源对上层客户机操作系统透明。\n不同于传统的“一虚多”方法，**这种 “多虚一” 的跨物理节点虚拟化架构可以将计算资源、存储资源和I/O资源虚拟化，构建跨节点的虚拟化资源池，向上对客户机操作系统提供统一的硬件资源视图，并且无须修改客户机操作系统。**目前典型的跨节点虚拟化产品有以色列公司的ScaleMP和美国公司的TidalScale。其中TidalScale提出了一种软件定义服务器，通过超内核构建虚拟资源池，将主板上所有的DRAM（Dynamic Random Access Memory，动态随机存取存储器）内存抽象为虚拟化的L4缓存，并且引入了虚拟主板提供跨节点的虚拟设备连接，通过虚拟通用处理器、虚拟内存和虚拟网络构建虚拟资源池，并且资源可以迁移，用户可以灵活使用资源。通过复杂的缓存一致性算法和缓存管理算法，有效提升了性能。在一个包含1亿行、100列的数据库和128GB内存的服务器上，由于所需内存容量大于单个服务器硬件的内存容量，导致分页 (Paging) 频繁发生，以至于一个应用程序需要花费7小时才能完成MySQL的三次查询作业。这个现象称为 “内存悬崖(Memory Cliff)”，即由于应用所需内存超过服务器内存导致性能急剧下降。而该查询运行在采用两个96GB内存节点组成TidalScale服务器上只需要7分钟，性能提升60倍。\n但ScaleMP和TidalScale都基于特定硬件和定制化闭源Type Ⅰ虚拟化平台。Type Ⅰ虚拟化在裸机上运行Hypervisor，然后加载客户机操作系统，技术生态和应用范围受限制，无法和主流的Type Ⅱ开源虚拟化系统KVM/QEMU兼容。不同于Type Ⅰ虚拟化，Type Ⅱ虚拟化是在宿主机操作系统上运行Hypervisor，然后加载客户机操作系统，是目前主流的虚拟化方法，如KVM/QEMU被全球最大的亚马逊云服务和国内最大的阿里云等采用。\n基于KVM/QEMU，GiantVM架构围绕高速网络RDMA技术实现虚拟化层面的硬件聚合和抽象，是目前首个开源的Type Ⅱ“多虚一”架构。GiantVM以Libvirt为客户虚拟机上层接口，分布式QEMU提供跨节点虚拟机抽象，KVM为下层物理机提供管理接口，基于RDMA提供低延迟DSM（Distributed Shared Memory，分布式共享内存）。目前GiantVM可以将八台服务器虚拟成一台虚拟机，支持的客户机操作系统有Linux和瑞士的苏黎世联邦理工学院(ETH)Timothy Roscoe教授等提出的多内核操作系统Barrelfish。关于GiantVM的相关实现技术，将在后文进行介绍。\n由于跨节点通信一般为亚微秒级，与常规内存访问有数量级差距（纳秒级），**虚拟化聚合“多虚一”的技术挑战是由于分布式共享内存同步需要跨节点通信，协议同步开销是性能损失的主要来源。**传统方法在DSM同步中引入了普林斯顿大学李凯教授等提出的基于顺序一致性(Sequential Consistency)的IVY协议，该协议的编程模型简单，但严格同步性能开销大。如何进一步降低DSM引入的“多虚一”性能开销是目前“多虚一”技术面临的重要技术挑战。\n轻量级容器虚拟化\n容器虚拟化技术最早可以追溯到1979年UNIX V7引入的chroot(change root)系统调用，通过将一个进程及其子进程的根目录（root目录）改变到原文件系统中的不同位置（虚拟根目录），使得这些进程只能访问该虚拟根目录下的文件和子目录。因此chroot为每个进程提供了相互隔离的虚拟文件系统（称为chroot Jail），被认为是轻量级进程隔离方法的雏形，标志着容器虚拟化的开始。\n2000年发布的FreeBSD Jails比chroot提供了更完善的进程级容器（称为Jail）运行环境。每个Jail容器拥有各自的文件系统（基于chroot，并修正了传统chroot的安全漏洞）和独立的IP（Internet Protocol，网际协议）地址，对进程间通信也加以限制。因此，Jail容器中进程无法访问Jail之外的文件、进程和网络。此后，还发布了与FreeBSD Jails类似的进程隔离技术，如2001年的Linux VServer、2004年的Solaris Containers和2005年的Open VZ。\n2006年谷歌公司发布了进程容器 (Process Containers)，提供了一系列进程级资源（包括CPU、内存和I/O资源）隔离技术，后来改名为控制组(Cgroups)，并被合入Linux内核2.6.24。Cgroups技术沿用至今，也是目前的核心容器技术之一。\n2008年Linux社区整合Cgroups和命名空间(Namespace)，提出了完整的容器隔离方案LXC（Linux Containers，Linux容器）。LXC通过Cgroups隔离各类进程且同时控制进程的资源占用，并通过Namespace使每个进程组有独立的进程标识PID、用户标识、进程间通信IPC和网络空间，两者构成了完整的进程级隔离环境。容器之所以被称为轻量级虚拟化技术，是因为LXC基于同一个宿主机操作系统，仅在操作系统层次通过软件隔离出类似虚拟机的效果（“欺骗”进程，使其认为自身运行在不同机器上），不需要虚拟整个ISA（“欺骗”操作系统，使其认为自身运行在物理机上）。因此容器虚拟化的缺点是只支持相同宿主机操作系统上的隔离，而系统虚拟化提供异构客户机操作系统的隔离，两者提供了不同层次的隔离，互为补充。\n2013年dotCloud公司（后更名为Docker）发布了基于LXC的开源容器引擎Docker，引入了分层镜像(Image)的概念，基于只读的文件系统提供容器运行时所需的程序、库和环境配置等，容器则作为一个轻量级应用沙箱，提供应用所需的Linux动态运行环境（包括进程空间、用户空间和网络空间等）。Docker引擎基于容器运行和管理各个用户态应用实例。Docker通过组合只读的静态镜像和可写的动态容器，部署方便且动态隔离，提供一整套容器的创建、注册、部署和管理的完整工具集。因此Docker问世后便迅速普及，成为容器技术的代名词。\n2014年6月谷歌公司开源了基于容器的集群管理平台Kubernetes（简称K8S，名字来源于希腊语“舵手”）。Kubernetes是基于谷歌内部使用的大规模集群管理系统Borg实现的，其核心功能是自动化管理容器，解决大规模容器的编排、管理和调度问题。Kubernetes现已成为容器编排领域的事实标准，得到了广泛的应用。\n此外，随着云原生(Cloud Native)和无服务器(Serverless)架构的日益成熟，更细粒度的FaaS（Function as a Service，函数即服务）将迎来更大的发展。虚拟机、容器和云函数作为资源抽象的不同层次，也会互为补充、相得益彰。\n2.2 趋势展望 自2005年Intel首次提供了针对CPU的硬件辅助虚拟化技术VT-x后，硬件辅助虚拟化已经成为主流的虚拟化方法，在CPU、内存和网络等资源配置上获得了成功。自2014年人们开始研究新型异构设备，新型硬件设备虚拟化抽象效率进一步得到提升。目前新型的虚拟化架构（如亚马逊的Nitro架构、阿里云的神龙服务器和华为的ZERO架构）都基于单节点弹性资源（“一虚多”）横向扩展架构，然而随着新型计算、存储和网络硬件的不断出现（例如，Optical Fabric网络带宽为400Gbps且延迟仅为100ns，与内存总线的带宽差距缩小到一个数量级以内），虚拟化架构也在不断演进。Intel首席架构师Raja Koduri提出：“晶体管尺寸每缩小1/10，就会衍生出一种全新的计算模式”。跨**越数量级的硬件性能提升使得当初设计虚拟化系统的两大基本假设发生变化，孕育出虚拟化新架构，**具体体现如下。\n基本假设一\n单节点资源以CPU为中心。新型异构硬件作为CPU的附属设备，由CPU集中化控制，造成频繁上下文切换，不仅增加了虚拟化开销（例如GPU与CPU的虚拟化页表同步占据了24.43%~79.45%不等的开销），同时设备与CPU紧耦合也限制了新型异构硬件的可扩展性。当前硬件平台趋向于异构化和去CPU中心化，GPU、FPGA等新型硬件成为算力输出的重要甚至主要部分，例如单个Nvidia V100 GPU的性能可能是CPU性能的100倍。单台服务器可能同时配置CPU与GPU/FPGA处理器、具有RDMA特性的InfiniBand网卡、普通内存和NVM内存以及SSD等外存，同时，Gen-Z、CCIX等新的缓存一致互联协议可以让新型去中心化分散硬件资源 Disaggregated Hardware Resource（例如GPU、FPGA以及其他异构加速硬件）相互通信，并独立发起资源访问请求与CPU解耦。因此，各个分散硬件资源通过高速总线和网络相互通信，并具备一定的自主性，能够独立访问其他硬件资源，导致了分散硬件资源的去CPU中心化。\n基本假设二\n跨节点资源以横向扩展为主。早期由于硬件性能的限制，倾向于将各个节点虚拟化后在客户机操作系统上构建分布式系统，将计算和数据进行分区，虚拟化架构以多节点横向扩展为主。近年来，随着硬件性能跨数量级提升，出现了纵向扩展的硬件资源聚合趋势，例如内存聚合有Infiniswap［NSDI 2017（学术会议论文）］、Remote Regions[ATC 2018]、Leap[ATC 2020]、Fastswap[EuroSys 2020]、Semeru[OSDI 2020]和AIFM[OSDI 2020]等，I/O资源聚合有ReFlex[ASPLOS 2017]、PolarFS[PVLDB 2018]等。因此，**利用新型硬件实现多硬件多特性的虚拟化聚合与抽象，提升硬件性能甚至突破单一硬件的物理极限，**也是目前学术界和工业界的重要探索方向。\n这两个基本假设的改变对虚拟化和系统软件的发展产生了深远影响。目前云业务对算力的需求越来越高，单个服务器往往已经无法满足算力需求，形成了机架规模 (Rack-Scale) 的分散式可组合基础设施。目前内存遭遇12nm的工艺瓶颈，工艺设计成本约占整体的2/3，新型持久性内存（如Intel傲腾）填补了内存纵向扩展的空白，高速网络互连使得远程内存访问不再是障碍，以内存为中心的系统虚拟化新架构正在兴起（如MemVerge分级大内存可以将深度学习性能提升20倍）。\n2017年惠普公司基于Gen-Z总线，推出了The Machine原型系统，以内存语义访问远程NVM存储池，构建了160TB的共享内存池，并支持多种异构加速硬件共享该大内存资源。IBM公司在2020年8月发布的IBM POWER 10处理器借助Memory Inception突破性新技术，允许集群中任何基于IBM POWER10处理器的服务器跨节点访问和共享彼此的内存，从而创建PB级聚合内存资源，最多支持2PB内存，并且节点之间的延迟为纳秒级。\nOSDI 2018最佳论文中提出了LegoOS分布式操作系统，将单一内核分解成独立的去中心化CPU、内存和I/O组件，并通过高速网络连接进行消息通信，在操作系统层面进行了前沿性探索。加州大学洛杉矶分校(UCLA)、麻省理工学院(MIT)等大学的学者在OSDI 2020论文中提出了基于分散内存(Disaggregated Memory)的Java运行时管理方法Semeru，能够通过统一的虚拟地址空间跨节点透明访问多个服务器的内存。而在虚拟化软件层面，高性能计算领域的先驱戈登·贝尔(Gordon Bell)等提出了硬件资源虚拟化聚合的方法，试图同时实现横向扩展的硬件成本优势和灵活性，以及纵向扩展固有的简单性。\n因此，通过虚拟化构建弹性资源池（计算、存储和I/O相结合），实现纵向/横向灵活扩展，资源按需聚合/分散，面向领域垂直整合巨型虚拟机/微型虚拟机，能够便捷共享集约化硬件资源，高效抽象具有多样性的硬件设备，通过虚拟化提供更为轻量化、细粒度的资源和接口抽象，现有的硬件、操作系统、应用程序生态可以继续演化，实现软件和硬件的松耦合及协同优化是虚拟化技术不断发展的方向。\n3 系统虚拟化的主要功能和分类 系统虚拟化向下管理硬件资源，向上提供硬件抽象。本节主要介绍系统虚拟化的基本功能（包括CPU、内存和I/O虚拟化），并根据Hypervisor与物理资源和操作系统交互方式的不同，介绍了两种基本的虚拟化分类。然后简要介绍三种虚拟化的实现方式，从而让大家在整体上了解虚拟化不同实现方式对功能和性能的影响。\n3.1 虚拟化基本功能 系统虚拟化架构如下图所示（以经典的“一虚多”架构为例）：\n底层是物理硬件资源，包括三大主要硬件：CPU、内存和I/O设备。Hypervisor运行在硬件资源层上，并为虚拟机提供虚拟的硬件资源。客户机操作系统 (Guest OS) 运行在虚拟硬件资源上，这与传统的操作系统功能一致，管理硬件资源并为上层应用提供统一的软件接口。总览整个架构，Hypervisor应当具有两种功能：①管理虚拟环境；②管理物理资源。\n虚拟环境的管理\nHypervisor需要为虚拟机提供虚拟的硬件资源，因此至少应当包括三个模块：①CPU虚拟化模块；②内存虚拟化模块；③I/O设备虚拟化模块； Hypervisor能够允许多个虚拟机同时执行，那么应当具有一套完备的调度机制来调度各虚拟机执行。考虑到一个虚拟机可能具有多个vCPU，每个vCPU独立运行，那么Hypervisor调度的基本单位应当是vCPU而非虚拟机； 在云环境下，用户有时需要查询虚拟机的状态或者暂停虚拟机的执行，这也通过Hypervisor实现。为了便于用户管理自己的虚拟机，云厂商往往会提供一套完整的管理接口（例如Libvirt）支持虚拟机的创建、删除、暂停和迁移等。 物理资源的管理\n除了管理虚拟环境，在某种程度上，Hypervisor还担负着操作系统的职责，即管理底层物理资源、提供安全隔离机制，以及保证某个虚拟机中的恶意代码不会破坏整个系统的稳定性。 此外，一些Hypervisor还提供了调试手段和性能采集分析工具（例如KVM单元测试工具 KVM-Unit-Tests），便于开发者进行测试与评估，例如可以利用Linux提供的 Perf 工具查看由各种原因触发的VM-Exit（虚拟机退出）次数。\nCPU、内存和I/O设备是现代计算机所必需的三大功能部件。如果系统虚拟化要构建出可运行的虚拟机，CPU虚拟化、内存虚拟化和I/O虚拟化是必要的，它们各自需要实现的功能与解决的问题简要概括如下：\nCPU虚拟化\n在物理环境下，操作系统具有最高权限级别，可以直接访问寄存器、内存和I/O设备等关键的物理资源。但是，在虚拟环境下，物理资源由Hypervisor接管，并且Hypervisor处于最高特权级。这也就意味着客户机操作系统处于非最高特权级，无法直接访问物理资源。因此虚拟机对物理资源的访问应当触发异常，陷入Hypervisor中进行监控和模拟。\n这些访问物理资源的指令称为敏感指令，上述处理方式称为**陷入-模拟 (Trap and Emulate)。**在Intel VT-x等硬件辅助虚拟化技术出现之前，软件虚拟化技术只能利用计算机体系中现有的特权级进行处理。这就要求所有的敏感指令都是特权指令，才能让Hypervisor截获所有的敏感指令。但系统中通常存在一些敏感非特权指令，它们称为 “虚拟化漏洞”，CPU虚拟化的关键就是消除虚拟化漏洞。\n此外，中断和异常的模拟及注入也是CPU虚拟化应当考虑的问题。虚拟设备产生的中断无法像物理中断一样直接传送到物理CPU上，而需要在虚拟机陷入Hypervisor中时，将虚拟中断注入虚拟机中。虚拟机在恢复执行后发现自己有未处理的中断，从而陷入相应的中断处理程序中。\n内存虚拟化\n操作系统对于物理内存的使用基于两个假设：①内存都是从物理地址0开始的；②物理内存都是连续的。\n对于一台物理机上的多个虚拟机而言，它们共享物理内存资源，因此无法满足假设①。对于假设②，可以采用将物理内存分区的方式保证每个虚拟机看到的内存是连续的，但这样牺牲了内存使用的灵活性。因此内存虚拟化引入了GPA（Guest Physical Address，客户机物理地址）供虚拟机使用。但是，当虚拟机需要访问内存时，是无法通过GPA找到对应的数据的，需要将GPA转换为HPA（Host Physical Address，宿主机物理地址）。\n因此Hypervisor需要提供两个功能：①维护GPA到HPA的映射关系，即GPA→HPA；②截获虚拟机对GPA的访问，并根据上述映射关系将GPA转换为HPA。\nI/O虚拟化\n在物理环境下，操作系统通过I/O端口访问特定的I/O设备，称为PIO（Port I/O，端口I/O），或者将I/O设备上的寄存器映射到预留的内存地址空间进行读写，称为MMIO（Memory-Mapped I/O，内存映射I/O）。因此Hypervisor需要截获所有的PIO和MMIO操作并对其进行模拟，再将结果告知虚拟机。设备中断也类似，Hypervisor需要将中断分发到不同的虚拟机中。\n此外，当多个虚拟机运行在同一个物理机上时，由于I/O设备只有一份（如网卡）不可能同时供多个虚拟机使用。Hypervisor需要为每个虚拟机提供一个软件模拟的网卡（也可以采用硬件直通或者硬件辅助虚拟化的方式），这个网卡应当与现实设备具有完全相同的接口，从而允许用户无须修改客户机操作系统中原有的驱动程序就能使用这个虚拟设备。但软件模拟的方式性能较差，目前一般采用软硬件协同优化的I/O虚拟化提升性能。\n3.2 虚拟化分类 1974年，Gerald J. Popek和Robert P. Goldberg在虚拟化方面的著名论文Formal Requirements for Virtualizable Third Generation Architectures中提出了一组称为虚拟化准则的充分条件，也称为波佩克与戈德堡虚拟化需求(Popek and Goldberg Virtualization Requirements)，即虚拟化系统必须满足以下三个条件：\n**资源控制(Resource Control)。**虚拟机对于物理资源的访问都应该在Hypervisor的监控下进行，虚拟机不能越过Hypervisor直接访问物理机资源，否则某些恶意虚拟机可能会侵占物理机资源导致系统崩溃。 **等价(Equivalence)。**在控制程序管理下运行的程序（包括操作系统），除时序和资源可用性之外的行为应该与没有控制程序时完全一致，且预先编写的特权指令可以自由地执行，即物理机与虚拟机的运行环境在本质上应该是相同的。对于上层应用来说，在虚拟机和物理机中运行应该是没有差别的。 **高效(Efficiency)。**绝大多数的客户虚拟机指令应该由主机硬件直接执行，而无须控制程序参与。 该论文为设计可虚拟化计算机架构给出了指导原则，遗憾的是，早期Intel x86架构并不全部符合这三个原则，也就是说，早期x86架构对虚拟化的支持是缺失的，因为其架构包含敏感非特权指令。Robert P. Goldberg还在其博士论文Architectural Principles for Virtual Computer Systems中介绍了**两种Hypervisor类型，分别是Type Ⅰ和Type Ⅱ虚拟化，**也就是沿用至今的虚拟化分类方法，如下图所示：\n根据Hypervisor与物理资源和操作系统交互方式的不同，可以将Hypervisor分为两类：\nType Ⅰ Hypervisor直接运行在物理硬件资源上，需要承担系统初始化、物理资源管理等操作系统职能。从某种程度上来说，**Type Ⅰ Hypervisor可以视为一个为虚拟化而优化裁剪过的内核，可以直接在Hypervisor上加载客户机操作系统。**它相当于在硬件和客户机操作系统之间添加了一个虚拟化层，避免了宿主机操作系统对Hypervisor的干预。Type Ⅰ Hypervisor包括Xen、ACRN等。 与Type Ⅰ Hypervisor不同，Type Ⅱ Hypervisor运行在宿主机操作系统中，只负责实现虚拟化相关功能，物理资源的管理等则复用宿主机操作系统中的相关代码。这种类型的Hypervisor更像是对操作系统的一种拓展。KVM就属于Type Ⅱ Hypervisor，它以模块的形式被动态地加载到Linux内核中。Type Ⅱ Hypervisor又名寄宿虚拟机监控系统，它将虚拟化层直接安装到操作系统之上，每台虚拟机就像在宿主机操作系统上运行的一个进程。一般而言，相对于Type Ⅱ Hypervisor，Type Ⅰ Hypervisor系统效率更高，具有更好的可扩展性、健壮性和性能。然而由于Type Ⅱ Hypervisor使用方便，与宿主机操作系统生态兼容，目前大多数桌面用户使用的都是该类型的虚拟机，目前常见的Type Ⅱ Hypervisor有KVM、VMware Fusion、VirtualBox和Parallels Desktop等。 3.3 系统虚拟化实现方式 系统虚拟化技术**按照实现方式可以分为基于软件的全虚拟化技术、硬件辅助虚拟化技术和半虚拟化技术。**一般来说，虚拟机上的客户机操作系统 “认为” 自己运行在真实的物理硬件资源上，但为了提升虚拟化性能，会修改客户机操作系统使之与Hypervisor相互协作共同完成某些操作。这种虚拟化方案称为PV（Para-Virtualization，半虚拟化），与之对应的是全虚拟化 (Full Virtualization)，无须修改客户机操作系统就可以正常运行虚拟机。下面分别从基于软件的全虚拟化、硬件辅助虚拟化和半虚拟化三个方面对上述虚拟化实现技术进行分析。\n基于软件的全虚拟化 基于软件的全虚拟化技术采用解释执行、扫描与修补、二进制翻译 (Binary Translation,BT)等模拟技术弥补虚拟化漏洞。\n解释执行采用软件模拟虚拟机中每条指令的执行效果，相当于每条指令都需要“陷入”，这无疑违背了虚拟化的高效原则； 扫描与修补为每条敏感指令在Hypervisor中生成对应的补丁代码，然后扫描虚拟机中的代码段，将所有的敏感指令替换为跳转指令，跳转到Hypervisor中执行对应的补丁代码； 二进制翻译则以基本块为单位进行翻译，翻译是指将基本块中的特权指令与敏感指令转换为一系列非敏感指令，它们具有相同的执行效果。对于某些复杂的指令，无法用普通指令模拟出其执行效果，二进制翻译采用和类似扫描与修补的方案，将其替换为函数调用，跳转到Hypervisor进行深度模拟。 以如下x86指令集中 cpuid 指令的执行代码为例。cpuid指令是x86架构中用于获取CPU信息的敏感指令。经过二进制翻译后，将其替换为对helper_cpuid 的函数调用，即获取虚拟CPU的配置信息并返回，模拟出真实 cpuid 指令的执行效果。翻译前后代码如下：\n对于内存虚拟化，前面提到虚拟化需要引入一层新的地址空间，即GPA。客户虚拟机中的应用使用的是GVA（Guest Virtual Address，客户机虚拟地址），而要访问内存中的数据，必须通过HVA（Host Virtual Address，宿主机虚拟地址）访问HPA。一种可能的解决方案是，将地址转换分为两部分，分别加载GPT（Guest Page Table，客户机页表）和HPT（Host Page Table，宿主机页表），完成GVA到GPA再到HPA的转换，其中转换GVA→GPA由GPT完成，而转换HVA→HPA由HPT完成，而中间转换GPA→HVA通常由Hypervisor维护，这样通过复杂的GVA→GPA→HVA→HPA多级转换，完成了客户虚拟机的内存访问，开销比较大。\n因此基于软件的虚拟化技术引入了SPT（Shadow Page Table，影子页表），记录了从GVA到HPA的直接映射，只需要将SPT基地址加载到页表基地址寄存器（例如CR3）中即可完成从GVA到HPA的转换。由于每个进程有自己的虚拟地址空间，因此SPT的数目与虚拟机中进程数目相同。为了维护SPT与GPT的一致性，Hypervisor需要截获虚拟机对GPT的修改，并在处理函数中对SPT进行相应的修改。\n物理机访问I/O设备一般是通过PIO或MMIO的方式，因此I/O虚拟化需要Hypervisor截获这些操作。在x86场景下，PIO截获十分简单，因为设备发起PIO一般需要执行IN、OUT、INS和OUTS指令，这四条指令都是敏感指令，可以利用CPU虚拟化中提到的方式进行截获。而对于MMIO而言，CPU是通过访问内存的方式发起的。因此Hypervisor采用一种巧妙的方式解决这一问题：在建立SPT时，Hypervisor不会为虚拟机MMIO所属的物理地址区域建立页表项，这样虚拟机MMIO操作就会触发缺页异常从而陷入Hypervisor中进行处理。\n因此，**全虚拟化区分普通用户态指令和系统特权指令。前者直接执行即可，而对后者使用陷入和模拟技术返回到虚拟机监视器系统来模拟执行。**问题的关键在于对敏感指令的处理：全虚拟化实现了一个二进制系统翻译模块，该模块负责在二进制代码层面对代码进行转换，从而将敏感代码转换为可以安全执行的代码，避免了敏感指令的副作用。二进制翻译系统往往还带有缓存功能，显著提高了代码转换的性能。该翻译技术的主要使用者有VMware公司早期版本的系统虚拟化产品。\n虽然全虚拟化无须对客户机操作系统进行任何修改，但却带来了二进制翻译的开销，导致了性能瓶颈。\n硬件辅助虚拟化 为了解决软件在虚拟化引入的性能开销，Intel和AMD等CPU制造厂商都在硬件上加入了对虚拟化的支持，称为硬件辅助虚拟化 (Hardware Assisted Virtualization)。基于硬件辅助虚拟化，也可以实现全虚拟化，不用修改客户机代码。这里以典型的Intel VT技术为例进行简要说明。\n前面提到，原本的x86架构存在虚拟化漏洞，并非所有敏感指令都是特权指令。最直接的解决方案就是让所有敏感指令都能触发异常，但是这将改变指令的语义，导致现有的软件无法正常运行。于是Intel VT-x引入了VMX（Virtual-Machine Extensions，虚拟机扩展）操作模式，包括根模式(root mode)和非根模式(non-root mode)，其中Hypervisor运行在根模式而虚拟机运行在非根模式。在非根模式下，所有敏感指令都会触发VM-Exit陷入Hypervisor中，而其他指令则可以在CPU上正常运行。VMX的引入使得Hypervisor无须大费周章地去识别所有的敏感指令，极大地提升了虚拟化的性能。\n而对于内存虚拟化，前面提到可能需要两次地址转换，这就需要不断地切换页表寄存器CR3的值。因此软件全虚拟化技术引入了SPT，直接将GVA转换为HPA，而Intel VT-x引入了EPT（Extended Page Table，扩展页表）。原本的CR3装载客户页表将GVA转换为GPA，而EPT负责将GPA转换为HPA，直接在硬件上完成了两次地址转换。两次地址转换即GVA→GPA→HPA，其中转换GVA→GPA仍由客户机操作系统的GPT转换，不用修改；而第二次GPA→HPA由硬件EPT自动转换，对客户虚拟机透明。\n虽然SPT是直接将GVA转换为HPA(GVA→HPA)，只有一次硬件转换。在没有页表修改的条件下，SPT更高效；然而客户机页表的修改需要通过VM-Exit陷出到Hypervisor进行模拟以保证页表同步，导致了SPT的性能问题。采用EPT后，客户机页表的修改不会导致EPT的同步（没有VM-Exit），因此EPT更为高效。此外，前面提到SPT的数量与虚拟机中进程数目相对应，而由于EPT是将GPA转换为HPA，所以理论上只需要为每个虚拟机维护一个页表即可，减少了内存占用。\n在I/O虚拟化方面，Intel引入了Intel VT-d（Virtualization Technology for Direct I/O，直接I/O虚拟化技术）等硬件优化技术。相较于软件全虚拟化技术需要对设备进行模拟，Intel VT-d支持直接将某个物理设备直通给某个虚拟机使用，这样虚拟机可以直接通过I/O地址空间操作物理设备。但也引入了新问题，原本物理设备发起直接内存访问（DMA）需要的是宿主机物理地址，但将它分配给某个虚拟机后，该虚拟机只能为其提供客户机物理地址。因此Intel VT-d硬件上必须有一个单元（DMA重映射硬件）负责将GPA转换为HPA。这种设备直通分配有一个明显缺陷，即一个物理设备只能供一个虚拟机独占使用，这就需要更多的物理硬件资源。\nSR-IOV（Single Root I/O Virtualization，单根I/O虚拟化）设备可以缓解这一问题。每个SR-IOV设备拥有一个PF（Physical Function，物理功能）和多个VF（Virtual Function，虚拟功能），每个VF都可以指定给某个虚拟机使用，这样从单个物理设备上提供了多个虚拟设备分配给不同的虚拟机使用。但SR-IOV的VF数量受硬件限制，限定了虚拟设备的可扩展性。\n半虚拟化 半虚拟化打破了虚拟机与Hypervisor之间的界限，在某种程度上，虚拟机不再对自己的物理运行环境一无所知，它会与Hypervisor相互配合以期获得更好的性能。在半虚拟化环境中，虚拟机将所有的敏感指令替换为主动发起的超调用 (Hypercall)。Hypercall类似于系统调用，是由客户机操作系统在需要Hypervisor服务时主动发起的。通过Hypercall，客户机操作系统主动配合敏感指令的执行（这些指令受Hypervisor监控），大大减少了虚拟化开销。\n相较于全虚拟化的暴力替换，半虚拟化则另辟蹊径。它对客户机操作系统中的敏感指令都进行了替换，取而代之的是Hypervisor新增的Hypercall。这些Hypercall实现了敏感指令本身的功能，同时在每次执行时都确保能够退回到Hypervisor。此外，不仅仅是敏感指令的处理，如果客户机操作系统能够意识到自身运行于虚拟机环境下，并与Hypervisor进行配合，修改代码进行针对性优化，就能提升性能。\nXen早期采用半虚拟化作为性能提升的主要手段，并大获成功。因此相较于CPU虚拟化，由于I/O半虚拟化性能提升明显，更受开发者的关注，如virtio、Vhost、Vhost-user和vDPA等一系列优化技术被提出来，并且不断演进。因此，相较于全虚拟化，半虚拟化能够提供更好的性能，但代价却是对客户机操作系统代码的修改。为了支持各种操作系统的各种版本，半虚拟化虚拟机监视器的实现者必须付出大量代价做适配工作（virtio提供了标准化的半虚拟化硬件接口，减少了适配难度），并且由于半虚拟化需要修改源代码，对不开源的系统（如Windows）适配比较困难（目前Windows设备驱动程序也广泛支持virtio接口）。\n总结 下表总结了三种虚拟化实现方式对CPU、内存和I/O虚拟化等方面的影响。\n目前主流硬件架构（包括x86、ARM等）都对硬件辅助虚拟化提供了支持，RISC-V的虚拟化硬件扩展作为重要的架构规范也正在完善中。同时，半虚拟化驱动规范virtio接口标准正在普及，目前主流的操作系统都提供了virtio的支持。因此，目前主流的虚拟化实现方式是硬件辅助虚拟化和半虚拟化，大幅降低了虚拟化性能的开销。\n在CPU、内存和I/O三类虚拟资源中，前两类都已经得到较好的解决。由于I/O设备抽象困难、切换频繁、非常态事件高发等原因，开销往往高达25%~66%（万兆网卡中断达70万次/秒，虚拟化场景下占用高达5个CPU核），因此I/O成为需要攻克的主要效能瓶颈。\n此外，虚拟化可以嵌套(Nested Virtualization)，即在虚拟机（L1，第一层）中创建和运行虚拟机（L2，第二层），以此类推（物理机看作第零层，L0）。如果CPU支持嵌套的硬件辅助虚拟化（如Intel x86），则可以降低嵌套的虚拟机性能损失。\n4 典型虚拟化系统 针对上述三种主要的虚拟化实现方式，本节主要介绍在虚拟化历史上起重要作用、并且目前仍在广泛使用的虚拟化系统。通过介绍典型虚拟化系统，加深对虚拟化方法的理解。\n4.1 典型虚拟化系统简介 VMware 斯坦福大学Mendel Rosenblum教授带领课题组研发了分布式操作系统Hive、机器模拟器SimOS和虚拟机监控器DISCO。基于这些技术积累，Mendel Rosenblum作为共同创始人在1998年创建了VMware公司，也是硅谷产学研结合的典型代表。由于当时x86架构在硬件上还不支持虚拟化，因此VMware公司采用动态二进制翻译技术与直接执行相结合的全虚拟化技术来优化性能，其虚拟机性能达到物理机的80%以上，CPU密集型的应用性能损失仅为3%~5%。具体而言，虚拟机用户态代码可直接运行（包括虚拟8086模式），虚拟机内核态代码基于动态二进制翻译执行。虽然动态二进制翻译增加了开销，但保证了敏感指令在Hypervisor监控下执行，符合波佩克与戈德堡虚拟化需求中第一条准则（资源控制），弥补了x86架构的虚拟化漏洞。同时，由于用户态代码直接运行，性能比采用纯二进制翻译的系统大为提高，取得了x86架构中虚拟化技术的突破，也被视为第一个成功商业化的虚拟化x86架构。\nVMware公司在1999年发布了桌面虚拟化产品Workstation 1.0（见下图(a)），可以在一台PC上以虚拟机的形式运行多个操作系统，属于Type Ⅱ全虚拟化技术，Windows客户机操作系统不加修改就可以运行。2002年VMware公司发布了其第一代Type I虚拟化产品ESX Server 1.5（见下图(b)），采用服务器整合 (Server Consolidation) 的方式，支持将多个服务器整合到较少的物理设备中。通常可以将10台虚拟机整合到1台物理机中(10∶1)，这种“一虚多”的虚拟化方式大幅提升了服务器资源利用率，降低了数据中心的硬件成本，因此得到了广泛应用。\n注：①管理物理硬件；②实现虚拟化功能。\nXen 2003年，剑桥大学Ian Pratt教授等发表了虚拟化领域的著名论文Xen and the art of virtualization，提出了以半虚拟化技术为基础的代表性开源项目Xen。由于当时x86硬件还没有支持虚拟化，如果不修改客户机操作系统，采用二进制翻译或软件模拟的全虚拟化技术，则内存和I/O的虚拟化开销比较大。以内存虚拟化为例，VMware采用影子页表技术，系统同时存在影子页表和客户机操作系统的原有页表，两套页表通过缺页异常(Page Fault)保持同步，引入了大量的同步开销。而Xen通过修改客户机操作系统，不再保留两套页表（消除同步开销），同时通过超调用主动更新页表。这样既保证了客户机操作系统可以快速访问页表，同时每次更新都需要经过Xen的监控，又保证了Hypervisor对内存资源的资源控制。\n注：①DomU前端驱动将数据写入I/O环；②Dom0后端驱动读取I/O环数据；③后端驱动调用设备驱动；④设备驱动操作物理设备。\nXen属于Type I Hypervisor（见上图），Hypervisor直接运行于物理硬件上，为了提供功能丰富的设备模型，提出了基于Dom0和DomU的资源管理架构。Dom0是修改后的特权Linux内核，专门提供访问物理设备的特权操作。并行存在多个DomU，但均没有访问物理设备的权限（设备直通除外），在客户机操作系统中提供了各种设备的前端驱动(Frontend Driver)，取代了原有的设备驱动（需要修改客户机操作系统）。前端驱动通过I/O环(I/O Ring)和后端驱动(Backend Driver)共享数据（基于共享内存避免数据复制），前端驱动和后端驱动的控制面通过事件通道(Event Channel)进行通信，并通过授权表(Grant Table)控制各个虚拟机的访问权限（事件通道和授权表在图中均未画出），后端驱动通过传统的设备驱动访问真实物理设备。\nQEMU/KVM VMware和Xen均是在x86硬件不支持虚拟化的情况下，采用软件的手段弥补虚拟化漏洞，随着Intel和AMD等厂商相继提出了硬件支持的虚拟化扩展，基于硬件辅助的虚拟化技术应运而生，并由于其性能优势，逐渐占据主流地位。KVM是Linux内核提供的开源Hypervisor，也是目前主流的虚拟化技术，拥有活跃的社区和论坛(KVM Forum)。KVM自内核2.6.20起被合并进Linux，作为Linux的一个内核模块，在Linux启动时被动态加载。KVM利用了硬件辅助虚拟化的特性，能够高效地实现CPU和内存的虚拟化。\n事实上，KVM无法单独使用，因为它既不提供I/O设备的模拟，也不支持对整体虚拟机的状态进行管理。它向用户态程序（如QEMU）暴露特殊的设备文件 /dev/kvm 作为接口，允许用户态程序利用它来实现最为关键的CPU和内存虚拟化，但还缺少I/O虚拟化需要的设备模型 (Device Model)，而QEMU正好可以弥补这块功能。QEMU是开源的软件仿真器，它能够通过动态二进制翻译技术来实现CPU虚拟化，同时提供多种I/O设备的模拟，因此可以作为低速的Type Ⅱ虚拟机监视器系统进行工作。然而，二进制翻译这种模拟方法带来了巨大的性能开销，导致虚拟机运行缓慢。为此QEMU利用KVM暴露的 /dev/kvm 接口，以KVM作为“加速器”，从而极大地改善虚拟机的性能。\n由于QEMU通常和KVM配合使用，因此整体称为QEMU/KVM，其架构如下图所示：\nQEMU通过打开设备文件 /dev/kvm 实现和KVM内核模块 (kvm.ko) 的交互。在创建虚拟机时，QEMU会根据用户配置完成创建vCPU线程、分配虚拟机内存、创建虚拟设备（包括磁盘、网卡等）等工作。在QEMU中，虚拟机的每个vCPU对应QEMU的一个线程，当QEMU完成了所有初始化工作后，会通过 ioctl 指令进入内核态的KVM模块中，由KVM模块通过虚拟机启动或恢复指令（如x86的 VMLAUNCH/VMRESUME 指令）切换到虚拟机运行，执行虚拟机代码。因此从宿主机操作系统的角度来看，虚拟机的每个CPU对应于系统中的一个线程，并且该线程受到QEMU的控制和管理。当CPU执行了特权指令或发生特定行为时，会触发虚拟机陷出事件退出到KVM，由KVM判断能否进行处理（比如对一个QEMU中模拟的I/O设备进行操作）。如果不能，则进一步返回到QEMU，由QEMU负责处理。\nACRN 随着万物互联时代的到来，**虚拟化技术进一步扩展到移动终端、嵌入式和车载设备等资源受限的平台，**一系列嵌入式、轻量级虚拟化被提出来，例如QNX Hypervisor、Jailhouse、Xvisor、PikeOS和OKL4等。Linux基金会于2018年3月发布了开源的轻量级虚拟化平台ACRN。ACRN针对实时性和安全性（针对车载场景）进行了适配优化，并为关键业务提供了安全性隔离。ACRN通过GVT-g支持GPU虚拟化，可以在车载场景下共享GPU。\n注：①前端驱动将数据写入I/O环；②后端驱动读取I/O环数据；③后端驱动调用设备驱动；④设备驱动操作物理设备。\nACRN是Type I虚拟化架构（见上图），类似于Xen的Dom0，采用特权级的服务操作系统 (Service OS) 来管理物理I/O设备的使用；类似于Xen的DomU，用户操作系统(User OS)由ACRN Hypervisor进行创建和管理。设备分为前、后端驱动，两者之间的数据共享（环/队列）通过标准的virtio接口进行访问。\n下表总结了上述4种典型虚拟化系统的特点，它们是2000年之后有代表性的虚拟化系统，至今仍广泛使用。\nIBM、微软等主流操作系统厂商的虚拟化产品、亚马逊和阿里巴巴等主流云计算提供商的新型虚拟化技术（如亚马逊Nitro Hypervisor、阿里神龙服务器）和VirtualBox、Xvisor等开源系统也在市场中占据重要地位，限于篇幅，本书不做介绍。\n4.2 openEuler的虚拟化技术 openEuler是一款开源操作系统。当前openEuler内核源于Linux，支持鲲鹏及其他多种处理器，是由全球开源贡献者构建的高效、稳定且安全的开源操作系统，适用于数据库、大数据、云计算和人工智能等应用场景。同时，openEuler是一个面向全球的操作系统开源社区，通过社区合作打造创新平台，构建支持多处理器架构、统一和开放的操作系统。openEuler社区还孵化了A-Tune和iSula两个开源子项目。A-Tune是智能性能优化系统软件，即通过机器学习引擎对业务应用建立精准模型，再根据业务负载智能匹配最佳操作系统配置参数组合，实现系统整体运行效率的提升；iSula是一种云原生轻量级容器解决方案，可以通过统一、灵活的架构满足ICT（Information Communications Technology，信息和通信技术）领域端、边与云场景的多种需求。在全场景虚拟化方面，提出了基于Rust语言的下一代虚拟化平台StratoVirt，构建面向云数据中心的企业级虚拟化平台，实现了一套架构统一支持虚拟机、容器和Serverless三种场景。\nopenEuler社区版本分为LTS（Long-Term Storage，长期支持版本）和创新版本，版本号按照交付年份和月份进行命名。\n长期支持版本。发布间隔定为2年，提供4年社区支持。社区LTS版本openEuler 20.03于2020年3月正式发布。\n社区创新版本。每隔6个月openEuler会发布一个社区创新版本，提供6个月社区支持。\nopenEuler 20.09于2020年9月发布。openEuler目前提供了支持AArch64和x86_64处理器架构的KVM虚拟化组件，支持鲲鹏处理器和容器虚拟化技术，与开源QEMU、StratoVirt和Libvirt等共同构成了完整的系统虚拟化运行环境。有关openEuler及虚拟化组件的安装、配置和管理请参考openEuler开源网站。\n下表列出了openEuler软件包提供的虚拟化相关组件。\n5 总结 本章简要介绍了系统虚拟化的基本概念，回顾系统虚拟化的发展历史并展望其发展趋势，还介绍了系统虚拟化的主要功能、分类和目前使用广泛的典型系统虚拟化项目，以及openEuler操作系统及其虚拟化技术。通过本章，希望各位对虚拟化技术的来龙去脉有较为宏观的理解，为后续的学习打下基础。\n","date":"2024-10-17T10:06:10+08:00","permalink":"https://zcxggmu.github.io/p/kvmbook-overview_1/","title":"Kvmbook Overview_1"},{"content":"Before RISC-V AIA \u0026hellip;\n1 RISC-V 异常与中断 1.1 基本概念与分类 在RISC-V标准中，将**异常(exception)定义为当前CPU运行时遇到的与指令有关的不寻常情况，而使用中断(interrupt)定义为因为外部异步信号而引起的让控制流脱离当前CPU的事件。而陷阱(trap)**表示的则是，由异常或者中断引起的控制权转移到陷阱处理程序的过程。\n中断也可以进一步细分为两种情况，一种是本地中断，一种是全局中断。本地中断包括软件中断和定时器中断，它们由CLINT (Core-Local Interruptor, 核心本地中断器) 来控制；另一种是外部设备引起的中断，也被称为全局中断，它们由PLIC (Platform-Level Interrupt Controller,平台级中断控制器) 来控制。PLIC、CLINT和RISC-V Core的连接逻辑示意图如下：\n而无论是异常还是中断，它们所导致的硬件陷阱流程都是一样的，具体来说有两个大动作：\n改变控制流 将PC的值保存在sepc中，然后将stvec中保存的陷阱处理函数地址放入PC。\n更新CSRs的值 包括：sstatus/stval/scause\n当我们说一个 X-Mode 的异常时 (X可以是U/S/M任何一种模式)，我们指的本质上是当异常发生时 CPU 正工作在 X-Mode。而 X-Mode 的中断我们指的是，PLIC触发了一个必须由 X-Mode 响应的中断，CPU并不一定需要正在工作在此模式下。\n时钟中断作为一种特例，在RISC-V处理器中被硬连线为一个M-Mode的中断，这也就是说它必须由M模式下的 interrupt handler 响应，当然这是后话，我们会在后面重提。\n1.2 陷阱的委派 事实上，在RISC-V的标准定义中，所有陷阱默认都是由机器模式(M-mode)来处理的。然而，在支持操作系统的设备上往往都实现了监管者模式 (S-Mode)，如果按照默认模式发生中断则应该首先陷入 M-Mode 下的中断处理程序，然后触发一个S-Mode 下的中断再 mret 回 S-Mode 下处理，这个过程过于繁琐且需要程序员自己实现，所以：\nRISC-V标准为了应对这种情况提出了陷阱委派机制。也就是说在M-Mode下可以配置寄存器，从而使得S-Mode下的所有陷阱都被S-Mode下的陷阱处理函数自动接管。有两个寄存器，medeleg/mideleg，分别用来管理异常和中断的委派。\n首先是 medeleg 寄存器，它的示意图如下所示，当要把特定的异常委托给S-Mode时，只需要将 mcause 寄存器中对应数值位置的比特位置 1 即可，参考下面的表格。比如当把系统调用委托给S-Mode时，因为它对应的Exception Code是 8，那么只需要将此寄存器中第 8 位置为 1 即可。\n注意，M-Mode下发生异常，即使它被托管到S-Mode，这个异常也不会被移交给S-Mode去处理，而是在M-Mode完成处理。这也就是RISC-V规范中所说的：Traps never transition from a more-privileged mode to a less-privileged mode(陷阱从不会从高优先级移交到低优先级)但是如果是发生在S-Mode下的异常，就会被S-Mode直接接管，不会再上交到M-Mode了，这被称为：traps may be taken horizontally(中断可以被水平接管)。\n但是，如果一个属于S-Mode的中断从M-Mode委托给S-Mode（比方我们后面要看到的外部中断），这个中断在M-Mode下就会被屏蔽掉，一定要等到进入S-Mode时才会被处理。负责管理中断委派的是 mideleg 寄存器。\n这里有一些点值得澄清：在实现时，无论是在 qemu 还是在真正的 Verilog 实现中，所谓 mideleg 寄存器中M态下的本地中断（包括时钟中断和软中断）都是被硬连线直接连接到0的，所以委派时赋值全赋值为1只是出于方便，并不能真的把M-Mode下的本地中断交给S-Mode处理。**唯一可以被委派到 S-Mode 的中断是外部中断 MEI，**外部中断一旦被委派到 S-Mode，PLIC 就只会触发 S-Mode 下的中断挂起位 sip.SEIP了，我们在后面会看到更多细节。（注意，这里 M-Mode 下的本地中断在实现时不能实现委派是 SiFive_Unleashed 这块RISC-V平台的实现特例，具体情况要具体分析，也许以后电路设计不同了，结论就会有所改变，但在这里是成立的）。\n所以其实时钟中断和软中断默认情况下都是M-Mode中断，这是由硬件实现决定的，可能你会问那么 STI/SSI 还有什么意义呢，反正它们也不会被触发。事实上，它们被触发的唯一可能场景就是在M-Mode下的中断处理程序中被置位，然后通过mret回到S-Mode下再处理。\n1.3 CSRs sstatus sstatus 寄存器的全称是 supervisor status register，顾名思义，它是用来反映S-Mode下处理器的工作状态的。当执行RV64时，sstatus 寄存器的示意图如下所示：\n在 sstatus 中与异常和中断有关的位有 SPP/SIE/SPIE，下面一个一个来看。\nSPP\nSPP记录的是在进入S-Mode之前处理器的特权级别，为0表示陷阱源自用户模式(U-Mode)，为1表示其他模式。当执行一个陷阱时，SPP会由硬件自动根据当前处理器所处的状态自动设置为0或者1。当执行SRET指令从陷阱中返回时，如果SPP位为0，那么处理器将会返回U-Mode，为1则会返回S-Mode，最后无论如何，SPP都被设置为0。\nSIE\nSIE位是S-Mode下中断的总开关，就像是一个总电闸。如果SIE位置为0，那么无论在S-Mode下发生什么中断，处理器都不会响应。但是如果当前处理器运行在U-Mode，那么这一位不论是0还是1，S-Mode下的中断都是默认打开的。也就是说在任何时候S-Mode都有权因为自己的中断而抢占位于U-Mode下的处理器，这一点要格外注意。想想定时器中断而导致的CPU调度，事实上这种设计是非常合理的。\n所以SIE位是S-Mode下的总开关，而不是任何情况下的总开关，真正完全负责禁用或者启用特定中断的位，都在 sie 寄存器中，下面再详细解释。\nSPIE\nSPIE位记录的是在进入S-Mode之前S-Mode中断是否开启。当进入陷阱时，硬件会自动将SIE位放置到SPIE位上，相当于起到了记录原先SIE值的作用，并最后将SIE置为0，表明硬件不希望在处理一个陷阱的同时被其他中断所打扰，也就是从硬件的实现逻辑上来说不支持嵌套中断(即便如此，我们还是可以手动打开SIE位以支持嵌套中断)。当使用SRET指令从S-Mode返回时，SPIE的值会重新放置到SIE位上来恢复原先的值，并且将SPIE的值置为1。\n关于RISC-V中位于不同特权模式下的响应问题，标准中有进一步的说明：\n当一个处理器在特权模式 x(x = U/S/M) 运行时，xIE 控制着对应模式下的中断总开关。**对于更低优先级的中断来说，是默认全部禁用的，而对于更高优先级的中断来说是默认全部打开的。**这意味着在任何时候高优先级都拥有着对处理器的抢占权，而低优先级则被强行剥夺打断高优先级的权力。\n那么用户难道对这种机制无能为力，只能任凭高优先级随意打断低优先级模式吗？其实也不是，标准上说可以通过禁用某种特定的中断，使得对应的中断发生时，低优先级的运行不受影响，这是通过设置 mie/sie 寄存器实现的。(Higher-privilege-level code can use separate per-interrupt enable bits to disable selected higher-privilege-mode interrupts before ceding control to a lower-privilege mode.)\n另外，RISC-V在硬件层面上来说不支持陷阱的嵌套，硬件只假设最理想的情况，即：发生陷阱-\u0026gt;处理它(在此期间完全不发生其他陷阱)-\u0026gt;恢复到原先状态。\nstvec stvec 寄存器的全称是 supervisor trap vector base address register(S态陷阱向量基址寄存器，stvec)，它的示意图如下：\n可以看到stvec寄存器分为两个域，分别是BASE域和MODE域：\nMODE 域\n在RISC-V标准的定义中，MODE域可能的取值有如下三种：、\n可以看到，MODE 域的取值影响了发生陷阱时要跳转到的地址。之前我们为了描述的方便，一般简单地说 stvec 中存放着中断处理程序的地址，发生陷阱时会直接跳转到。但其实 stvec 寄存器中不仅仅是中断处理程序的地址，还存放着 MODE 域的值。\n当BASE域为0时，所有的陷阱全部跳转到BASE地址指向的程序入口； 当BASE域为1时，同步陷阱 (指因为指令而引起的异常) 还是会跳转到BASE地址处，而非同步陷阱 (指由外部信号引起的中断) 则会跳转到 BASE + 4*cause 的地方，这里手册上举了一个例子：如果我们将MODE设置为Vectored 状态，同时触发了一个 S-Mode 定时器中断 (中断号为5)，则程序会跳转到 BASE + 4*5 = BASE + 0x14 的位置。 BASE 域\nBASE域存放着一个基地址，在 Direct/Vectored 模式下的同步陷阱情况下，它都指向中断处理程序的地址 (这里的地址可以是虚拟地址，也可以是物理地址)。\n在Vectored模式的非同步陷阱的情况下，比如由外部中断引起的陷阱时，它是中断服务程序的基址，加上一个特定偏移量之后才可以对应到对应的中断处理程序。\n另外，BASE域有额外的限制因素，它必须是 4 字节对齐的 (4-byte aligned)。内核实现时，代码中需要添加：\n1 .align 4 这是为了让代码文件按照4字节对齐，从而满足了 stvec 寄存器的写入需求。\nsip/sie sip/sie 这两个寄存器与中断的处理密切相关，在手册对应的部分不仅有关于这两个寄存器的说明。而且还有大量有关中断机制的细节描述，接下来的各个小段都是从RISC-V标准中阅读得到的总结，在此做一个详细记录。\nsip/sie 都是处理器与中断有关的寄存器，它们合称 (Supervisor Interrupt Registers，S-Mode 中断寄存器)，sip 专门用来记载挂起的中断位，sie 专门用来记载使能的中断位，它们的示意图如下：\n在RV-64标准中，这两个寄存器都是64位大小的。 标准中断只占据了这两个寄存器的低16位(15:0)，更高的位都被保存下来以作为扩展用途。和 medeleg/mideleg 寄存器的用法差不多，将中断号对应的位在这两个寄存器中置位即可，如下图所示：\n如果我们想触发一个 S-Mode 下的软件中断，因为它的异常编号是1，则只需要在sip中将第 1 位设置为 1 即可，sie 寄存器的使用方法也是一样的，它表示的是对应编号的中断是否使能。\n中断被呈递到何种特权级下处理？ 这是一个非常重要的话题，在RISC-V处理器中的三种特权模式来回切换让人眼花缭乱，而它们又与对应模式下的中断紧密联系在一起，再加上我们上面说的陷阱委托机制，让整个RISC-V架构的陷阱与特权模式对应关系非常混乱。幸好，RISC-V标准中明确说明了中断应该被放在哪种特权模式下处理的对应原则：\n当下述条件全部满足时，中断一定要在M-Mode下处理： 当前特权级别a为 M-Mode 且 mstatus中的 MIE 打开，或当前特权级别低于 M-Mode（中断总开关打开） mip/mie 中的第 i 位都被设置为 1 (表明中断待处理且使能位打开) mideleg 寄存器存在，且第 i 位没有被设置 （中断没有委托给S Mode） 所以，如果内核使用了陷阱委托机制，这里彻底将外部中断的第三个条件破坏了。\n当下述条件同时满足时，中断一定要在S-Mode下处理： 当前特权级别为S-Mode且sstatus中的SIE打开，或当前特权级别低于S-Mode (中断总开关打开) sip/sie 中的第 i 位都被设置为 1 (表明中断待处理且使能位打开) 响应中断的时机 在RISC-V标准中有关sip和sie寄存器的介绍中，手册中有这样一段话：\nThese conditions for an interrupt trap to occur must be evaluated in a bounded amount of time from when an interrupt becomes, or ceases to be, pending in sip, and must also be evaluated immediately following the execution of an SRET instruction or an explicit write to a CSR on which these interrupt trap conditions expressly depend (including sip, sie and sstatus).\n译：这些中断陷阱发生的条件(指上述5.1节中的条件)，必须在一个中断在sip中被置位或清除后的一段时间内得到检测。 并且在紧跟着SRET指令之后，或对CSR寄存器中与中断陷阱条件有关的寄存器(包括sie、sip、sstatus)进行显式写入之后立即检测。\n这就是标准中有关中断检测时机的描述，具体的实现可能和芯片的微体系结构有关，但现在我们可以假定在上述动作完成之后，就会进入中断的条件检测，而一旦条件满足就开始响应中断。\n外部中断、claim/complete机制 sip 寄存器中的每一位在实现时，可以是可读可写的，也可以是只读的，也就是说这里RISC-V标准并没有对 sip 寄存器的读写性加以严格的约束。但是后面标准中提到，如果 sip 寄存器中的位可写，那么清除某个特定中断的操作就是向它的对应位写入 0 即可。但如果 sip 寄存器是只读的，那么必须提供其他机制来实现对 sip 对应位的清零，具体怎么实现则要看微处理器的具体构成和实现。**注意，这里所说的只读 (read-only) 只是说，我们不能用指令的形式去有效地清除对应的pending bit，但绝非没有硬件电路能去改变它，**我们下面就会看到是怎么做的了。\n对应的，sie 作为 sip 对应的使能位，如果一个中断可以被挂起，那么它对应的 sie 位一定是可写的。但如果对应的位在 sip 中没有对应，那么 sie 对应的位就不能写入，而是只读的 0 (read-only zero)。\n上述寄存器中负责控制外部中断的位有 sip.SEIP/sie.SEIE 这两个位，在一般的处理器中 sip.SEIP 就会被实现为只读的 (read-only)，它的清除与置位操作一般会直接交给特定平台的中断控制器来做。在 SiFive Unleashed 开发板上，处理外部中断的就是 PLIC 这个负责管理全局外部中断的 IP 核。CPU核心只需要对 sip.SEIP 进行读取即可知道是否有外部中断挂起。\n这里我们直接顺势对 PLIC 和中断处理流程做出延伸，打开 SiFive 的开发板手册，找到关于 Interrupts 一章的概述。我们可以看到这一章的开头就对开发板的 PLIC 做出了描述：\nGlobal interrupts, by contrast, are routed through a Platform-Level Interrupt Controller (PLIC), which can direct interrupts to any hart in the system via the external interrupt. Decoupling global interrupts from the hart(s) allows the design of the PLIC to be tailored to the platform, permitting a broad range of attributes like the number of interrupts and the prioritization and routing schemes.\n(译：相比之下，全局中断通过平台级中断控制器(PLIC)进行路由，PLIC可以通过外部中断将中断引导到系统中的任何hart。将全局中断与hart解耦，可以根据平台定制PLIC的设计，允许广泛的属性，如中断数量、优先级和路由方案)。\n所以其实 PLIC 这样的中断控制器的引入是为了更加灵活地对外部中断进行管理，使得开发板有更好的适应性与可裁剪性。就像本科时我们学过的微机原理，8059A也就是8086的中断控制器一样，可以对外部中断进行更好的管理，在我们的SiFive 开发板上一共支持53个13大类外部设备中断，它们预留的ID范围如下图所示。这么多的中断其实都属于外部中断，但是它们在RISC-V的CPU中其实只有一个 sip.SEIP/sie.SEIE 两个位对应。所以，我们也不得不用一个强大的PLIC核心去细化和丰富对外部设备中断的管理过程。\n在更加深入地对中断进行介绍之前，我们还要对PLIC的 claim/completion 机制进行解释。让我们回顾一下PLIC与RISC-V核心之间的连接关系，一个PLIC本质上连接了多个RISC-V核心。当一个外部设备发出中断请求时，经过PLIC中的Interrupt Gateways的信号转换等操作，其实PLIC会将这个中断转发给多个核心(前提是对应核心中 sie 寄存器中对应位是打开的)，这个操作叫做 interrupt notification，本质是PLIC将对应核心 sip 寄存器中的对应位置位。\n这个 notification 操作在简单系统的硬件实现中，是通过将PLIC核心中的 PIE 位硬连线到对应的 sip.SEIP 寄存器位来实现的，而在复杂的系统中是通过复杂的片上路由网络实现的。在外部中断全部被委派到S-Mode的条件下，它只会导致多个核心中的 sip.SEIP 位置位 (The notification only appear in lower-privilege xip registers if external interrupts have been delegated to the lower-privilege modes, quoted from PLIC doc)。根据我们之前总结的触发中断的条件，多个核心或早或晚会这个对挂起的中断进行响应。那么，多个核心之间其实就已经有了对中断响应的竞争。\n一个核心如何对 interrupt notification 进行响应呢？这个操作我们叫做 claim 操作，本质就是一个简单的对claim/complete 寄存器的读操作，claim/complete 寄存器会保存一个当前未被处理且具有最高优先级的中断的 ID 号。CPU读取 claim/complete 寄存器就可以获得这个 ID 号，同时也就完成了对此中断的 claim 操作，回到内核中就可以匹配对应的中断处理程序 (interrupt handler) 进行对应的处理。在 drivers/irqchip/irq-sifive-plic.c 中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 /* * Handling an interrupt is a two-step process: first you claim the interrupt * by reading the claim register, then you complete the interrupt by writing * that source ID back to the same claim register. This automatically enables * and disables the interrupt, so there\u0026#39;s nothing else to do. */ static void plic_handle_irq(struct irq_desc *desc) { //... while ((hwirq = readl(claim))) { int err = generic_handle_domain_irq(handler-\u0026gt;priv-\u0026gt;irqdomain, hwirq); if (unlikely(err)) pr_warn_ratelimited(\u0026#34;can\u0026#39;t find mapping for hwirq %lu\\n\u0026#34;, hwirq); } //... } 当一个中断的 notification 被 claim 后 (也就是 claim/complete 寄存器被读取后)，PLIC 核心会自动清除掉对应中断源在PLIC 中的挂起位 (clear down the corresponding source’s IP bit)，表示这个中断已经被处理了。\n当一个高优先级的中断被处理之后，低优先级中断可能又会补上，所以PLIC中的PIE位可能还是不会被清零，这也就导致了多个核心中的 sip.SEIP 位其实还是没有被清 0 。也就是说，在一个多核平台上，可能同时发生的，需要被处理的中断不止一个。这时，它们可能会被其他核心响应。即使是在一个核心内部，PLIC标准上也建议，在 interrupt handler 返回前可以查看一下核心自己的 sip.SEIP 位是否还为1，如果还是1，就可以跳过恢复现场的步骤，直接 claim 下一个中断并服务吧，节省时间 ！\n最后，如果一个核心 claim 时已经没有中断要处理了 (有可能是没有竞争过其他核心，也有可能是确实没有中断要处理)，读取PLIC中的 claim/complete 寄存器就会返回0，进而回到 OS 内核后什么也不做，这在上面的代码中也有对应。实际上，中断ID为 0 在标准中对应的含义就是：“无需响应的中断”。\n现在一个核心 claim 了一个中断并将其服务完毕，它就会进行一个 complete 操作，这个操作和 claim 操作很像，只是完全相反——它是向 claim/complete 寄存器写入它刚刚完成服务的中断ID号，这个ID号会被转发给 Gateways，进而Gateways 会再次开始对对应ID号的中断进行转发。在没有完成 complete 操作之前，Gateways 是不会转发同样ID号的中断给PLIC核心的，所以在完成complete操作之前这种中断也就不会被处理。\n最后梳理一下这个过程：\n外部设备发起一个中断给PLIC中的Gateways，Gateways负责将中断信号进行转换并转发给PLIC核心，这个过程中会涉及Priority、Threshold等等一系列的判断 (详见上面的电路)，最终会体现在PLIC核心的 EIP 位上； EIP 位一旦被置 1，会直接导致多个核心上的 sip.SEIP 位被置 1。进而导致多个打开此中断的CPU核心开始竞争claim 这个中断，没有竞争过其他核心时会得到中断ID为 0，进而不响应，这个过程叫做 claim 操作； 响应最早的核心成功开始服务这个中断，并在服务完成之后将中断ID号再次写入 claim/completion 寄存器，这个过程叫做 complete 操作； 这个 complete 操作会被转发到 Gateways，进而再次开启对这种中断的转发。 软中断和定时器中断 注意力再次回到 sip/sie 寄存器中，除了 sip.SEIP 只读位用来反映外部中断以外，还有 sip.SSIP/sip.STIP 位来分别反映软中断和时钟中断。在开发板的手册中，这两者都属于本地中断，都交由 CLINT(核心本地中断器) 来管理。\n时钟中断\nsip.STIP/sie.STIE 位分别用来管理时钟中断的挂起和使能，时钟中断在内核中主要是用来做CPU调度的。也就是我们常说的时间片轮转调度算法 (Round-Robin, RR)，一个进程经过一段时间之后会自动让出CPU的使用权，让其他进程进行执行。\nsip.STIP 位也非常特殊，和 sip.SEIP 一样，一旦被实现就只能是只读的 (read-only)，只能被执行环境 (execution environment) 所设置和清除。这里所谓的执行环境指的就是操作系统里的中断服务程序。\n之前，我们就简单提过**时钟中断是被 CLINT 硬连线为一个 M-Mode 中断的，**并且这个中断不能被委派到 S-Mode，所以时钟中断怎么也轮不到内核去处理，它真的得由硬件机制去触发。这里就非常巧妙了，目前简单给出一个结论：**内核中 M-Mode 下的时钟中断的 interrupt handler 会使用汇编代码主动触发一个 S-Mode 下的软中断，从而将时钟中断的处理权移交给位于 S-Mode 下的操作系统，操作系统处理完时钟中断之后会将这个软中断清除以表明完成了对时钟中断的处理。**所以，其实内核借用了 S-Mode 下的软中断去完成对时钟中断的响应，这是因为操作系统往往要借助时钟中断来完成一些功能，而这些功能属于 S-Mode 下的操作系统的一些概念 (比如CPU调度、sigalarm等)，所以其实将原本位于 M-Mode 下的时钟中断移交给 S-Mode 非常自然。\n软中断\n软中断则与 sip.SSIP/sie.SSIE 有关，与前两者不同，它既可以由指令读写，也可以由 CLINT 来控制。**软件中断是通过指令来触发的，具体是写入 CLINT 中的 msip 寄存器。**这个寄存器有 32 位，高 31 位全部被硬连线到 0，最低位则直接映射到 mip.MSIP 位。软件中断在硬件实现中，根据示意图分析，应该也是像时钟中断一样，被强制硬连线到M-Mode下。软中断一般用于多个处理器核心之间的通信，各个核心都可以通过指令触发另外核心的软件中断。\n至此，我们将 sip/sie 寄存器讲解的差不多了，并顺带介绍了大量有关外部中断、时钟中断和软件中断的处理流程和细节。在实际实现RISC-V核心时，并不一定要实现上述的所有中断类型，当某种中断类型没有被实现时，它对应的 sip/sie 寄存器的对应域会被硬连线到 0 。\n最后，当这三种中断同时发生时，它们的相对优先级是 SEI \u0026gt; STI \u0026gt; SSI。\nsscratch sscratch 寄存器的设计是RISC-V中一个非常巧妙的机制，这个寄存器中存放着一个 (虚拟) 地址。当在执行用户代码时，这个地址指向的是一块保存当前程序上下文 (其实就是寄存器组内容，context) 的一块内存区域，可以将其称做陷阱帧trapframe。\n当CPU处理陷阱的时候，PC会被设置为 stvec 寄存器的值，进而陷入一段过渡程序，在这段程序的开头就会将 sscratch寄存器和 a0 寄存器进行交换，这一方面将 a0 的值保存在了 sscratch 寄存器中，同时 a0 指向了 trapframe 的开始地址，这样进而就可以通过 a0 寄存器寻址将所有寄存器的值保存在 trapframe 里，妙绝！\nsepc sepc 寄存器，一言以蔽之就是记录陷阱陷入到 S-Mode 时发生陷阱的指令的虚拟地址，它在执行陷阱时会记录下被中断的指令的虚拟地址，除了刚才说的陷阱场景之外，硬件不会主动改变 sepc 的值。但是 spec 寄存器可以被指令写入，如果执行系统调用的话，内核会将返回地址 +4 并赋给 sepc，这表明系统调用会返回 ecall 的下一条指令继续执行。\nscause scause 寄存器在执行陷阱时由硬件自动记录下导致本次陷阱的原因，其中 interrupt 位表示本次陷阱的原因是否为中断(我们上面说过，陷阱是动作，而中断和异常是导致陷阱的原因)。而 Exception Code 则表示细分的原因，对应关系如下表，可以看到scause还是有很多可扩展的异常号没有被使用的：\n在内核的具体实现中，我们会根据 scause 中记录的异常号实现对陷阱“分门别类”的处理。和 sepc 一样，scause 也支持使用指令写入，但是一般不这么做。\nstval stval 寄存器的全称是 Supervisor Trap Value Register，这个寄存器专门用来存放与陷阱有关的其他信息，目的是帮助操作系统或其他软件更快确定和完成陷阱的处理。手册上相关的叙述显得有些晦涩，这里简单地做一个总结：\n首先，stval 寄存器中存储的值可以是零值或者是非零值，对于大部分未经硬件平台定义的陷阱情况而言，stval 并不会存储与这些陷阱有关的信息，这时候 stval 就会存储零，本质上 stval 没有存储什么有效信息。**存储非零值的情况又分为两种，一种是因为内存访问非法，一种则是因为指令本身非法，**下面详细说说：\n内存访问非法\n这种情况包括硬件断点 (Hardware Breakpoints)、地址未对齐 (address misaligned)、访问故障 (access-fault，可能是没有权限等)、缺页故障 (page-fault) 等情况。当这种情况发生时，stval 会存储出错位置的虚拟地址。比如缺页故障发生时，stval 就会记录到底是对哪个虚拟地址的访问导致了本次缺页故障，内核就可以根据此信息去加载页面进入内存。\n指令执行非法\n当执行的指令非法时，stval 会将这条指令的一部分位记录下来。illegal instruction, 异常号为 2。\n2 CLINT/PLIC riscv/riscv-plic-spec: PLIC Specification (github.com)\n","date":"2024-10-17T10:03:29+08:00","permalink":"https://zcxggmu.github.io/p/riscv-trap-handle/","title":"Riscv Trap Handle"},{"content":"0 资料 SVE spec 3.1 Exception model fpsimd.c - arch/arm64/kvm/fpsimd.c - Linux source code (v6.7-rc6) - Bootlin https://elixir.bootlin.com/linux/v6.7-rc7/C/ident/kvm_hyp_handle_fpsimd 1 ARMv8 SVE异常模型 1.1 ARMv8的硬件虚拟化支持 armv8在虚拟化扩展中提供了敏感指令和异常的捕获功能，可以通过配置hypervisor控制寄存器 hcr_el2 实现，它将vm执行的敏感指令或异常路由到EL2中，hypervisor捕获到相关异常后可为其模拟相关功能。其中 hcr_el2 寄存器的定义如下：\n其中：TGE位只有在支持VHE的架构下才有效。当配置该位后所有需要路由到EL1的异常都会被路由到EL2。因此只要设置该位后，不再设置以上这些位，其对应的异常也会被路由到EL2下。\n关于VHE特性支持 VHE是由HCR_EL2中的两个bits决定的。可以将这两个bits的功能总结如下：\nE2H：控制VHE是否使能（用于访问重定向） TGE：VHE使能后，控制EL0是host还是guest (用于异常重定向) 通过以上扩展以后，前面的问题看来都已经完美解决了。但是我们再回忆一下type 2 hypervisor的原理，这种方案中hypervisor只是host os的一个组件，由于host os本身就是普通的操作系统（如linux），而其通常被设计为运行在EL1异常等级。\n若将其运行在EL2下，则需要在代码层面做较多改动，如在linux中使用vbar_el1访问异常向量表基地址寄存器，此时需要修改为vbar_el2。更麻烦的是el1下支持两个页表基地址寄存器ttbr0_el1和ttbr1_el1，而在armv8.0的el2异常等级中只包含一个ttbr0_el2寄存器。\n因此为了不修改host os本身的代码，最初type2 hypervisor方案中host os被设计为运行在el1下，而hypervisor作为它的一个模块运行在EL2下，其架构如下：\n此时由于hypervisor和host os位于不同的异常等级，因此它们之间的调用都需要通过异常完成，该流程需要执行上下文的保存和恢复，因此会影响hypervisor的效率。为了解决该问题，arm在armv8.1架构之后增加了vhe特性，从而使得host os不经改动就能运行于EL2中。\nvhe主要是为了支持将host os运行在EL2中，以提高hypervisor和虚拟机之间的切换代价。\n由于os内核（如linux）被设计为访问EL1异常等级下的系统寄存器，因此为了在不修改代码的情况下使其实际访问EL2寄存器，vhe实现了**寄存器重定向功能。**即在使能vhe之后，所有EL1寄存器的访问操作都被硬件转换为对相应EL2寄存器的访问，如OS访问ttbr0_el1寄存器，则会按下图方式被重定向到ttbr0_el2：\n当然这里还有一个问题，hypervisor实际上还是有访问实际el1寄存器的需求，因此vhe也对其做了扩展，当访问实际el1寄存器时，则需要使用新的特殊指令。如需要访问实际的ttbr0_el1时，则可通过下图所示访问ttbr0_el12的方式实现：\n当支持vhe且hcr.tge被设置后，不管hcr_el2.imo/fmo/amo是否被设置，所有EL0和EL1的异常都会被路由到EL2中。有了以上扩展之后，host os就可以不经任何修改，而像运行在EL1中一样运行于EL2中，此时系统架构变为如下形式：\nSVE在EL3、EL2和EL1级别上添加了分层陷阱和使能控制。这些控制是通过以下系统寄存器字段来实现的：\nCPTR_EL3.EZ CPTR_EL2.TZ, when HCR_EL2.E2H == 0 CPTR_EL2.ZEN, when HCR_EL2.E2H == 1 CPACR_EL1.ZEN 其中，guest os运行在EL1，hypervisor运行在EL2，即 CPACR_EL1 控制了从 guest user/EL0 到 guest os/EL1 的trap，而 CPTR 控制了从 guest os/EL1 到 hypervisor/EL2 的trap。\n1.2 CPACR_EL1 作用：控制跟踪、SVE、高级SIMD和浮点功能的访问。\n当在当前安全状态下实施并启用EL2，并且HCR_EL2.{E2H, TGE} == {1, 1}时，该寄存器中的字段对EL0和EL1的执行没有影响。在这种情况下，由CPTR_EL2提供的控制被使用。\nFPEN 当执行状态为EL1和EL0的指令访问高级SIMD和浮点寄存器时，会被陷阱捕获。如果EL2被实现并在当前安全状态中启用，且HCR_EL2.TGE为1，则会报告为ESR_ELx.EC值0x00到EL2。如果EL2未启用，则报告为ESR_ELx.EC值0x07到EL1。\n在AArch64状态下，对FPCR、FPSR以及任何SIMD和浮点寄存器V0-V31的访问，包括它们作为D0-D31寄存器或S0-S31寄存器的视图； 将SVE指令在EL1和EL0的执行转移到EL1，或在当前安全状态下EL2已实现且启用时转移到EL2，此时HCR_EL2.TGE为1。异常使用ESR_ELx.EC值0x07进行报告。\n由于CPACR_EL1.ZEN引发的陷阱具有优先权，优先于由CPACR_EL1.FPEN引发的陷阱。\nZEN 将SVE指令和直接访问ZCR_EL1系统寄存器的指令在EL1和EL0执行时陷入EL1，或在当前安全状态下实现和启用EL2时陷入EL2，且HCR_EL2.TGE为1。异常使用ESR_ELx.EC值0x19进行报告。由于CPACR_EL1.ZEN引发的陷阱优先于由CPACR_EL1.FPEN引发的陷阱。\nMRS/MSR CPACR_EL1 访问重定向。\n1.3 CPTR_EL2 作用：控制将对CPACR、CPACR_EL1、跟踪、活动监视器、SVE以及高级SIMD和浮点功能的访问陷入到EL2。\n在当前安全状态下，当EL2被启用时，对访问高级SIMD和浮点寄存器的指令在EL2、EL1和EL0的执行陷入到EL2。异常情况使用ESR_ELx.EC值0x07进行报告。\n在当前安全状态下，将SVE指令在EL2、EL1和EL0级别的执行陷入到EL2。异常使用ESR_ELx.EC值0x07进行报告。\n由于CPTR_EL2.ZEN导致的陷阱优先于由CPTR_EL2.FPEN导致的陷阱：\n疑问：CPTR_EL2.FPEN/ZEN = 0b01，这种配置应用于什么场景？\n1 2 3 4 5 #define CPACR_EL1_FPEN_EL1EN\t(BIT(20)) /* enable EL1 access */\t//0b01 #define CPACR_EL1_FPEN_EL0EN\t(BIT(21)) /* enable EL0 access, if EL1EN set */ //0b11 #define CPACR_EL1_ZEN_EL1EN\t(BIT(16)) /* enable EL1 access */ #define CPACR_EL1_ZEN_EL0EN\t(BIT(17)) /* enable EL0 access, if EL1EN set */ 允许EL1执行，但禁止EL0执行\n2 KVM: FPSIMD/SVE虚拟化支持 2.1 数据结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 struct kvm_vcpu_arch { struct kvm_cpu_context ctxt; /* * Guest floating point state * * The architecture has two main floating point extensions, * the original FPSIMD and SVE. These have overlapping * register views, with the FPSIMD V registers occupying the * low 128 bits of the SVE Z registers. When the core * floating point code saves the register state of a task it * records which view it saved in fp_type. */ void *sve_state; enum fp_type fp_type; unsigned int sve_max_vl; u64 svcr; /* Values of trap registers for the guest. */ u64 hcr_el2; u64 cptr_el2; /* Ownership of the FP regs */ enum { FP_STATE_FREE, FP_STATE_HOST_OWNED, FP_STATE_GUEST_OWNED, } fp_state; struct user_fpsimd_state *host_fpsimd_state;\t/* hyp VA */ // host_fpsimd_state } struct kvm_cpu_context { struct user_pt_regs regs;\t/* sp = sp_el0 */ struct user_fpsimd_state fp_regs; // guest_fpsimd_state u64 sys_regs[NR_SYS_REGS]; struct kvm_vcpu *__hyp_running_vcpu; }; struct cpu_fp_state { struct user_fpsimd_state *st; void *sve_state; void *sme_state; u64 *svcr; unsigned int sve_vl; unsigned int sme_vl; enum fp_type *fp_type; enum fp_type to_save; }; 2.2 框架分析 相关函数：\nfpsimd.c - arch/arm64/kvm/fpsimd.c - Linux source code (v6.7-rc6) - Bootlin 1 2 3 4 5 6 7 8 9 // arch/arm64/kvm/fpsimd.c /* Guest/host FPSIMD coordination helpers */ int kvm_arch_vcpu_run_map_fp(struct kvm_vcpu *vcpu); void kvm_arch_vcpu_load_fp(struct kvm_vcpu *vcpu); void kvm_arch_vcpu_ctxflush_fp(struct kvm_vcpu *vcpu); void kvm_arch_vcpu_ctxsync_fp(struct kvm_vcpu *vcpu); void kvm_arch_vcpu_put_fp(struct kvm_vcpu *vcpu); void kvm_vcpu_unshare_task_fp(struct kvm_vcpu *vcpu); 代码流程：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 // vcpu首次运行和再次进入循环 kvm_arch_vcpu_ioctl_run +-\u0026gt; vcpu_load -\u0026gt; kvm_vcpu_arch_load +-\u0026gt; kvm_arch_vcpu_load_fp +-\u0026gt; //goto LOAD_FP\t+-\u0026gt; while(ret \u0026gt; 0) //kvm第一层循环 +-\u0026gt; kvm_arch_vcpu_ctxflush_fp +-\u0026gt; //goto FLUSH_FP /* Enter the guest... */ +-\u0026gt; kvm_arm_vcpu_enter_exit // arch/arm64/kvm/hyp/vhe/switch.c +-\u0026gt; __kvm_vcpu_run_vhe +-\u0026gt; __activate_traps +-\u0026gt; //AC_TRAPS +-\u0026gt; do { //kvm第二层循环 /* Jump in the fire! */ exit_code = __guest_enter(vcpu); /* And we\u0026#39;re baaack! */ } while (fixup_guest_exit(vcpu, \u0026amp;exit_code)); +-\u0026gt; __guest_enter +-\u0026gt; fixup_guest_exit +-\u0026gt; kvm_hyp_handle_exit /* [ESR_ELx_EC_SVE]\t= kvm_hyp_handle_fpsimd, [ESR_ELx_EC_FP_ASIMD] = kvm_hyp_handle_fpsimd, */ +-\u0026gt; kvm_hyp_handle_fpsimd //guest触发fpsimd/sve_trap +-\u0026gt; __deactivate_traps +-\u0026gt; //DEAC_TRAPS +-\u0026gt; if (vcpu-\u0026gt;arch.fp_state == FP_STATE_GUEST_OWNED) __fpsimd_save_fpexc32(vcpu); /* Back from guest... */ +-\u0026gt; kvm_arch_vcpu_ctxsync_fp +-\u0026gt; //SYNC_FP +-\u0026gt; ret = handle_exit(vcpu, ret); +-\u0026gt; vcpu_put -\u0026gt; kvm_vcpu_arch_put +-\u0026gt; kvm_arch_vcpu_put_fp +-\u0026gt; //PUT_FP //LOAD_FP /* * Prepare vcpu for saving the host\u0026#39;s FPSIMD state and loading the guest\u0026#39;s. * The actual loading is done by the FPSIMD access trap taken to hyp. * * Here, we just set the correct metadata to indicate that the FPSIMD * state in the cpu regs (if any) belongs to current on the host. */ kvm_arch_vcpu_load_fp +-\u0026gt; system_supports_fpsimd() == TRUE +-\u0026gt; fpsimd_kvm_prepare /* * We will check TIF_FOREIGN_FPSTATE just before entering the * guest in kvm_arch_vcpu_ctxflush_fp() and override this to * FP_STATE_FREE if the flag set. */ +-\u0026gt;\tif (test_and_clear_thread_flag(TIF_SVE)) { sve_to_fpsimd(current); current-\u0026gt;thread.fp_type = FP_STATE_FPSIMD; } +-\u0026gt; vcpu-\u0026gt;arch.fp_state = FP_STATE_HOST_OWNED; +-\u0026gt; vcpu_clear_flag(vcpu, HOST_SVE_ENABLED); if (read_sysreg(cpacr_el1) \u0026amp; CPACR_EL1_ZEN_EL0EN) vcpu_set_flag(vcpu, HOST_SVE_ENABLED); +-\u0026gt; //假设不支持SME //FLUSH_FP //如果进入guest前触发中断，TIF_FOREIGN_FPSTATE被清理掉，当前CPU上实时保存的是一份和host/guest无关的fpsimd状态 //注意，该函数执行前需要preempt_disable()/local_irq_disable() /* * Called just before entering the guest once we are no longer preemptable * and interrupts are disabled. If we have managed to run anything using * FP while we were preemptible (such as off the back of an interrupt), * then neither the host nor the guest own the FP hardware (and it was the * responsibility of the code that used FP to save the existing state). */ void kvm_arch_vcpu_ctxflush_fp(struct kvm_vcpu *vcpu) { if (test_thread_flag(TIF_FOREIGN_FPSTATE)) vcpu-\u0026gt;arch.fp_state = FP_STATE_FREE; } //AC_TRAPS/DEAC_TRAPS /* Check whether the FP regs are owned by the guest */ static inline bool guest_owns_fp_regs(struct kvm_vcpu *vcpu) { return vcpu-\u0026gt;arch.fp_state == FP_STATE_GUEST_OWNED; } /* 进入guest前，检查CPU的fp_state的持有者是否为guest: 1) 首次进入fp_state持有者为host或free，禁止guest在EL0/EL1执行fpsimd/sve指令 2) 如果为guest，其支持SVE，则允许guest在EL0/EL1执行fpsimd/sve指令 */ __activate_traps val \u0026amp;= ~(CPACR_EL1_ZEN_EL0EN | CPACR_EL1_ZEN_EL1EN; if (guest_owns_fp_regs(vcpu)) { if (vcpu_has_sve(vcpu)) val |= CPACR_EL1_ZEN_EL0EN | CPACR_EL1_ZEN_EL1EN; } else { val \u0026amp;= ~(CPACR_EL1_FPEN_EL0EN | CPACR_EL1_FPEN_EL1EN); __activate_traps_fpsimd32(vcpu); } __deactivate_traps //SYNC_FP /* * Called just after exiting the guest. If the guest FPSIMD state * was loaded, update the host\u0026#39;s context tracking data mark the CPU * FPSIMD regs as dirty and belonging to vcpu so that they will be * written back if the kernel clobbers them due to kernel-mode NEON * before re-entry into the guest. */ /* 在退出客户机后立即调用。如果加载了客户机的FPSIMD状态，更新主机的上下文跟踪数据， 将CPU的FPSIMD寄存器标记为脏的，并属于虚拟CPU， 这样如果内核在重新进入客户机之前由于内核模式NEON而破坏它们，它们将被写回。 */ void kvm_arch_vcpu_ctxsync_fp(struct kvm_vcpu *vcpu) { struct cpu_fp_state fp_state; WARN_ON_ONCE(!irqs_disabled()); //如果guest作为CPU fpsimd/sve寄存器组持有者，将vcpu-\u0026gt;arch的相关内容全部拉取下来，保存至本地cpu_fp_state if (vcpu-\u0026gt;arch.fp_state == FP_STATE_GUEST_OWNED) { /* * Currently we do not support SME guests so SVCR is * always 0 and we just need a variable to point to. */ fp_state.st = \u0026amp;vcpu-\u0026gt;arch.ctxt.fp_regs; fp_state.sve_state = vcpu-\u0026gt;arch.sve_state; fp_state.sve_vl = vcpu-\u0026gt;arch.sve_max_vl; fp_state.sme_state = NULL; fp_state.svcr = \u0026amp;vcpu-\u0026gt;arch.svcr; fp_state.fp_type = \u0026amp;vcpu-\u0026gt;arch.fp_type; if (vcpu_has_sve(vcpu)) fp_state.to_save = FP_STATE_SVE; else fp_state.to_save = FP_STATE_FPSIMD; //用本地cpu_fp_state刷新per-cpu变量`fpsimd_last_state`，让其指向最近的上下文 (guest) fpsimd_bind_state_to_cpu(\u0026amp;fp_state); //清除TIF_FOREIGN_FPSTATE，该标识用于控制vcpu线程的两种状态(host/guest) clear_thread_flag(TIF_FOREIGN_FPSTATE); } } //PUT_FP /* * Write back the vcpu FPSIMD regs if they are dirty, and invalidate the * cpu FPSIMD regs so that they can\u0026#39;t be spuriously reused if this vcpu * disappears and another task or vcpu appears that recycles the same * struct fpsimd_state. */ /* 如果虚拟中央处理器（vCPU）的浮点/ SIMD 寄存器（FPSIMD regs）已被修改，将其内容写回， 并使该 CPU 的 FPSIMD 寄存器无效。 这样做的目的是防止在该 vCPU 消失后，出现另一个任务或 vCPU 并且重新使用相同的 fpsimd_state 结构时发生误用 */ void kvm_arch_vcpu_put_fp(struct kvm_vcpu *vcpu) { if (vcpu-\u0026gt;arch.fp_state == FP_STATE_GUEST_OWNED) { if (vcpu_has_sve(vcpu)) { __vcpu_sys_reg(vcpu, ZCR_EL1) = read_sysreg_el1(SYS_ZCR); /* Restore the VL that was saved when bound to the CPU */ if (!has_vhe()) sve_cond_update_zcr_vq(vcpu_sve_max_vq(vcpu) - 1, SYS_ZCR_EL1); } fpsimd_save_and_flush_cpu_state(); //??? why not defer to userland } else if (has_vhe() \u0026amp;\u0026amp; system_supports_sve()) { /* * The FPSIMD/SVE state in the CPU has not been touched, and we * have SVE (and VHE): CPACR_EL1 (alias CPTR_EL2) has been * reset by kvm_reset_cptr_el2() in the Hyp code, disabling SVE * for EL0. To avoid spurious traps, restore the trap state * seen by kvm_arch_vcpu_load_fp(): */ if (vcpu_get_flag(vcpu, HOST_SVE_ENABLED)) sysreg_clear_set(CPACR_EL1, 0, CPACR_EL1_ZEN_EL0EN); else sysreg_clear_set(CPACR_EL1, CPACR_EL1_ZEN_EL0EN, 0); } } // guest首次fpsimd/sve_trap陷入hyp /* * We trap the first access to the FP/SIMD to save the host context and * restore the guest context lazily. * If FP/SIMD is not implemented, handle the trap and inject an undefined * instruction exception to the guest. Similarly for trapped SVE accesses. */ static bool kvm_hyp_handle_fpsimd(struct kvm_vcpu *vcpu, u64 *exit_code) { bool sve_guest; u8 esr_ec; u64 reg; if (!system_supports_fpsimd()) return false; sve_guest = vcpu_has_sve(vcpu); esr_ec = kvm_vcpu_trap_get_class(vcpu); /* Only handle traps the vCPU can support here: */ switch (esr_ec) { case ESR_ELx_EC_FP_ASIMD: break; case ESR_ELx_EC_SVE: if (!sve_guest) return false; break; default: return false; } /* Valid trap. Switch the context: */ /* First disable enough traps to allow us to update the registers */ if (has_vhe() || has_hvhe()) { reg = CPACR_EL1_FPEN_EL0EN | CPACR_EL1_FPEN_EL1EN; if (sve_guest) reg |= CPACR_EL1_ZEN_EL0EN | CPACR_EL1_ZEN_EL1EN; sysreg_clear_set(cpacr_el1, 0, reg); } else { reg = CPTR_EL2_TFP; if (sve_guest) reg |= CPTR_EL2_TZ; sysreg_clear_set(cptr_el2, reg, 0); } isb(); /* Write out the host state if it\u0026#39;s in the registers */ // host_sve_sate 为什么不保存？ if (vcpu-\u0026gt;arch.fp_state == FP_STATE_HOST_OWNED) __fpsimd_save_state(vcpu-\u0026gt;arch.host_fpsimd_state); /* Restore the guest state */ if (sve_guest) __hyp_sve_restore_guest(vcpu); else __fpsimd_restore_state(\u0026amp;vcpu-\u0026gt;arch.ctxt.fp_regs); /* Skip restoring fpexc32 for AArch64 guests */ if (!(read_sysreg(hcr_el2) \u0026amp; HCR_RW)) write_sysreg(__vcpu_sys_reg(vcpu, FPEXC32_EL2), fpexc32_el2); vcpu-\u0026gt;arch.fp_state = FP_STATE_GUEST_OWNED; return true; } 关于TIF_FOREIGN_FPSTATE在哪些场景下设置、取消、更新？ 该标识的作用是：判断当前pCPU上加载的是否为被调度线程的最近用户态上下文，其值为FALSE时可以减少一次上下文的恢复工作。该标识的引入可以将调度执行流中的SIMD恢复工作解耦出来并推迟到返回用户态之前。\n一些情况会影响TIF_FOREIGN_FPSTATE，除了该标识的合法设置方式外（通过两个变量以判断同步情况）。因为除了用户态，内核态也可能使用FPSIMD能力，此时硬件FPSIMD状态的持有者就不再是用户了，用户应该即时保存这部分状态，同时设置TIF_FOREIGN_FPSTATE。\n内核态中断/抢占场景下，FPSIMD状态应该如何维护？ 调用链：preempt_enable =\u0026gt; _schedule =\u0026gt; context_switch =\u0026gt; __switch_to =\u0026gt; fpsimd_thread_switch\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 void fpsimd_thread_switch(struct task_struct *next) { bool wrong_task, wrong_cpu; if (!system_supports_fpsimd()) return; __get_cpu_fpsimd_context(); /* Save unsaved fpsimd state, if any: */ fpsimd_save(); /* * Fix up TIF_FOREIGN_FPSTATE to correctly describe next\u0026#39;s * state. For kernel threads, FPSIMD registers are never loaded * and wrong_task and wrong_cpu will always be true. */ wrong_task = __this_cpu_read(fpsimd_last_state.st) != \u0026amp;next-\u0026gt;thread.uw.fpsimd_state; wrong_cpu = next-\u0026gt;thread.fpsimd_cpu != smp_processor_id(); update_tsk_thread_flag(next, TIF_FOREIGN_FPSTATE, wrong_task || wrong_cpu); __put_cpu_fpsimd_context(); } vcpu首次进入guest，guest首次执行FPSIMD指令陷入hypervisor，保存恢复上下文后再次返回guest\nguest已持有FPSIMD状态，再次陷入hypervisor，处理后返回guest执行：（陷入原因）\nvcpu线程退出，hypervisor处理后直接返回\n内层循环 外层循环 vcpu线程退出，需要在用户态处理\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 kvm_arch_vcpu_put_fp +-\u0026gt; fpsimd_save_and_flush_cpu_state +-\u0026gt; fpsimd_save() +-\u0026gt; fpsimd_flush_cpu_state() /* * Ensure FPSIMD/SVE storage in memory for the loaded context is up to * date with respect to the CPU registers. Note carefully that the * current context is the context last bound to the CPU stored in * last, if KVM is involved this may be the guest VM context rather * than the host thread for the VM pointed to by current. This means * that we must always reference the state storage via last rather * than via current, if we are saving KVM state then it will have * ensured that the type of registers to save is set in last-\u0026gt;to_save. */ static void fpsimd_save(void); 这段话大致意思是要确保内存中加载的上下文中的FPSIMD/SVE存储与CPU寄存器的状态是最新的。需要注意的是，当前上下文是最后绑定到CPU的上下文，存储在last中。如果涉及到KVM，这可能是指与current指向的VM主线程相关的guest VM上下文，而不是主机线程的上下文。这意味着我们必须始终通过last引用状态存储，而不是通过current引用。如果正在保存KVM状态，那么last-\u0026gt;to_save中将确保设置要保存的寄存器类型。 /* * Invalidate any task\u0026#39;s FPSIMD state that is present on this cpu. * The FPSIMD context should be acquired with get_cpu_fpsimd_context() * before calling this function. */ static void fpsimd_flush_cpu_state(void) { WARN_ON(!system_supports_fpsimd()); __this_cpu_write(fpsimd_last_state.st, NULL); /* * Leaving streaming mode enabled will cause issues for any kernel * NEON and leaving streaming mode or ZA enabled may increase power * consumption. */ if (system_supports_sme()) sme_smstop(); set_thread_flag(TIF_FOREIGN_FPSTATE); } do_notify_resume +-\u0026gt; fpsimd_restore_current_state /* * Load the userland FPSIMD state of \u0026#39;current\u0026#39; from memory, but only if the * FPSIMD state already held in the registers is /not/ the most recent FPSIMD * state of \u0026#39;current\u0026#39;. This is called when we are preparing to return to * userspace to ensure that userspace sees a good register state. */ void fpsimd_restore_current_state(void) { /* * TIF_FOREIGN_FPSTATE is set on the init task and copied by * arch_dup_task_struct() regardless of whether FP/SIMD is detected. * Thus user threads can have this set even when FP/SIMD hasn\u0026#39;t been * detected. * * When FP/SIMD is detected, begin_new_exec() will set * TIF_FOREIGN_FPSTATE via flush_thread() -\u0026gt; fpsimd_flush_thread(), * and fpsimd_thread_switch() will set TIF_FOREIGN_FPSTATE when * switching tasks. We detect FP/SIMD before we exec the first user * process, ensuring this has TIF_FOREIGN_FPSTATE set and * do_notify_resume() will call fpsimd_restore_current_state() to * install the user FP/SIMD context. * * When FP/SIMD is not detected, nothing else will clear or set * TIF_FOREIGN_FPSTATE prior to the first return to userspace, and * we must clear TIF_FOREIGN_FPSTATE to avoid do_notify_resume() * looping forever calling fpsimd_restore_current_state(). */ if (!system_supports_fpsimd()) { clear_thread_flag(TIF_FOREIGN_FPSTATE); return; } get_cpu_fpsimd_context(); if (test_and_clear_thread_flag(TIF_FOREIGN_FPSTATE)) { task_fpsimd_load(); fpsimd_bind_task_to_cpu(); } put_cpu_fpsimd_context(); } vcpu线程被host调度\n//TODO\n","date":"2024-10-17T10:02:30+08:00","permalink":"https://zcxggmu.github.io/p/arm-sve-kvm-support/","title":"Arm Sve Kvm Support"},{"content":"0 资料 riscv_vector riscv-v-spec/v-spec.adoc at master · riscv/riscv-v-spec (github.com) [PATCH v10 00/16] riscv: Add vector ISA support - Greentime Hu (kernel.org) ARM与RISC-V的向量扩展比较 - 知乎 (zhihu.com) riscv-v-spec-1.0（矢量指令） 学习理解（1-5 \u0026amp; 18 segment）-CSDN博客 armv8_sve Scalable Vector Extension support for AArch64 Linux — The Linux Kernel documentation [PATCH v5 00/30] ARM Scalable Vector Extension (SVE) - Dave Martin (kernel.org) [PATCH v7 00/27] KVM: arm64: SVE guest support [LWN.net] ARM SVE特性学习 - 知乎 (zhihu.com) Linux 内核使用浮点问题_内核开启硬浮点-CSDN博客 基于armv8的kvm实现分析（四）cpu虚拟化 - 知乎 (zhihu.com) 1 ARM与RISC-V向量扩展对比 ARM SVE和RISC-V Vector特性引入的目的都是针对向量计算，实现指令级别的SIMD高性能运算。本文并不会分析两者在向量运算上的一些异同，比如：执行的向量指令格式有什么区别、提供的寄存器组的区别等，因为这些都是用户态应用程序或者标准库、编译器关注的事情，本文只会讨论一件事：目前Linux Kernel在多核系统上支持这两种特性上有什么区别？\n相应规范如下：\nriscv riscv-v-spec/v-spec.adoc at master · riscv/riscv-v-spec (github.com) armv8 ARM Architecture Reference Manual Supplement - The Scalable Vector Extension (SVE), for ARMv8-A 我们只关注部分控制状态寄存器的作用即可。\n1.1 RISC-V \u0026ldquo;V\u0026rdquo; Vector Extension Vector Extension Programmer’s Model 向量扩展向基标量RISC-V ISA中增加了32个向量寄存器(v0-v31)以及7个无特权的CSRS（控制和状态寄存器），分别是vstart、vxsat、vxrm、vcsr、vl、vtype、vlenb。向量寄存器的位宽为固定的VLEN宽度。\nriscv 架构规定了一些只在机器模式下支持的寄存器，机器模式是riscv中硬件线程执行时的最高权限模式。机器模式对内存，I/O和一些对于启动和配置系统来说必要的底层功能有着完全的使用权。通常用户写的程序都在用户模式执行，用户态具有最低级别的权限。当用户程序需要使用一些底层硬件设备或者执行出现了异常又或者外围设备对正在执行的处理器发起了中断请求，那么cpu就会由用户态切换至内核态，也就是切换到机器模式下，将cpu的控制权，转交给内核程序。\nriscv把程序执行时出现的异常，或者外部中断统称为陷阱（陷阱其实很好理解，因为不管是异常还是中断，cpu都会由用户态陷入内核态，这个陷入内核的过程就可以理解成踩入了陷阱里，要经过一些其他的操作，才能爬出陷阱，恢复正常行走）。当遇到陷阱时，首先要将当前的pc保存下来，方便之后进行恢复。然后清空异常指令之前的流水线。接下来根据对应的陷阱类型，切换到对应的程序入口，开始执行内核程序（ 内核程序也是由一条条指令组成的，同样需要在流水线上执行）。等内核程序执行完成后，在重新把cpu的控制权转交给用户程序，从之前保存的pc指针开始重新取指，执行。\n在重新回到riscv规定了一些只支持机器模式相关的寄存器的话题。mstatus 寄存器是机器模式下的状态寄存器，其中 mstatus[10:9] 是向量上下文状态域。\n一个进程存储在处理器各寄存器中的中间数据叫做进程的上下文，所以进程的切换实质上就是被中止运行进程与待运行进程上下文的切换\nmstatus.vs 域同 mstatus.fs 类似。当 mstatus.vs 域被写成0时，试图执行向量指令或者访问向量寄存器均会引发非法指令异常。当mstatus.vs 域被设置为初始状态或者干净的状态，只要执行了相关的指令改变了vpu（vector processor unit）状态，该域就会被改写成dirty的状态。\nmisa 寄存器用于指示当前处理器的架构特性，该寄存器高两位用于表示当前处理器所支持的架构位数；该寄存器低 26 位用于指示当前处理器所支持的不同模块化指令集。riscv架构文档规定 misa 寄存器可读可写，从而允许处理器可以动态的配置某些特性。misa.v 表示 Vector 扩展指令集域，即使 misa.v 域被清0，mstatus.vs 域也存在。这样设计可以简化 mstatus.vs 的处理。\nmstatus: Vector上下文状态 在mstatus[10:9]中添加了一个向量上下文状态字段VS，并在sstatus[10:9]中进行了镜像。它的定义与浮点上下文状态字段FS类似。\n当mstatus.VS设置为Off时，尝试执行任何向量指令或访问向量CSRs都会引发非法指令异常。\n当mstatus.VS设置为Initial或Clean时，执行任何改变向量状态的指令，包括向量CSRs，都将把mstatus.VS更改为Dirty。实现也可以在向量状态没有改变的情况下随时将mstatus.VS从Initial或Clean更改为Dirty。\n准确设置mstatus.VS是一种优化方法。软件通常会使用VS来减少上下文切换的开销。\n如果mstatus.VS是脏的，那么mstatus.SD为1；否则，mstatus.SD根据现有规范进行设置。\n实现可能具有可写的misa.V字段。类似于对浮点单元的处理方式，即使misa.V被清除，mstatus.VS字段仍然可能存在。\n在misa.V未设置的情况下，允许存在mstatus.VS，可以实现向量仿真，并简化对于具有可写misa.V的系统中mstatus.VS的处理。\nvsstatus: Vector上下文状态 当存在虚拟机扩展时，向vsstatus[10:9]添加一个向量上下文状态字段VS。它的定义类似于浮点上下文状态字段FS。\n当V=1时，vsstatus.VS和mstatus.VS都生效：当任一字段设置为Off时，执行任何向量指令或访问向量CSRs都会引发非法指令异常。 当V=1且vsstatus.VS和mstatus.VS都未设置为Off时，执行任何改变向量状态的指令，包括向量CSRs，都会将mstatus.VS和vsstatus.VS都改为Dirty。即使向量状态没有改变，实现也可以随时将mstatus.VS或vsstatus.VS从Initial或Clean更改为Dirty。 如果vsstatus.VS为Dirty，则vsstatus.SD为1；否则，根据现有规范设置vsstatus.SD。 如果mstatus.VS为Dirty，则mstatus.SD为1；否则，根据现有规范设置mstatus.SD。 对于具有可写的misa.V字段的实现，即使misa.V被清除，vsstatus.VS字段也可能存在。 vstart: Vector Start Index CSR XLEN位宽的可读写vstart CSR指定了向量指令执行的第一个元素的索引，如第Prestart、Active、Inactive、Body和Tail元素定义部分所述。\n通常情况下，vstart只在向量指令发生陷阱时由硬件进行写入，vstart的值表示陷阱发生的元素（可以是同步异常或异步中断），并且在可恢复陷阱处理后应该从该元素处恢复执行。\n所有的向量指令都被定义为从vstart CSR所给出的元素编号开始执行，不会改变目标向量中较早的元素，并且在执行结束时将vstart CSR重置为零。\n关于元素的定义：\n向量指令执行期间的元素索引，可以分成四个没有交集的子集。\nprestart elememts：元素索引小于vstart寄存器中的初始值，对应的元素集合。该元素集合不产生异常，也不会更新目的向量寄存器； active elements：表示在向量指令执行过程中，在向量长度设置范围内，且掩码有效的元素集合。该元素集合可以产生异常并且也可以更新目的向量寄存器； inactive elements：表示在向量指令执行过程中，在向量长度设置范围内，但是掩码无效的严肃集合。该元素集合不产生异常，vma=0的设置下，不更新目的向量寄存器；vma=1的设置下，被掩码掩蔽的元素将被写1； tail elemememts：表示在向量执行过程中，超过了向量设置的长度。该元素集合不产生异常，vta=0的设置下，不更新目的向量寄存器；vta=1的设置下，tail元素将被写1。当分组因子LMUL\u0026lt;1时，超过LMAX的元素也纳入tail元素集合中。 除上述定义的子集外。inactive和active的集合还可以统称body element。该集合在prestart elements之后，在tail elements之前。\nvstart为可读可写寄存器，该寄存器规定了一条向量指令中第一个被执行的元素的索引。\nvstart寄存器通常在向量指令执行的过程中产生了陷阱被写入。该寄存器记录了进入陷阱时向量指令操作元素的索引，以便跳出陷阱之后能够继续执行剩下的元素。 所有向量指令都根据vstart中指定的元素索引开始执行。执行的过程保证该索引之前的目的寄存器的元素不被干扰，在执行的末尾，将vstart寄存器的值复位到0。当向量指令产生非法指令异常时，vstart寄存器将不会被改写。所以vstart寄存器被改写，只能是程序执行时出现了可恢复的同步异常，或者外部产生中断的情况。\n**思考：**假如不能通过vstart存储异常时的元素索引，那么在执行向量指令过程中发生的可恢复异常，必须要等到这条向量指令执行完，才能进入异常处理程序。这就要求向量指令必须是原子的，增加了控制复杂度，并且对于一些长延时的指令，比如load，将会导致响应中断的时间特别的长。\n若vstart索引值大于vl的值，说明vstart指向的元素索引已经超过了当前所有元素的范围，该指令不会执行，并且同时会把vstart寄存器复位到0。 vstart可写的bit位，根据VLMAX确定，在vl部分已经描述过。\nException Handling 在矢量指令的陷阱中（由同步异常或异步中断引起），现有的 *epc CSR会被写入指向陷阱矢量指令的指针，而 vstart CSR则包含陷阱发生时的元素索引。\n我们选择添加vstart CSR以允许恢复部分执行的向量指令，以减少中断延迟并简化前进保证。这类似于IBM 3090向量设施中的方案。为了确保没有vstart CSR的前进进展，实现必须保证整个向量指令始终可以原子地完成而不生成陷阱。在存在跨步或分散/聚集操作和需求分页虚拟内存的情况下，这特别难以保证。\n异常和中断从广义上来看，都叫做异常。RISC-V 架构规定，在处理器的程序执行过程中，一旦遇到异常发生，则终止当前的程序流，处理器被强行跳转到一个新的 PC 地址。该过程在 RISC-V 的架构中定义为“陷阱(trap)”， 字面含义为“跳入陷阱”，更加准确的意译为“进入异常”。在一条向量指令执行的过程中遇到了陷阱，需要将当前指令pc保存在 *epc 中，并且需要记录当前遇到异常时的元素索引到 vstart，以便退出异常服务程序后能恢复原先执行的程序。\n如果发生异常时，只记录指令的pc指针，没有用vstart记录发生异常时的元素索引，那么需要保证该指令的执行是原子的，这加大了设计控制难度。并且向量指令设计为原子的，在有些long latency的指令执行过程中，会导致中断的响应非常缓慢。(思考假如指令不是原子的，那么出现异常直接被打断，又没有保存被打断时的索引，那么下一次从这一条指令重新开始执行会有什么问题？)\n中断通常是由外设（中断源）产生的，而异常通常是由程序执行过程中遇到的异常。\n因此中断是一种外因，通常不能精确定位到某条指令引起，因为外设发起中断的时间是偶然的，程序执行过程中遇到中断，任何指令都可能碰到，这些倒霉的指令只是一个背锅侠，因此称这种中断为异步异常； 异常的产生通常是内因，比如某条指令解码的时候出错，或者执行的时候进行了除 0 的操作，这些异常都可以被精确的定位到某一条指令。并且同样的程序执行n遍，都是能复现的。因此称这种异常为同步异常。 对于异步异常，还可以被细分为**精确异步异常和非精确异步异常。**精确和不精确的区分主要在于进入异常的程序能否精确区分到某条指令的边界\n比如某条指令之前的指令都执行完了，而该条指令之后的指令都没有执行，外部中断就是一种精确异步异常。 而非精确异步异常是指当前处理器的状态可能是一个很模糊的状态，没法明确的根据一条指令划界。举个例子，比如写存指令，访问存储器需要一定的时间，但是为了流水线高效率的执行，通常写存指令发出去，没等到写的响应有没有正确响应，后续的指令就在继续执行。那么有可能出现等访问完成，发现出现了异常，此时已经过去了很多条指令了，难以精准确的定位到某一条指令。 异常可能发生在处理器流水线的各个阶段，为了保证处理器按照指令先后顺序处理异常，需要将异常的处理放在 commit 阶段。\nPrecise vector traps 我们假设大多数具有需求分页的监管模式环境都需要精确的向量陷阱。\n精确的向量陷阱具有如下要求：\n所有比陷阱指令旧的指令都已经完成了 commit 过程； 所有比陷阱指令新的指令都应该被flush掉，不允许新指令更改处理器的状态； 陷阱指令中，所有小于vstart元素索引的元素操作都完成了 commit； 陷阱指令中，所有大于等于vstart元素索引的元素都不会被执行。但是如果操作重新开始该指令，可以将处理器恢复到正确的状态，那么该条约束可以放松； 举个例子，对于具有幂等性的存储区域进行操作，就可以允许发生trap对应的索引之后的元素改变存储器的状态，而不具有幂等性的存储区域就不能允许大于等于vsart索引的元素更新存储器状态。\n幂等性是指多次执行一个操作与执行一次该操作得到的结果相同，该操作就可以说是幂等的。\n跳出异常服务程序时，需要从vstart记录的元素索引开始。这是因为比如有些指令源寄存器与目的寄存器有交叠，那么可能vstart之前的源寄存器内容已经被目的寄存器覆盖了，那么重新执行这部分元素，将会得到错误的结果。\nImprecise vector traps 非精确向量陷阱，spec中写的是比 *epc 新的指令可能已经commit，比 *epc 老的指令有可能还在执行。对这个地方我是有疑问的，因为即使是超标量流水线，也会经历一个inorder-outoforder-inorder的过程，也就是真正commit的时候，指令肯定是会按照程序指令顺序，依次改变处理器状态，怎么会出现比 *epc 老的指令还在执行。鉴于这部分理解的有分叉，我就不多描述了，希望能和大家一起讨论清楚。\n//TODO: 1.2 ARM Scalable Vector Extension 2 RISC-V Vector内核支持 [PATCH v10 00/16] riscv: Add vector ISA support - Greentime Hu (kernel.org)\n[v6, 00/10] riscv: support kernel-mode Vector - Andy Chiu\n该补丁集是基于向量1.0规范实现的，以在riscv Linux内核中添加向量支持。对于这些实现，有一些假设。\n我们假设系统中的所有harts都具有相同的ISA。 我们在某些方面使用类似浮点单元(FPU)的向量，而不是使用特定IP的向量。 默认情况下，在内核空间中禁用向量，除非内核使用内核模式的向量kernel_rvv_begin()/kernel_rvv_end()。 我们通过检测\u0026quot;riscv,isa\u0026quot;来确定是否支持向量。 我们在struct thread_struct中定义了一个名为 __riscv_v_state 的新结构，用于保存/恢复与向量相关的寄存器。它用于内核空间和用户空间。\n在内核空间中，__riscv_v_state 中的datap指针将被分配用于保存向量寄存器； 在用户空间中： 在用户空间的信号处理程序中，datap将指向 __riscv_v_state 数据结构的地址，以在堆栈中保存向量寄存器。我们还为未来的扩展在sigcontext中创建了一个 reserved[] 数组； 在ptrace中，数据将被放入ubuf中，我们使用 riscv_vr_get()/riscv_vr_set() 从中获取或设置 __riscv_v_state 数据结构，datap指针将被清零，并且向量寄存器将被复制到riscv_v_state结构之后的地址中。 该补丁集还添加了对内核模式向量的支持，使用向量ISA实现了内核XOR，并包括了几个错误修复和代码优化。\n该补丁集已重新基于v5.18-rc6，并通过同时运行多个向量程序进行了测试。它还可以在信号处理程序中获取正确的ucontext_t，并在sigreturn后恢复正确的上下文。它还经过了使用ptrace()系统调用来使用PTRACE_GETREGSET/PTRACE_SETREGSET获取/设置向量寄存器的测试。\n待办事项：\n优化start_thread()中的 __riscv_v_state 分配； 通过延迟保存/恢复来优化向量上下文切换函数； 添加AMP支持，以支持具有不同ISA的harts。 2.1 Lazy save/restore [PATCH -next v21 00/27] riscv: Add vector ISA support - Andy Chiu (kernel.org)\nnon-virtualization 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 static inline void riscv_v_vstate_off(struct pt_regs *regs) { regs-\u0026gt;status = (regs-\u0026gt;status \u0026amp; ~SR_VS) | SR_VS_OFF; } static struct linux_binfmt elf_format = { .module\t= THIS_MODULE, .load_binary\t= load_elf_binary, .load_shlib\t= load_elf_library, #ifdef CONFIG_COREDUMP .core_dump\t= elf_core_dump, .min_coredump\t= ELF_EXEC_PAGESIZE, #endif }; /* kernel_entry* start_kernel setup_arch* trap_init* mm_init mem_init* init_IRQ* time_init* rest_init kernel_thread kernel_thread cpu_startup_entry */ do_execve +-\u0026gt; load_elf_binary +-\u0026gt;begin_new_exec +-\u0026gt; flush_thread +-\u0026gt; riscv_v_vstate_off 内核中实际执行execv()或execve()系统调用的程序是 do_execve()，这个函数先打开目标映像文件，并从目标文件的头部（第一个字节开始）读入若干（当前Linux内核中是128）字节（实际上就是填充ELF文件头），然后调用另一个函数 search_binary_handler()，在此函数里面，它会搜索我们上面提到的Linux支持的可执行文件类型队列，让各种可执行程序的处理程序前来认领和处理。如果类型匹配，则调用 load_binary 函数指针所指向的处理函数来处理目标映像文件。在ELF文件格式中，处理函数是load_elf_binary 函数。\nvector trap处理 [PATCH -next v21 11/27] riscv: Allocate user\u0026rsquo;s vector context in the first-use trap - Andy Chiu (kernel.org)\n向量单元对于所有用户进程默认情况下是禁用的。因此，当一个进程第一次使用向量单元时，它会触发一个陷阱（非法指令），将控制权转移到内核。在接收到陷阱后，内核为该特定用户进程分配一个向量上下文，并开始管理该上下文。从那时起，内核将处理用户进程的向量上下文，使其能够使用向量单元进行向量操作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 +asmlinkage __visible __trap_section void do_trap_insn_illegal(struct pt_regs *regs) +{ +\tif (user_mode(regs)) { +\tirqentry_enter_from_user_mode(regs); + +\tlocal_irq_enable(); + +\tif (!riscv_v_first_use_handler(regs)) +\tdo_trap_error(regs, SIGILL, ILL_ILLOPC, regs-\u0026gt;epc, +\t\u0026#34;Oops - illegal instruction\u0026#34;); + +\tirqentry_exit_to_user_mode(regs); +\t} else { +\tirqentry_state_t state = irqentry_nmi_enter(regs); + +\tdo_trap_error(regs, SIGILL, ILL_ILLOPC, regs-\u0026gt;epc, +\t\u0026#34;Oops - illegal instruction\u0026#34;); + +\tirqentry_nmi_exit(regs, state); +\t} +} +bool riscv_v_first_use_handler(struct pt_regs *regs) +{ +\tu32 __user *epc = (u32 __user *)regs-\u0026gt;epc; +\tu32 insn = (u32)regs-\u0026gt;badaddr; + +\t/* Do not handle if V is not supported, or disabled */ +\tif (!(ELF_HWCAP \u0026amp; COMPAT_HWCAP_ISA_V)) +\treturn false; + +\t/* If V has been enabled then it is not the first-use trap */ +\tif (riscv_v_vstate_query(regs)) +\treturn false; + +\t/* Get the instruction */ +\tif (!insn) { +\tif (__get_user(insn, epc)) +\treturn false; +\t} + +\t/* Filter out non-V instructions */ +\tif (!insn_is_vector(insn)) +\treturn false; + +\t/* Sanity check. datap should be null by the time of the first-use trap */ +\tWARN_ON(current-\u0026gt;thread.vstate.datap); + +\t/* +\t* Now we sure that this is a V instruction. And it executes in the +\t* context where VS has been off. So, try to allocate the user\u0026#39;s V +\t* context and resume execution. +\t*/ +\tif (riscv_v_thread_zalloc()) { +\tforce_sig(SIGBUS); +\treturn true; +\t} +\triscv_v_vstate_on(regs); +\treturn true; +} 就如同 riscv_vector_spec 中描述的：\nAccurate setting of mstatus.VS is an optimization. Software will typically use VS to reduce context-swap overhead.\n内核启动默认关闭 mstatus.VS，用户态程序首次执行 vector 指令会触发Trap，这是一个非法指令异常。随后内核会判断该指令是否为vector指令以及是否首次执行，一切确定后则设置 mstatus.VS = INITIAL 并分配vector_context内存，返回用户态后程序就能正常执行vector指令了。\nvector上下文的保存恢复函数为 __switch_to_vector，该函数就在常规的调度控制流中，且与常规的 x0~x31 寄存器保存/恢复流程解耦，分布在不同的函数中。Linux内核将其称之为 “lazy store/restore”，在 vector_store/restore 函数中实际上存在对 mstatus.VS 的判断，这一举动**可以在用户态程序不用vector指令的情况下帮助内核节省下一大笔开销，**具体来说：\n当 mstatus.VS == OFF时：内核不需要保存/恢复vector上下文，因为用户态程序根本没使用过这些vector寄存器组； 当 mstatus.VS != OFF 时：内核需要保存/恢复vector上下文，表示用户态程序使用了vector寄存器组； mstatus.VS 提供了一种用户态程序通知内核的硬件机制，触发Trap时相当于告诉内核：“用户确实使用了vector，请内核使能必要的vector工作”。\n当前的Lazy还不够 为什么这么说，因为目前的实现在开启 mstatus.VS 之后vector_context的保存/恢复工作似乎无法阻拦了，那么在任何场景下都必须执行这些工作吗？并不是，具体见 3.1。\nvirtualization 目前内核对 vector 虚拟化的支持不完善，主要体现在两方面：\nvector_contex的host/guest管理； guest侧vector_trap处理； vector_context保存/恢复 [PATCH -next v21 19/27] riscv: KVM: Add vector lazy save/restore support - Andy Chiu (kernel.org)\n为区分不同的执行流，命名如下：\n宿主机内核态vcpu线程：vcpu_host_kernel 宿主机用户态vcpu线程：vcpu_host_user 虚拟机内核态vcpu线程：vcpu_guest_kernel 虚拟机用户态vcpu线程：vcpu_guest_user 在虚拟化模型中，涉及CPU上下文保存恢复的场景有 ( / 左侧为需要保存到内存的执行流状态，右侧为需要恢复到CPU寄存器上的执行流状态)：\nhost/guest world switch (vcpu线程进入退出)\n相关函数：__kvm_riscv_switch_to/__kvm_switch_return\nvcpu_host_kernel/vcpu_guest_kernel\nhost对vcpu线程的全局调度\n相关函数：vcpu_load/vcpu_put\n对两个vcpu线程的切换，如果被抢占的vcpu线程处于guest-Mode，除了做world switch外还需要做不同vcpu线程的上下文切换：\nstep_1：vcpu_guest_kernel/vcpu_host_kernel step_2：vcpu_host_kernel_prev/vcpu_host_kernel_next step_3：vcpu_host_kernel/vcpu_guest_kernel 退出到host user处理guest异常\n相关函数：vcpu_load/vcpu_put\n先做world switch，再做trap return：\nstep_1：vcpu_guest_kernel/vcpu_host_kernel step_2：vcpu_host_kernel/vcpu_host_user https://elixir.bootlin.com/linux/v6.7-rc6/C/ident/kvm_arch_vcpu_load\nhttps://elixir.bootlin.com/linux/v6.7-rc6/C/ident/kvm_arch_vcpu_put\n1 2 3 4 5 6 7 8 9 10 11 kvm_arch_vcpu_load +-\u0026gt; kvm_riscv_vcpu_host_fp_save(\u0026amp;vcpu-\u0026gt;arch.host_context); +-\u0026gt; kvm_riscv_vcpu_guest_fp_restore(\u0026amp;vcpu-\u0026gt;arch.guest_context, vcpu-\u0026gt;arch.isa); +-\u0026gt; kvm_riscv_vcpu_host_vector_save(\u0026amp;vcpu-\u0026gt;arch.host_context); +-\u0026gt; kvm_riscv_vcpu_guest_vector_restore(\u0026amp;vcpu-\u0026gt;arch.guest_context, vcpu-\u0026gt;arch.isa); kvm_arch_vcpu_put +-\u0026gt; kvm_riscv_vcpu_guest_fp_save(\u0026amp;vcpu-\u0026gt;arch.guest_context, vcpu-\u0026gt;arch.isa); +-\u0026gt; kvm_riscv_vcpu_host_fp_restore(\u0026amp;vcpu-\u0026gt;arch.host_context); +-\u0026gt; kvm_riscv_vcpu_guest_vector_save(\u0026amp;vcpu-\u0026gt;arch.guest_context, vcpu-\u0026gt;arch.isa); +-\u0026gt; kvm_riscv_vcpu_host_vector_restore(\u0026amp;vcpu-\u0026gt;arch.host_context); 存在的缺陷：\n对于 kvm_arch_vcpu_load：\nkvm_riscv_vcpu_host_fp_save/kvm_riscv_vcpu_host_vector_save\n仅检查是否支持f扩展，就直接保存host状态 =\u0026gt; 考虑guest对fp/vector的使用情况，应该延迟保存\nkvm_riscv_vcpu_guest_fp_restore/kvm_riscv_vcpu_guest_vector_restore\n仅检查 vsstatus.FS != SR_FS_OFF 和 vsstatus.VS != SR_VS_OFF ，说明guest已允许float/vector指令的执行，直接恢复guest状态\n如果条件判断为guest禁用了float/vector_trap，恢复其上下文完全没有问题；但考虑这样一种情况：host没使用过float/vector指令，当前pCPU的float/vector寄存器仅为guest所用，guest此时完全不需要执行本次恢复工作。 如果条件判断为guest启用了float/vector_trap，那么之前的host状态完全没必须要保存； put 操作同理\nguest发生vector_trap处理 2.2 *Delayed update strategy non-virtualization 见3.1\nvirtualization switch.h - arch/arm64/kvm/hyp/include/hyp/switch.h - Linux source code (v6.7-rc6) - Bootlin https://elixir.bootlin.com/linux/v6.7-rc6/C/ident/FP_STATE_GUEST_OWNED 3 ARMv8 FPSIMD/SVE内核支持 3.1 Linux-ARM引入的FPSIMD状态跟踪机制 fpsimd.c - arch/arm64/kernel/fpsimd.c - Linux source code (v6.7-rc6) - Bootlin\n（注意：在本讨论中，关于FPSIMD的陈述同样适用于SVE。）\n为了减少不必要地保存和恢复FPSIMD状态的次数，我们需要跟踪两件事情：\n对于每个任务，我们需要记住最后一个将任务的FPSIMD状态加载到其FPSIMD寄存器中的CPU是哪一个； 对于每个CPU，我们需要记住最近加载到其FPSIMD寄存器中的用户态FPSIMD状态属于哪个任务，或者在此期间是否已被用于执行内核模式NEON操作。 对于（a），我们向thread_struct添加了一个fpsimd_cpu字段，每当状态被加载到CPU上时，该字段会更新为当前CPU的ID。对于（b），我们添加了每个CPU的变量 fpsimd_last_state，其中包含最近加载到CPU上的任务的用户空间FPSIMD状态的地址，如果在此之后执行了内核模式NEON，则为NULL。\n有了这个设置，我们在任务切换时就不再需要立即恢复下一个FPSIMD状态。相反，我们可以将这个检查推迟到用户空间的恢复阶段，在这个阶段我们验证CPU的 fpsimd_last_state 和任务的 fpsimd_cpu 是否仍然保持同步。如果是这种情况，我们可以省略FPSIMD的恢复操作。\n作为一种优化，我们使用线程信息标志 TIF_FOREIGN_FPSTATE 来指示当前任务的用户态FPSIMD状态是否存在于寄存器中。除非当前CPU的FPSIMD寄存器当前包含当前任务的最新用户态FPSIMD状态，否则设置该标志。如果任务表现为虚拟机监视器（VMM），则由KVM管理，KVM将清除该标志以指示vcpu的FPSIMD状态当前加载在CPU上，以便在FPSIMD感知的软中断触发时保存状态。在vcpu_put()时，KVM将保存vcpu的FP状态并标记寄存器状态为无效。\n为了允许softirq处理程序使用FPSIMD，kernel_neon_begin() 可能会在softirq上下文中将任务的FPSIMD上下文保存回task_struct。 为了防止这与任务上下文中的FPSIMD状态的操作竞争，并因此破坏状态，有必要使用 {，__}get_cpu_fpsimd_context() 保护对任务的fpsimd_state或TIF_FOREIGN_FPSTATE标志的任何操作。这仍将允许softirq运行，但防止它们使用FPSIMD。\n对于某个任务，其序列可能如下所示：\n**任务被调度：**如果任务的fpsimd_cpu字段包含当前CPU的ID，且CPU的 fpsimd_last_state per-cpu变量指向任务的fpsimd_state，TIF_FOREIGN_FPSTATE标志位被清除，否则被设置； **任务返回到用户空间：**如果设置了TIF_FOREIGN_FPSTATE标志，任务的用户空间FPSIMD状态将从内存复制到寄存器中，任务的fpsimd_cpu字段将设置为当前CPU的ID，当前CPU的 fpsimd_last_state 指针将设置为该任务的fpsimd_state，并清除TIF_FOREIGN_FPSTATE标志； **该任务执行一个普通的系统调用：**当返回到用户空间时，TIF_FOREIGN_FPSTATE标志仍将被清除，因此不会恢复FPSIMD状态； **该任务执行一个系统调用，该系统调用执行一些NEON指令：**在此之前，调用 kernel_neon_begin() 函数，将任务的FPSIMD寄存器内容复制到内存中，清除fpsimd_last_state每CPU变量，并设置TIF_FOREIGN_FPSTATE标志； **在调用kernel_neon_end()之后，任务被抢占：**由于我们还没有从第二个系统调用中返回，TIF_FOREIGN_FPSTATE仍然被设置，因此FPSIMD寄存器中的内容不会被保存到内存中，而是被丢弃。 task0首次被调度：\n判断是否保持同步 * task0-\u0026gt;fpsimd_cpu ?= pCPU0; * fpsimd_last_state ?= task0; =\u0026gt; TIF_FOREIGN_FPSTATE = true;\ntask0返回用户态：\n* 判断TIF_FOREIGN_FPSTATE，这里为TRUE， 那就恢复FPSIMD状态到寄存器上； * task0-\u0026gt;fpsimd_cpu = pCPU0; * fpsimd_last_state = task0; * TIF_FOREIGN_FPSTATE = false;\ntask0在用户态主动或被动地让出CPU控制权：\n注意：并不是被另一个task抢占，这种情况也比较常见，而且假设在task0等待期间pCPU0处于闲置状态，即pCPU0并没有装载其它task的上下文。\ntask0再次被调度运行，目标CPU仍然为pCPU0：\n还是判断和1）相同的两个变量，看是否同步，此时： task0-\u0026gt;fpsimd_cpu = pCPU0； fpsimd_last_state = task0; =\u0026gt; TIF_FOREIGN_FPSTATE = false;\ntask0再次返回用户态：\ntask0再次被调度运行，目标CPU仍然为pCPU0：\n3.2 针对虚拟化场景的优化 Linux KVM-ARM PATCH [PATCH v5 0/8] arm64/sve: Clean up KVM integration and optimise syscalls - Mark Brown (kernel.org)\n该补丁系列试图澄清在支持SVE系统中跟踪哪组浮点寄存器的保存情况，特别是与KVM相关的情况，并利用这种澄清结果来提高用户空间使用SVE的简单系统调用的性能。\n目前，我们通过为当前任务使用的TIF_SVE标志来跟踪活动的寄存器状态，该标志还控制用户空间是否能使用SVE，这在一定程度上是相对简单的，但对于KVM来说稍微复杂一些，因为我们可能在寄存器中加载了客户机状态。这导致KVM在客户机运行时修改VMM任务的TIF_SVE，这并没有完全有助于使事情易于理解。为了使事情更清晰，该系列对此进行了更改，除了TIF_SVE之外，我们还明确跟踪了当前保存在任务结构体中的寄存器类型以及在保存时应该保存的寄存器类型。\n因此，TIF_SVE仅控制用户空间是否可以无需陷阱地使用SVE，对于KVM客户机而言，它没有功能，我们可以从KVM中删除管理它的代码。\n通过添加单独的跟踪，重新设计的工作开始进行，同时还添加了在查看分离的状态之前对状态进行验证的检查。此举的目标是将重复性更高的部分拆分出来，并更容易调试可能出现的任何问题。\n在单独跟踪状态后，我们开始优化使用SVE的系统调用的性能。目前，每个系统调用都会为用户空间禁用SVE，这意味着下一个SVE指令需要再次陷入EL1，刷新SVE寄存器，并为EL0重新启用SVE，从而为混合使用SVE和系统调用的任务创建了开销。借助上述的重新设计，我们通过在不需要从内存重新加载状态时保持SVE启用，来消除对直接返回用户空间的简单系统调用的开销，这意味着如果系统调用不阻塞，我们避免了下一次使用SVE时陷入EL1的开销。\n该系列还包括一个相关的补丁，简化了 fpsimd_bind_state_to_cpu() 的接口，减少了该函数接受的大量参数。不管有没有这个系列的影响，这已经是一个问题，但该系列进一步放放大了这个问题，如果这种方法对大家都可以接受，我们可以在更多地方使用该结构体。为了避免用户可见的改进被代码清理所拖延，此补丁被放在最后。\n[PATCH v2 04/19] KVM: arm64: Move FP state ownership from flag to a tristate - Marc Zyngier (kernel.org)\nKVM FP代码使用一对标志来表示三种状态：\n设置FP_ENABLED：客户机拥有FP状态 设置FP_HOST：主机拥有FP状态 清除FP_ENABLED和FP_HOST：没有任何人拥有FP状态 同时设置两个标志是非法状态，但没有任何地方会检查这种状态\u0026hellip;\n事实证明，这对标志并不是一个好的匹配方式，如果我们将其改为更简单的三态，并且每个状态都有一个实际反映状态的名称，那会更好：\nFP_STATE_FREE（空闲状态） FP_STATE_HOST_OWNED（主机拥有状态） FP_STATE_GUEST_OWNED（客户机拥有状态） 删除这两个标志，并改为使用枚举来表示这三种状态。这样可以减少令人困惑的代码，并减少忘记清除两个标志中的一个而导致进入未知领域的风险。\nfpsimd/sve虚拟化框架分析 https://elixir.bootlin.com/linux/v6.7-rc7/C/ident/kvm_hyp_handle_fpsimd\n在armv8中，fp和sve复用一套寄存器，可以看到在函数中针对sve的处理为：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 /* SVE exposed to guest */ #define GUEST_HAS_SVE\t__vcpu_single_flag(cflags, BIT(0)) #define vcpu_has_sve(vcpu) (system_supports_sve() \u0026amp;\u0026amp;\t\\ vcpu_get_flag(vcpu, GUEST_HAS_SVE)) static bool kvm_hyp_handle_fpsimd(struct kvm_vcpu *vcpu, u64 *exit_code) { bool sve_guest; u8 esr_ec; u64 reg; if (!system_supports_fpsimd()) return false; sve_guest = vcpu_has_sve(vcpu); esr_ec = kvm_vcpu_trap_get_class(vcpu); /* Only handle traps the vCPU can support here: */ switch (esr_ec) { case ESR_ELx_EC_FP_ASIMD: break; case ESR_ELx_EC_SVE: if (!sve_guest) return false; break; default: return false; } /* Valid trap. Switch the context: */ /* First disable enough traps to allow us to update the registers */ /* Write out the host state if it\u0026#39;s in the registers */ if (vcpu-\u0026gt;arch.fp_state == FP_STATE_HOST_OWNED) __fpsimd_save_state(vcpu-\u0026gt;arch.host_fpsimd_state); /* Restore the guest state */ if (sve_guest) __hyp_sve_restore_guest(vcpu); else __fpsimd_restore_state(\u0026amp;vcpu-\u0026gt;arch.ctxt.fp_regs); vcpu-\u0026gt;arch.fp_state = FP_STATE_GUEST_OWNED; return true; } 可以看到，在arm中guest首次访问fp/sve被定义为不同类型的异常，因为当硬件支持SVE时fp/sve复用一套寄存器组，各自对恢复上下文的处理方式得到了统一：\nESR_ELx_EC_FP_ASIMD sve_guest == true：恢复至SVE寄存器组的低bit； sve_guest == false：guest不支持sve，恢复至传统的fp_regs； ESR_ELx_EC_SVE sve_guest == true：恢复至SVE寄存器组的完整bit； sve_guest == false：Invalid Trap!!! 疑问：riscv支持vector扩展时，是否和fp复用一套寄存器？\n这关系到vector_guest启用时，恢复哪些寄存器组，涉及到代码结构的设计。\n1 2 3 4 5 /* Check whether the FP regs are owned by the guest */ static inline bool guest_owns_fp_regs(struct kvm_vcpu *vcpu) { return vcpu-\u0026gt;arch.fp_state == FP_STATE_GUEST_OWNED; } ","date":"2024-10-17T10:02:03+08:00","permalink":"https://zcxggmu.github.io/p/riscv-arm-simd-compare/","title":"Riscv Arm Simd Compare"},{"content":"0 参考 [vector-isa-support]([PATCH -next v20 00/26] riscv: Add vector ISA support - Andy Chiu (kernel.org))\n[vector-1.0-spec](riscv/riscv-v-spec: Working draft of the proposed RISC-V V vector extension (github.com))\n[vector-1.0解读](【个人笔记】RISC-V \u0026ldquo;V\u0026rdquo; Vector Extension Version 1.0 - 知乎 (zhihu.com))\n1 背景 [PATCH -next v21 00/27] riscv: Add vector ISA support - Andy Chiu (kernel.org)在这个实现中，有一些假设：\n我们假设系统中的所有harts都具有相同的ISA。 默认情况下，我们在内核和用户空间中禁用向量。只有在一个非法指令陷阱（第一次使用陷阱）中，实际开始执行向量后，才启用用户的向量。 我们检测 \u0026ldquo;riscv,isa\u0026rdquo; 来确定是否支持向量。 我们在结构体 thread_struct 中定义了一个新的结构体 __riscv_v_ext_state，用于保存/恢复与向量相关的寄存器。它用于内核空间和用户空间。\n在内核空间中，__riscv_v_ext_state 中的 datap 指针将被分配用于保存向量寄存器。 在用户空间中： 在用户空间的信号处理器中，该结构体位于 __riscv_ctx_hdr 之后，该结构体嵌入在fp保留区域中。这是为了避免ABI中断。并且datap指向 __riscv_v_ext_state 的末尾。 在ptrace中，数据将被放入ubuf中，我们使用 riscv_vr_get()/riscv_vr_set() 从ubuf中获取或设置 __riscv_v_ext_state 数据结构，datap指针将被清零，向量寄存器将被复制到ubuf中 __riscv_v_ext_state 结构体后的地址。 重点关注以下几点：\nvector的Trap处理 vector的上下文保存与恢复 2 解析 2.1 vector_trap ISA mstatus/sstatus A vector context status field, VS, is added to mstatus[10:9] and shadowed in sstatus[10:9]. It is defined analogously to the floating-point context status field, FS.\nAttempts to execute any vector instruction, or to access the vector CSRs, raise an illegal-instruction exception when mstatus.VS is set to Off.\nWhen mstatus.VS is set to Initial or Clean, executing any instruction that changes vector state, including the vector CSRs, will change mstatus.VS to Dirty. Implementations may also change mstatus.VS from Initial or Clean to Dirty at any time, even when there is no change in vector state.\nvsstatus When the hypervisor extension is present, a vector context status field, VS, is added to vsstatus[10:9]. It is defined analogously to the floating-point context status field, FS.\nWhen V=1, both vsstatus.VS and mstatus.VS are in effect: attempts to execute any vector instruction, or to access the vector CSRs, raise an illegal-instruction exception when either field is set to Off.\nWhen V=1 and neither vsstatus.VS nor mstatus.VS is set to Off, executing any instruction that changes vector state, including the vector CSRs, will change both mstatus.VS and vsstatus.VS to Dirty. Implementations may also change mstatus.VS or vsstatus.VS from Initial or Clean to Dirty at any time, even when there is no change in vector state.\nIf vsstatus.VS is Dirty, vsstatus.SD is 1; otherwise, vsstatus.SD is set in accordance with existing specifications.\nIf mstatus.VS is Dirty, mstatus.SD is 1; otherwise, mstatus.SD is set in accordance with existing specifications.\nFor implementations with a writable misa.V field, the vsstatus.VS field may exist even if misa.V is clear.\nfpsimd_state 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 union __riscv_fp_state { struct __riscv_f_ext_state f; struct __riscv_d_ext_state d; struct __riscv_q_ext_state q; }; struct __riscv_v_ext_state { unsigned long vstart; unsigned long vl; unsigned long vtype; unsigned long vcsr; unsigned long vlenb; void *datap; /* * In signal handler, datap will be set a correct user stack offset * and vector registers will be copied to the address of datap * pointer. */ }; 考虑到 vector_context 的大小可能变化且可能很大，它们被保存在动态分配的内存中，由__riscv_v_ext_state 中的datap指针指向。\ntrap_control/handle 流程如下：\n系统启动，主/从核拉起时，设置fpsimd为OFF状态；\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 SYM_CODE_START(_start_kernel) /* * Disable FPU \u0026amp; VECTOR to detect illegal usage of * floating point or vector in kernel space */ li t0, SR_FS_VS csrc CSR_STATUS, t0 #ifdef CONFIG_SMP .global secondary_start_sbi secondary_start_sbi: /* * Disable FPU \u0026amp; VECTOR to detect illegal usage of * floating point or vector in kernel space */ li t0, SR_FS_VS csrc CSR_STATUS, t0 进程在用户态首次执行fpsimd指令，陷入处理并返回：\n向量单元默认情况下对所有用户进程都是禁用的。因此当进程首次使用向量时，它会因非法指令而陷入内核。只有在那之后，内核才会为该用户进程分配vector上下文，并开始管理该上下文。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 static int riscv_v_thread_zalloc(void) { void *datap; datap = kzalloc(riscv_v_vsize, GFP_KERNEL); if (!datap) return -ENOMEM; current-\u0026gt;thread.vstate.datap = datap; memset(\u0026amp;current-\u0026gt;thread.vstate, 0, offsetof(struct __riscv_v_ext_state, datap)); return 0; } bool riscv_v_first_use_handler(struct pt_regs *regs) { u32 __user *epc = (u32 __user *)regs-\u0026gt;epc; u32 insn = (u32)regs-\u0026gt;badaddr; /* Do not handle if V is not supported, or disabled */ if (!(ELF_HWCAP \u0026amp; COMPAT_HWCAP_ISA_V)) return false; /* If V has been enabled then it is not the first-use trap */ if (riscv_v_vstate_query(regs)) return false; /* Get the instruction */ if (!insn) { if (__get_user(insn, epc)) return false; } /* Filter out non-V instructions */ if (!insn_is_vector(insn)) return false; /* Sanity check. datap should be null by the time of the first-use trap */ WARN_ON(current-\u0026gt;thread.vstate.datap); /* * Now we sure that this is a V instruction. And it executes in the * context where VS has been off. So, try to allocate the user\u0026#39;s V * context and resume execution. */ if (riscv_v_thread_zalloc()) { force_sig(SIGBUS); return true; } riscv_v_vstate_on(regs); riscv_v_vstate_restore(current, regs); return true; } 用户态首次vector_trap将分配实际内存，用于保存/恢复vector上下文，返回用户态之前将 sstatus.VS 设置为Initial。之后用户态将正常使用vector。\n2.2 vector_context_switch 相较于通用寄存器组，vector上下文更庞大。因此在大量使用vector的场景中，需要尽可能的避免vector上下文的保存/恢复，降低系统开销。在多核多进程系统中，内核需要跟踪比对 task - cpu 的 (绑定指向) 状态以确定某一次的vector上下文的保存/恢复上是否 “真的有必要”。先看一下目前内核在该问题的处理方式。\nkernel: context_save/restore 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 static inline void riscv_v_vstate_save(struct task_struct *task, struct pt_regs *regs) { if ((regs-\u0026gt;status \u0026amp; SR_VS) == SR_VS_DIRTY) { struct __riscv_v_ext_state *vstate = \u0026amp;task-\u0026gt;thread.vstate; __riscv_v_vstate_save(vstate, vstate-\u0026gt;datap); __riscv_v_vstate_clean(regs); } } static inline void riscv_v_vstate_restore(struct task_struct *task, struct pt_regs *regs) { if ((regs-\u0026gt;status \u0026amp; SR_VS) != SR_VS_OFF) { struct __riscv_v_ext_state *vstate = \u0026amp;task-\u0026gt;thread.vstate; __riscv_v_vstate_restore(vstate, vstate-\u0026gt;datap); __riscv_v_vstate_clean(regs); } } static inline void __switch_to_vector(struct task_struct *prev, struct task_struct *next) { struct pt_regs *regs; regs = task_pt_regs(prev); riscv_v_vstate_save(prev, regs); riscv_v_vstate_restore(next, task_pt_regs(next)); } vector-1.0-spec 中有一句注释：\nAccurate setting of mstatus.VS is an optimization. Software will typically use VS to reduce context-swap overhead.\n内核中也确实这么做了，这是硬件的优化机制：\nvector上下文如果为DIRTY状态，保存的时候就必须去硬件上拉取最新的内容，否则就不需要做任何工作； vector上下文如果为OFF状态，恢复的时候就无需做任何工作，否则就必须恢复vector上下文状态； 内核最近提了一组PATCH，考虑了这样一种调度场景：\n用户只有在内核真正返回到用户空间后才会使用其向量寄存器。因此，只要我们还在内核模式下运行，可以延迟恢复向量寄存器。内核需要添加一个线程标志以指示是否需要恢复向量，并在最后的特定于架构的退出用户模式的临界点处进行恢复。这样可以节省在内核模式下切换多个运行V操作的进程时的上下文恢复开销。\n举个例子：如果内核执行 A-\u0026gt;B-\u0026gt;C 的上下文切换，并最终返回到 C 的用户空间，那么就没有必要恢复 B 的vector寄存器。\n此外，这还可以防止我们在多次执行内核模式的向量操作时重复恢复vector上下文。这样做的代价是，在执行 vstate_{save,restore} 期间我们必须禁用抢占并将向量标记为忙碌。因为如果在 vstate_{save,restore} 过程中发生导致陷阱的上下文切换，vector上下文将不会立即恢复。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 riscv_v_first_use_handler +-\u0026gt; riscv_v_vstate_set_restore __switch_to_vector +-\u0026gt; riscv_v_vstate_set_restore #define TIF_RISCV_V_DEFER_RESTORE\t12 /* restore Vector before returing to user */ #define _TIF_RISCV_V_DEFER_RESTORE\t(1 \u0026lt;\u0026lt; TIF_RISCV_V_DEFER_RESTORE) static inline void riscv_v_vstate_set_restore(struct task_struct *task, struct pt_regs *regs) { if ((regs-\u0026gt;status \u0026amp; SR_VS) != SR_VS_OFF) { set_tsk_thread_flag(task, TIF_RISCV_V_DEFER_RESTORE); riscv_v_vstate_on(regs); } } +static inline void arch_exit_to_user_mode_prepare(struct pt_regs *regs, +\tunsigned long ti_work) +{ +\tif (ti_work \u0026amp; _TIF_RISCV_V_DEFER_RESTORE) { +\tclear_thread_flag(TIF_RISCV_V_DEFER_RESTORE); +\t/* +\t* We are already called with irq disabled, so go without +\t* keeping track of riscv_v_flags. +\t*/ +\triscv_v_vstate_restore(current, regs); +\t} +} + +#define arch_exit_to_user_mode_prepare arch_exit_to_user_mode_prepare 总体来说，将vector恢复工作推迟到返回用户态的节点，有两个优势：\nS-Mode下无论发生了多少次抢占，内核只关注返回用户态的那个task，中间的多个 task_vector_context 没必要恢复； S-Mode下也可能会使用vector，这种情况下会覆盖掉用户态的vector上下文，内核必须在使用vector前保存用户态vector上下文，在使用vector结束时恢复。但S-Mode有可能多次执行向量操作时，内核没必要重复恢复vector上下文，因此也是标记一个 REFER_RESTORE，一并推迟到返回用户态的节点； kvm: context_save/restore KVM: Add vector lazy save/restore support\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 kvm_arch_vcpu_load +-\u0026gt; kvm_riscv_vcpu_host_fp_save(\u0026amp;vcpu-\u0026gt;arch.host_context); +-\u0026gt; kvm_riscv_vcpu_guest_fp_restore(\u0026amp;vcpu-\u0026gt;arch.guest_context, vcpu-\u0026gt;arch.isa); +-\u0026gt; kvm_riscv_vcpu_host_vector_save(\u0026amp;vcpu-\u0026gt;arch.host_context); +-\u0026gt; kvm_riscv_vcpu_guest_vector_restore(\u0026amp;vcpu-\u0026gt;arch.guest_context, vcpu-\u0026gt;arch.isa); kvm_arch_vcpu_put +-\u0026gt; kvm_riscv_vcpu_guest_fp_save(\u0026amp;vcpu-\u0026gt;arch.guest_context, vcpu-\u0026gt;arch.isa); +-\u0026gt; kvm_riscv_vcpu_host_fp_restore(\u0026amp;vcpu-\u0026gt;arch.host_context); +-\u0026gt; kvm_riscv_vcpu_guest_vector_save(\u0026amp;vcpu-\u0026gt;arch.guest_context, vcpu-\u0026gt;arch.isa); +-\u0026gt; kvm_riscv_vcpu_host_vector_restore(\u0026amp;vcpu-\u0026gt;arch.host_context); kvm_arch_vcpu_load(vcpu); while(ret \u0026gt; 0) { //... __guest_enter(); //... ret = handle_exit(vcpu); } kvm_arch_vcpu_put(vcpu); void kvm_riscv_vcpu_guest_vector_save(struct kvm_cpu_context *cntx, unsigned long *isa) { if ((cntx-\u0026gt;sstatus \u0026amp; SR_VS) == SR_VS_DIRTY) { if (riscv_isa_extension_available(isa, v)) __kvm_riscv_vector_save(cntx); kvm_riscv_vcpu_vector_clean(cntx); } } void kvm_riscv_vcpu_guest_vector_restore(struct kvm_cpu_context *cntx, unsigned long *isa) { if ((cntx-\u0026gt;sstatus \u0026amp; SR_VS) != SR_VS_OFF) { if (riscv_isa_extension_available(isa, v)) __kvm_riscv_vector_restore(cntx); kvm_riscv_vcpu_vector_clean(cntx); } } void kvm_riscv_vcpu_host_vector_save(struct kvm_cpu_context *cntx) { /* No need to check host sstatus as it can be modified outside */ if (riscv_isa_extension_available(NULL, v)) __kvm_riscv_vector_save(cntx); } void kvm_riscv_vcpu_host_vector_restore(struct kvm_cpu_context *cntx) { if (riscv_isa_extension_available(NULL, v)) __kvm_riscv_vector_restore(cntx); } 存在的缺陷：\n对于 kvm_arch_vcpu_load：\nkvm_riscv_vcpu_host_fp_save/kvm_riscv_vcpu_host_vector_save\n仅检查是否支持f扩展，就直接保存host状态 =\u0026gt; 考虑guest对fp/vector的使用情况，应该延迟保存\nkvm_riscv_vcpu_guest_fp_restore/kvm_riscv_vcpu_guest_vector_restore\n仅检查 vsstatus.FS != SR_FS_OFF 和 vsstatus.VS != SR_VS_OFF ，说明guest已允许float/vector指令的执行，直接恢复guest状态\n如果条件判断为guest禁用了float/vector_trap，恢复其上下文完全没有问题；但考虑这样一种情况：host没使用过float/vector指令，当前pCPU的float/vector寄存器仅为guest所用，guest此时完全不需要执行本次恢复工作。 如果条件判断为guest启用了float/vector_trap，那么之前的host状态完全没必须要保存； ","date":"2024-10-17T10:00:38+08:00","permalink":"https://zcxggmu.github.io/p/vector-isa-support/","title":"Vector Isa Support"},{"content":"0 资料 【原创】（一）Linux进程调度器-基础 - LoyenWang - 博客园 (cnblogs.com)\n【原创】（三）Linux进程调度器-进程切换 - LoyenWang - 博客园 (cnblogs.com)\n进程调度 - 知乎 (zhihu.com)\n[内核抢占机制(preempt)-CSDN博客](https://blog.csdn.net/u014426028/article/details/106927601#:~:text=内核抢占指用户程序在执行系统调用期间可以被抢占，该进程暂时挂起，使新唤醒的高优先级进程能够运行。,这种抢占并非可以在内核中任意位置都能安全进行，比如在临界区中的代码就不能发生抢占。 临界区是指同一时间内不可以有超过一个进程在其中执行的指令序列。)\nsched.h - include/linux/sched.h - Linux source code (v6.7-rc7) - Bootlin\n1 Linux进程/线程的抽象模型 1.1 进程/线程的统一抽象: task_struct 关于task_struct 任何教科书上都会这样写：进程是资源分配的最小单位，而线程是CPU调度的的最小单位。\n**对于进程而言：**进程不仅包括可执行程序的代码段，还包括一系列的资源，比如：打开的文件、内存、CPU时间、信号量、多个执行线程流等等。进程不仅提供了一个线程资源容器，还负责线程调度工作； **对于线程而言：**线程是程序的执行实体，相比于进程，线程更真实地呈现在CPU上，CPU上各寄存器保存的值支撑了程序控制流的执行。其中 thread_struct 保存的是最核心的CPU寄存器内容，其和特定架构相关； 但在Linux内核中，两者均使用 task_struct 进行抽象，给人一种进程和线程相同的错觉。在之前学习的 rCore 实现中，进程/线程模型有着比较清晰的数据结构上的层次感：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 pub struct ProcessControlBlock { // immutable pub pid: PidHandle, // mutable inner: UPSafeCell\u0026lt;ProcessControlBlockInner\u0026gt;, } pub struct ProcessControlBlockInner { pub is_zombie: bool, pub memory_set: MemorySet, pub parent: Option\u0026lt;Weak\u0026lt;ProcessControlBlock\u0026gt;\u0026gt;, pub children: Vec\u0026lt;Arc\u0026lt;ProcessControlBlock\u0026gt;\u0026gt;, pub exit_code: i32, pub fd_table: Vec\u0026lt;Option\u0026lt;Arc\u0026lt;dyn File + Send + Sync\u0026gt;\u0026gt;\u0026gt;, pub signals: SignalFlags, pub tasks: Vec\u0026lt;Option\u0026lt;Arc\u0026lt;TaskControlBlock\u0026gt;\u0026gt;\u0026gt;, pub task_res_allocator: RecycleAllocator, ... // 其他同步互斥相关资源 } 可以看到，进程控制块中通过 tasks 保存着全部线程资源。\n但 Linux 进程/线程模型与之有很大的区别。那么，进程和线程在 task_struct 结构体中是否有标识上的不同？实际上，在struct task_struct中并没有明确的标识（枚举类型），区分该task是线程还是进程，不过可以通过pid和tgid简单判断当前task是哪种类型。\n1 2 3 4 5 6 7 struct task_struct { //... pid_t pid; pid_t tgid; //... struct *group_leader; } pid用于标识不同进程和线程； tgid用于标识线程组id，在同一进程中的所有线程具有同一tgid。tgid值等于进程第一个线程（主线程）的pid值。接着以CLONE_THREAD来调用clone建立的线程，都具有同样的tgid。也就是说，对于没有创建线程的进程(只包含一个主线程)来说，这个 pid 就是进程的 PID，tgid 和 pid 是相同的。 group_leader 线程组中的主线程的task_struct指针。 那么除了tgid和group_leadr是进程/线程的区别外，还有什么其他的区别么？\n进程还是线程的创建都是由父进程/父线程调用系统调用接口实现的。创建的主要工作实际上就是创建task_strcut结构体，并将该对象添加至工作队列中去。而线程和进程在创建时，通过CLONE_THREAD flag的不同，而选择不同的方式共享父进程/父线程的资源，从而造成在用户空间所看到的进程和线程的不同。\n从进程/线程创建角度来看task_struct 无论以何种方式创建线程/进程在Linux kernel最终都是调用do_fork接口（定义在kernel/fork.c）\n其函数原型为：\n1 2 3 4 5 long do_fork(unsigned long clone_flags, unsigned long stack_start, unsigned long stack_size, int __user *parent_tidptr, int __user *child_tidptr) clone_flags是一个标志集合，用来指定控制复制过程的一些属性。最低字节指定了在子进程终止时被发给父进程的信号号码。其余的高位字节保存了各种常数。 stack_start是用户状态下栈的起始地址。 stack_size是用户状态下栈的大小。 arent_tidptr和child_tidptr是指向用户空间中地址的两个指针，分别指向父子进程的PID。 do_fork的代码流程图如下所示：\n上面流程特别判断了是否是vfork，该接口是vfork接口call下来，在子进程没有执行完前，父进程处于阻塞态。一般用于子进程直接调用execv时使用。因为子进程不需要copy父进程的资源从而减少do_fork时的消耗，不过由于fork增加了写时复制机制，vfork也很少使用。这些不是这篇介绍的重点。\n那么拿掉vfork的过程，do_fork主要做了三件事：\ncopy_process 确定PID wake_up_new_task wake_up_new_task即是将新创建的线程/进程添加至调度程序的队列中。do_fork主要的一部分工作集中在copy_process中，线程与进程之间的区别也是在该接口中体现，接口的代码流程图如下所示：\n当上层以pthread_create接口call到kernel时，clone_flag是有CLONE_PTHREAD标识。但CLONE_PTHREAD标识只在最后一个步骤（设置各个ID、进程关系）时体现：（current为当前进程/线程的task_struct结构体 ，p为新创建的结构体对象）：\n1 2 3 4 5 6 7 if (clone_flags \u0026amp; CLONE_THREAD) { p-\u0026gt;group_leader = current-\u0026gt;group_leader; p-\u0026gt;tgid = current-\u0026gt;tgid; } else { p-\u0026gt;group_leader = p; p-\u0026gt;tgid = p-\u0026gt;pid; } 那么，以我们的理解来看，线程会共享信号、共享虚拟地址空间\u0026hellip;又以什么体现呢？去glibc查询了pthread_create的实现，当call到kernel时的clone_flag如下：\n1 2 3 4 5 const int clone_flags = (CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SYSVSEM | CLONE_SIGHAND | CLONE_THREAD | CLONE_SETTLS | CLONE_PARENT_SETTID | CLONE_CHILD_CLEARTID | 0); 所以在创建线程时，clone_flags有其他许多项共同构成，才让我们看出来最终线程与进程间的不同。这些flag主要体现在【分享/复制进程各个部分中】步骤。\n这些CLONE_abc的使用方法相似。在这些形如copy_abc的接口中，通过判断该flag标识，决定对内核子系统资源是与父进程/线程公用还是新创建出来。可参考下图。\n一开始父进程和子进程对于res_abc指向同一个内容（通过dup_task_struct接口实现，子进程完全copy父进程），然后经过copy_abc程序，当有CLONE_abc标识时，父进程会共享资源，同时res_abc的引用计数+1，当!CLONE_abc时，会创建一个res_abc的副本。\n又去glibc查询了fork的clone_flags：\n1 CLONE_CHILD_SETTID | CLONE_CHILD_CLEARTID | SIGCHLD; 那么线程创建就比进程多CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SYSVSEM | CLONE_SIGHAND | CLONE_THREAD，从这些宏的字面意义上看，子线程会与父线程共享虚拟地址空间、文件、信号等。\n每个flag的实现原理基本一致，上文已讨论过，这里仅对具体的哪些资源造成了影响进行分析。\nCLONE_VM 为虚拟地址空间：所以子线程会共享父线程的虚拟地址空间（通过struct mm_struct *mm指向实例描述）,active_mm当用户线程切换至内核线程时使用，这里不详述。 CLONE_FS struct fs_struct *fs 进程当前目录及工作目录； CLONE_SIGHAND struct sighand_struct *sighand 信号及信号处理函数 进程/线程状态 task_struct结构很复杂，下边只针对与调度相关的某些字段进行介绍：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 struct task_struct { /* ... */ /* 进程状态 */ volatile long\tstate; /* 调度优先级相关，策略相关 */ int\tprio; int\tstatic_prio; int\tnormal_prio; unsigned int\trt_priority; unsigned int\tpolicy; /* 调度类，调度实体相关，任务组相关等 */ const struct sched_class\t*sched_class; struct sched_entity\tse; struct sched_rt_entity\trt; #ifdef CONFIG_CGROUP_SCHED struct task_group\t*sched_task_group; #endif struct sched_dl_entity\tdl; /* 进程之间的关系相关 */ /* Real parent process: */ struct task_struct __rcu\t*real_parent; /* Recipient of SIGCHLD, wait4() reports: */ struct task_struct __rcu\t*parent; /* * Children/sibling form the list of natural children: */ struct list_head\tchildren; struct list_head\tsibling; struct task_struct\t*group_leader; /* CPU-specific state of this task: */ struct thread_struct\tthread; } 下图中左侧为操作系统中通俗的进程三状态模型，右侧为Linux对应的进程状态切换。每一个标志描述了进程的当前状态，这些状态都是互斥的：\n注意：Linux中的就绪态和运行态对应的都是TASK_RUNNING标志位，就绪态表示进程正处在队列中，尚未被调度；运行态则表示进程正在CPU上运行；\n内核中主要的状态字段定义如下：\n1 2 3 4 5 6 /* Used in tsk-\u0026gt;state: */ #define TASK_RUNNING\t0x0000 #define TASK_INTERRUPTIBLE\t0x0001 #define TASK_UNINTERRUPTIBLE\t0x0002 //... 1.2 调度器: scheduler 后文中将不再区分进程/线程，而采用统一的 “进程” 来描述。因为Linux内核使用 task_struct 对两者在数据结构上进行了统一，但两者在引用资源的方式上有所不同。\n所谓调度，就是按照某种调度的算法，从进程的就绪队列中选取进程分配CPU，主要是协调对CPU等的资源使用。进程调度的目标是最大限度利用CPU时间。内核默认提供了5个调度器，Linux内核使用 struct sched_class 来对调度器进行抽象：\nStop调度器， stop_sched_class：优先级最高的调度类，可以抢占其他所有进程，不能被其他进程抢占； Deadline调度器， dl_sched_class：使用红黑树，把进程按照绝对截止期限进行排序，选择最小进程进行调度运行； RT调度器， rt_sched_class：实时调度器，为每个优先级维护一个队列； CFS调度器， cfs_sched_class：完全公平调度器，采用完全公平调度算法，引入虚拟运行时间概念； IDLE-Task调度器， idle_sched_class：空闲调度器，每个CPU都会有一个idle线程，当没有其他进程可以调度时，调度运行idle线程； Linux内核提供了一些调度策略供用户程序来选择调度器，其中 Stop调度器 和 IDLE-Task调度器 ，仅由内核使用，用户无法进行选择：\nSCHED_DEADLINE：限期进程调度策略，使task选择 Deadline调度器 来调度运行； SCHED_RR：实时进程调度策略，时间片轮转，进程用完时间片后加入优先级对应运行队列的尾部，把CPU让给同优先级的其他进程； SCHED_FIFO：实时进程调度策略，先进先出调度没有时间片，没有更高优先级的情况下，只能等待主动让出CPU； SCHED_NORMAL：普通进程调度策略，使task选择 CFS调度器 来调度运行； SCHED_BATCH：普通进程调度策略，批量处理，使task选择 CFS调度器 来调度运行； SCHED_IDLE：普通进程调度策略，使task以最低优先级选择 CFS调度器 来调度运行； 1.3 运行队列: runqueue 每个CPU都有一个运行队列，每个调度器都作用于运行队列。分配给CPU的task，作为调度实体加入到运行队列中。task首次运行时，如果可能，尽量将它加入到父task所在的运行队列中（分配给相同的CPU，缓存affinity会更高，性能会有改善）；\nLinux内核使用 struct rq 结构来描述运行队列，关键字段如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 /* * This is the main, per-CPU runqueue data structure. * * Locking rule: those places that want to lock multiple runqueues * (such as the load balancing or the thread migration code), lock * acquire operations must be ordered by ascending \u0026amp;runqueue. */ struct rq { /* runqueue lock: */ raw_spinlock_t lock; /* * nr_running and cpu_load should be in the same cacheline because * remote CPUs use both these fields when doing load calculation. */ unsigned int nr_running; /* 三个调度队列：CFS调度，RT调度，DL调度 */ struct cfs_rq cfs; struct rt_rq rt; struct dl_rq dl; /* stop指向迁移内核线程， idle指向空闲内核线程 */ struct task_struct *curr, *idle, *stop; /* ... */ } 1.4 任务分组: task_group 利用任务分组的机制，可以设置或限制任务组对CPU的利用率，比如将某些任务限制在某个区间内，从而不去影响其他任务的执行效率。引入task_group 后，调度器的调度对象不仅仅是进程了，Linux内核抽象出了 sched_entity/sched_rt_entity/sched_dl_entity 描述调度实体，调度实体可以是进程或 task_group。\nLinux内核使用数据结构 struct task_group 来描述任务组，任务组在每个CPU上都会维护一个 CFS调度实体、CFS运行队列，RT调度实体，RT运行队列 。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 /* task group related information */ struct task_group { /* ... */ /* 为每个CPU都分配一个CFS调度实体和CFS运行队列 */ #ifdef CONFIG_FAIR_GROUP_SCHED /* schedulable entities of this group on each cpu */ struct sched_entity **se; /* runqueue \u0026#34;owned\u0026#34; by this group on each cpu */ struct cfs_rq **cfs_rq; unsigned long shares; #endif /* 为每个CPU都分配一个RT调度实体和RT运行队列 */ #ifdef CONFIG_RT_GROUP_SCHED struct sched_rt_entity **rt_se; struct rt_rq **rt_rq; struct rt_bandwidth rt_bandwidth; #endif /* task_group之间的组织关系 */ struct rcu_head rcu; struct list_head list; struct task_group *parent; struct list_head siblings; struct list_head children; /* ... */ }; 1.5 调度程序: schedule/\u0026hellip; 2 进程/线程调度 根据进程是否自愿放弃cpu，调度方式可分为主动调度和抢占调度两类，它们的区别如下：\n**主动调度：**进程需要等待IO、锁等资源，而主动放弃cpu；\n**抢占调度：**进程由于时间片用完，或被优先级更高的进程抢占，而被强制剥夺cpu；\n内核中那些由于等待资源而需要阻塞的场景，会直接调用 schedule() 执行实际的调度流程。而其它需要调度的场景一般都只是设置一个TIF_NEED_RESCHED标志，并在下一个抢占点到来时才执行实际的抢占操作。\n在支持内核抢占之前，只有在系统调用返回用户态之前，或者中断返回用户态之前才能执行抢占操作。而在支持内核抢占以后，即使在内核执行路径中也允许抢占，因此内核支持了更多的抢占点。\ntask_struct 结构体的首个字段放置的正是 struct thread_info，struct thread_info结构体中 flag 字段就可用于设置TIF_NEED_RESCHED，此外该结构体中的 preempt_count 也与抢占相关。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 struct task_struct { #ifdef CONFIG_THREAD_INFO_IN_TASK /* * For reasons of header soup (see current_thread_info()), this * must be the first element of task_struct. */ struct thread_info\tthread_info; #endif ... } /* * low level task data that entry.S needs immediate access to. */ struct thread_info { unsigned long\tflags;\t/* low level flags */ mm_segment_t\taddr_limit;\t/* address limit */ #ifdef CONFIG_ARM64_SW_TTBR0_PAN u64\tttbr0;\t/* saved TTBR0_EL1 */ #endif int\tpreempt_count;\t/* 0 =\u0026gt; preemptable, \u0026lt;0 =\u0026gt; bug */ }; #include \u0026lt;asm/current.h\u0026gt; #define current_thread_info() ((struct thread_info *)current) //通过该宏可以直接获取thread_info的信息 #endif 2.1 用户抢占 在抢占触发前，进程处于用户态，触发条件通常为：执行系统调用或时钟中断。\n内核即将返回用户空间的时候，如果need resched标志被设置，会导致schedule()被调用，此时就会发生用户抢占。在内核返回用户空间的时候，它知道自己是安全的。所以，内核无论是在从中断处理程序还是在系统调用后返回，都会检查need resched标志。如果它被设置了，那么，内核会选择一个其他 (更合适的) 进程投入运行。\n抢占触发点/条件设置 看看具体哪些函数过程中，设置了 TIF_NEED_RESCHED 标志吧：\n抢占执行点 具体来说，在用户态执行抢占在以下几种情况：\n异常处理 (系统调用) 后返回到用户态； 中断处理后返回到用户态； 用户程序在执行过程中，遇到异常或中断后，将会跳到 ENTRY(vectors) 向量表处开始执行； 返回用户空间时进行标志位判断，设置了 TIF_NEED_RESCHED 则需要进行调度切换； 2.2 内核抢占 当进程位于内核空间时，有一个更高优先级的任务出现时，如果当前内核允许抢占，则可以将当前任务挂起，执行优先级更高的进程。内核抢占对实时性有帮助。\n为什么Linux kernel需要内核抢占？\n内核抢占在一定程度上减少了对某种事件的响应延迟，这也是内核抢占被引入的目的。之前的内核中，除了显示调用系统调度器的某些点，内核其他地方是不允许中重新调度的，如果内核在做一些比较复杂的工作，就会造成某些急于处理的事得不到及时响应。针对内核抢占其实本质上也是对当前进程而言（不知道这么描述是否合适），因为内核是为用户程序提供服务，换言之，其本身不会主动的去执行某个动作。这里内核抢占，重点在于用户程序请求内核服务时，CPU切换到内核态在执行某个系统服务期间，被抢占。\n当然，即使支持内核抢占，也不是什么时候都可以的，还是要考虑对临界区的保护。类似于多处理器架构，如果进程A陷入到内核模式访问某个临界资源，而在访问期间，进程B也要访问临界区，如果这种抢占被允许，那么就发生了临界区被重入。所以，在访问临界资源时需要禁止内核抢占，在出临界区则要开启内核抢占。\npreempt_count 禁止内核抢占的情况列出如下：\n（1）内核执行****中断处理例程*时不允许内核抢占，中断返回时再执行内核抢占。 （2）当内核执行*软中断或tasklet*时，禁止内核抢占，软中断返回时再执行内核抢占。 （3）在临界区*禁止内核抢占，临界区保护函数通过抢占计数宏控制抢占，计数大于0，表示禁止内核抢占。\n为保证Linux内核在以上情况下不会被抢占，抢占式内核使用了一个变量preempt_ count，称为内核抢占锁。这一变量被设置在进程的PCB结构task_struct中。每当内核要进入以上几种状态时，变量preempt_count就加1，指示内核不允许抢占。每当内核从以上几种状态退出时，变量preempt_ count就减1，同时进行可抢占的判断与调度。抢占式Linux内核的修改主要有两点：\n**对中断的入口代码和返回代码进行修改。**在中断的入口内核抢占锁preempt_count加1，以禁止内核抢占；在中断的返回处，内核抢占锁preempt_count减1，使内核有可能被抢占。 **重新定义了自旋锁、读、写锁，**在锁操作时增加了对preempt count变量的操作。在对这些锁进行加锁操作时preemptcount变量加1，以禁止内核抢占；在释放锁时preemptcount变量减1，并在内核的抢占条件满足且需要重新调度时进行抢占调度。 【进程】preempt_count解析 - 知乎 (zhihu.com)\nLinux内核中使用 struct thread_info 中的 preempt_count 字段来控制内核抢占：\npreempt_count 的低8位用于控制抢占，当大于0时表示不可抢占，等于0表示可抢占； preempt_enable() 会将 preempt_count 值减1，并判断是否需要进行调度，在条件满足时进行切换； preempt_disable() 会将 preempt_count 值加1； 此外，preemt_count 字段还用于判断进程处于各类上下文以及开关控制等，如图：\n抢占触发点/条件设置 在内核中抢占触发点，也是设置 struct thread_info 的 flag 字段，设置 TIF_NEED_RESCHED 表明需要请求重新调度； 抢占触发点的几种情况，在用户抢占中已经分析过，不管是用户抢占还是内核抢占，触发点都是一致的； 抢占执行点 总体而言，内核抢占执行点可以归属于两大类：\n中断执行完毕后进行抢占调度； 主动调用preemp_enable或schedule等接口的地方进行抢占调度； 2.3 进程上下文切换 进程上下文切换的入口就是 __schedule()，分析也围绕这函数展开。\n主要的逻辑：\n根据CPU获取运行队列，进而得到运行队列当前的task，也就是切换前的prev; 根据prev的状态进行处理，比如pending信号的处理等，如果该任务是一个worker线程还需要将其睡眠，并唤醒同CPU上的另一个worker线程; 根据调度类来选择需要切换过去的下一个task，也就是next； context_switch完成进程的切换； context_switch() 的调用分析如下：\n核心的逻辑有两部分：\n进程的地址空间切换：切换的时候要判断切入的进程是否为内核线程 所有的用户进程都共用一个内核地址空间，而拥有不同的用户地址空间； 内核线程本身没有用户地址空间。在进程在切换的过程中就需要对这些因素来考虑，涉及到页表的切换，以及cache/tlb的刷新等操作。 寄存器的切换：包括CPU的通用寄存器切换、浮点寄存器切换，以及ARM处理器相关的其他一些寄存器的切换； 实际上进程的切换，带来的开销不仅是页表切换和硬件上下文的切换，还包含了Cache/TLB刷新后带来的miss的开销。\n","date":"2024-10-17T09:59:38+08:00","permalink":"https://zcxggmu.github.io/p/linux-task-schedule/","title":"Linux Task Schedule"},{"content":"0 资料 KVM虚拟机热迁移优化策略研究 - 中国知网 (cnki.net) KVM虚拟机热迁移算法分析及优化 - 中国知网 (cnki.net) QEMU/KVM源码解析与应用-李强编著-微信读书 (qq.com) x86 kvm和qemu虚拟化介绍-腾讯云开发者社区-腾讯云 (tencent.com) qemu live migration代码分析-腾讯云开发者社区-腾讯云 (tencent.com) 1 热迁移用法和基本原理 1.1 热迁移使用 虚拟化环境下的热迁移指的是在**虚拟机运行的过程中透明地从源宿主机迁移到目的宿主机，**热迁移的好处是很明显的，QEMU/KVM很早就支持热迁移了。早期的QEMU热迁移仅支持内存热迁移，也就是迁移前后的虚拟机使用一个共享存储，现在的QEMU热迁移已经支持存储的热迁移了。\n首先来看热迁移是怎么使用的。一般来说需要迁移的虚拟机所在的源宿主机（src）和目的宿主机（dst）需要能够同时访问虚拟机镜像，为了简单起见，这里只在两台宿主机上使用同一个虚拟机镜像。\n在src启动一个虚拟机vm1：\n在dst启动另一个虚拟机vm2：\n在vm1的monitor里面输入：\n隔了十几秒可以看到，vm2已经成为了vm1的状态，vm1则处于stop状态。\n总结一下：\nsrc启动qemu-kvm增加参数-incoming tcp:0:6666\ndst进去qemu monitor执行migrate tcp:$ip:6666，可以用info migrate查看信息\n如果是post copy执行migrate_set_capability postcopy-ram on，然后执行migrate_start_postcopy\nopenstack环境nova-compute通过libvirt操作qemu，可以用virsh qemu-monitor-command domain−−hmpcommand执行\n1.2 热迁移整体概述 在热迁移过程中，Guest OS完全无感，其运行的任务，在快速迁移过后能继续运行。\n首先，对于Guest OS从一个VM迁移到其他VM，涉及到对register配置，dirty cache，runtime context等数据的迁移。\n如下图Guest OS1从VM1到VM2迁移过程：\n先由qemu monitor发起live migration，PC1的QEMU进程执行precopy，对源Guest OS的register，dirty cache bitmap，runtime context进行备份，创建传输channel，包括RDMA, tcp socket（网络），unix socket（进程间），exec stdin/stdout（进程间），fd（虚拟文件）等类型。迁移过程中 dirty page bitmap由QEMU copy，send，recv，restore，SRC和DST的QEMU进程间负责交互。\nqemu中有两个概念 save_vm 和 load_vm，migration和snaptshot等都用到。\nsave_vm 把qemu cpu state，mem，device state保存在一个fd，这个fd可以本地文件也可以是socket等； load_vm 正好相反，把保存的信息恢复到虚拟机； 热迁移就是在dst启动一个虚拟机，把src虚拟机的发送过来的状态都恢复到对应位置。因为mem比较大，发送时间长，根据发送时机不同分为pre_copy 和 post_copy 。\npre_copy 先发送mem，达到一定阈值，停止src虚拟机运行，发送cpu state和device state，dst收到，然后运行； post_copy 先发送cpu state和device state，停止src虚拟机运行，dst标志page都无效，开始运行，qemu捕捉pagefault，然后从src请求page，src把page加入发送队列，dst等到这个page通知内核处理pagefualt然后继续运行。 postcopy 模式src背后还是一直默默给dst发送page的，只是dst等不着了一些page时插个队，优先发送要的page。总体来说，live migration状态多，线程多 (数据传递加保护，同步)，和kvm交互多 (log dirty page和userfault)，容易出错，可优化地方多。\n1.3 基本原理 热迁移要处理的内容氛围三部分：cpu state, ram和device state。\ncpu就一堆register和stack什么，VMCS定义的那些状态； ram是大头，包括ROM，PCI mem和DIMM等，要求按page访问的； device就多了，有寄存器，队列等，千差万别，肯定得自己实现save和load函数，然后register给migration流程。 数据结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 typedef struct SaveStateEntry { QTAILQ_ENTRY(SaveStateEntry) entry; char idstr[256]; int instance_id; int alias_id; int version_id; /* version id read from the stream */ int load_version_id; int section_id; /* section id read from the stream */ int load_section_id; SaveVMHandlers *ops; const VMStateDescription *vmsd; void *opaque; CompatEntry *compat; int is_ram; } SaveStateEntry; typedef struct SaveState { QTAILQ_HEAD(, SaveStateEntry) handlers; int global_section_id; uint32_t len; const char *name; uint32_t target_page_bits; } SaveState; static SaveState savevm_state = { .handlers = QTAILQ_HEAD_INITIALIZER(savevm_state.handlers), .global_section_id = 0, }; typedef struct SaveVMHandlers { /* This runs inside the iothread lock. */ SaveStateHandler *save_state; void (*save_cleanup)(void *opaque); int (*save_live_complete_postcopy)(QEMUFile *f, void *opaque); int (*save_live_complete_precopy)(QEMUFile *f, void *opaque); /* This runs both outside and inside the iothread lock. */ bool (*is_active)(void *opaque); bool (*has_postcopy)(void *opaque); /* is_active_iterate * If it is not NULL then qemu_savevm_state_iterate will skip iteration if * it returns false. For example, it is needed for only-postcopy-states, * which needs to be handled by qemu_savevm_state_setup and * qemu_savevm_state_pending, but do not need iterations until not in * postcopy stage. */ bool (*is_active_iterate)(void *opaque); /* This runs outside the iothread lock in the migration case, and * within the lock in the savevm case. The callback had better only * use data that is local to the migration thread or protected * by other locks. */ int (*save_live_iterate)(QEMUFile *f, void *opaque); /* This runs outside the iothread lock! */ int (*save_setup)(QEMUFile *f, void *opaque); void (*save_live_pending)(QEMUFile *f, void *opaque, uint64_t threshold_size, uint64_t *res_precopy_only, uint64_t *res_compatible, uint64_t *res_postcopy_only); /* Note for save_live_pending: * - res_precopy_only is for data which must be migrated in precopy phase * or in stopped state, in other words - before target vm start * - res_compatible is for data which may be migrated in any phase * - res_postcopy_only is for data which must be migrated in postcopy phase * or in stopped state, in other words - after source vm stop * * Sum of res_postcopy_only, res_compatible and res_postcopy_only is the * whole amount of pending data. */ LoadStateHandler *load_state; int (*load_setup)(QEMUFile *f, void *opaque); int (*load_cleanup)(void *opaque); } SaveVMHandlers; static SaveVMHandlers savevm_ram_handlers = { .save_setup = ram_save_setup, .save_live_iterate = ram_save_iterate, .save_live_complete_postcopy = ram_save_complete, .save_live_complete_precopy = ram_save_complete, .has_postcopy = ram_has_postcopy, .save_live_pending = ram_save_pending, .load_state = ram_load, .save_cleanup = ram_save_cleanup, .load_setup = ram_load_setup, .load_cleanup = ram_load_cleanup, }; void ram_mig_init(void) { qemu_mutex_init(\u0026amp;XBZRLE.lock); register_savevm_live(NULL, \u0026#34;ram\u0026#34;, 0, 4, \u0026amp;savevm_ram_handlers, \u0026amp;ram_state); } struct VMStateField { const char *name; const char *err_hint; size_t offset; size_t size; size_t start; int num; size_t num_offset; size_t size_offset; const VMStateInfo *info; enum VMStateFlags flags; const VMStateDescription *vmsd; int version_id; bool (*field_exists)(void *opaque, int version_id); }; struct VMStateDescription { const char *name; int unmigratable; int version_id; int minimum_version_id; int minimum_version_id_old; MigrationPriority priority; LoadStateHandler *load_state_old; int (*pre_load)(void *opaque); int (*post_load)(void *opaque, int version_id); int (*pre_save)(void *opaque); bool (*needed)(void *opaque); VMStateField *fields; const VMStateDescription **subsections; }; static const VMStateDescription vmstate_e1000； static void e1000_class_init(ObjectClass *klass, void *data) { dc-\u0026gt;vmsd = \u0026amp;vmstate_e1000; } int vmstate_register_with_alias_id(DeviceState *dev, int instance_id, const VMStateDescription *vmsd, void *base, int alias_id, int required_for_version, Error **errp); /* Returns: 0 on success, -1 on failure */ static inline int vmstate_register(DeviceState *dev, int instance_id, const VMStateDescription *vmsd, void *opaque) { return vmstate_register_with_alias_id(dev, instance_id, vmsd, opaque, -1, 0, NULL); } 其中，全局变量 savevm_state 是链表，ram和device把实现的save和load函数放在链表节点上，迁移时遍历链表执行一遍就OK了。但ram和device不同之处在于，ram用 SaveVMHandlers 结构，device用 VMStateDescription 结构，VMStateDescription 可以嵌套，实现基本数据类型的save和load操作。\npre_copy pre_copy 先处理ram，开始标志所有ram为dirty page，循环发送ram，同时CPU在执行写ram，每次循环从kvm获取CPU写过的ram，直到达到一个条件，停止CPU，发送剩下的ram，再发送CPU和device state。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 migrate_fd_connect { //创建cleanup bh用于migration结束时，结束时触发执行 s-\u0026gt;cleanup_bh = qemu_bh_new(migrate_fd_cleanup, s); //创建migration工作线程 qemu_thread_create(migration_thread) } migration_thread { qemu_savevm_state_setup { ram_save_setup { ram_init_all-\u0026gt;ram_init_bitmaps { ram_list_init_bitmaps memory_global_dirty_log_start migration_bitmap_sync-\u0026gt;kvm_physical_sync_dirty_bitmap } 创建线程compress_threads_save_setup } } while(true) { qemu_savevm_state_pending-\u0026gt;ram_save_pending-\u0026gt;migration_bitmap_sync migration_iteration_run { if(!threshhold) qemu_savevm_state_iterate ram_save_iterate-\u0026gt;ram_find_and_save_block else qemu_savevm_state_complete_precopy-\u0026gt;ram_save_complete //其它设备状态 vmstate_save_state } } migration_iteration_finish-\u0026gt;qemu_bh_schedule(s-\u0026gt;cleanup_bh); } migrate_fd_cleanup { qemu_savevm_state_cleanup-\u0026gt;ran_save_cleanup 停止线程 migration_thread } process_incoming_migration_co { qemu_loadvm_state { qemu_loadvm_state_setup-\u0026gt;ram_load_setup-\u0026gt;compress_threads_load_setup //创建线程do_data_decompress和compress_threads_save_setup对应 vmstate_load_state qemu_loadvm_state_main { case: qemu_loadvm_section_start_full-\u0026gt;vmstate_load_state case: qemu_loadvm_section_part_end-\u0026gt;vmstate_load_state } qemu_loadvm_state_cleanup } process_incoming_migration_bh } post_copy 为什么需要 postcopy，因为 pre_copy 有可能无法收敛，虚拟机运行的飞快，不断产生dirty page，fd比较慢发送不完，无法达到预定的条件。postcopy 就是先发送cpu和device state，停止执行，再慢慢发送ram，如果dst发现少page，再从src请求这个page。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 migrate_fd_connect { //创建cleanup bh用于migration结束时，结束时触发执行 s-\u0026gt;cleanup_bh = qemu_bh_new(migrate_fd_cleanup, s); /****************************************/ //如果是postcopy，创建收到page请求处理的线程 open_return_path_on_source-\u0026gt;qemu_thread_create(source_return_path_thread) /***************************************/ //创建migration工作线程 qemu_thread_create(migration_thread) } migration_thread { /******************************/ qemu_savevm_send_open_return_path qemu_savevm_send_ping qemu_savevm_send_postcopy_advise /******************************/ qemu_savevm_state_setup { ram_save_setup { ram_init_all-\u0026gt;ram_init_bitmaps { ram_list_init_bitmaps memory_global_dirty_log_start migration_bitmap_sync-\u0026gt;kvm_physical_sync_dirty_bitmap } 创建线程compress_threads_save_setup } } while(true) { qemu_savevm_state_pending-\u0026gt;ram_save_pending-\u0026gt;migration_bitmap_sync migration_iteration_run { if(!threshhold\u0026amp;!post_copy) /******************************/ if (postcopy_start()\u0026amp;\u0026amp;first) return; /*****************************/ qemu_savevm_state_iterate ram_save_iterate else migration_completion { if pre_copy qemu_savevm_state_complete_precopy-\u0026gt;ram_save_complete //其它设备状态 vmstate_save_state /******************************/ else if post_copy qemu_savevm_state_complete_postcopy-\u0026gt;-\u0026gt;ram_save_complete /******************************/ } } } migration_iteration_finish-\u0026gt;qemu_bh_schedule(s-\u0026gt;cleanup_bh); } migrate_fd_cleanup { qemu_savevm_state_cleanup-\u0026gt;ran_save_cleanup 停止线程 migration_thread } process_incoming_migration_co { qemu_loadvm_state { qemu_loadvm_state_setup-\u0026gt;ram_load_setup-\u0026gt;compress_threads_load_setup //创建线程do_data_decompress和compress_threads_save_setup对应 vmstate_load_state qemu_loadvm_state_main { case: qemu_loadvm_section_start_full-\u0026gt;vmstate_load_state case: qemu_loadvm_section_part_end-\u0026gt;vmstate_load_state /******************************/ //只有post_copy才执行这个case case: loadvm_process_command { case: loadvm_handle_cmd_packaged { //这儿递归了，只执行前两个case qemu_loadvm_state_main } case: loadvm_postcopy_handle_advise //用于接收请求返回的page case: loadvm_postcopy_handle_listen { //从kernel接收pagefault，然后发送src请求page qemu_thread_create(postcopy_ram_fault_thread) ///接收src给的page qemu_thread_create(postcopy_ram_listen_thread) } case: loadvm_postcopy_handle_run-\u0026gt;loadvm_postcopy_handle_run_bh //src启动发送page thread，src修改page然后停止，dst cpu执行 case: loadvm_postcopy_ram_handle_discard } /******************************/ } qemu_loadvm_state_cleanup } process_incoming_migration_bh } postcopy_ram_listen_thread { //只执行前两个case qemu_loadvm_state_main qemu_loadvm_state_cleanup } post_copy 相比 pre_copy 多了如下过程：\nsrc\n1 2 3 4 5 6 source_return_path_thread qemu_savevm_send_open_return_path qemu_savevm_send_ping qemu_savevm_send_postcopy_advise postcopy_start qemu_savevm_state_complete_postcopy dst\n1 2 3 4 5 loadvm_process_command和src同步，src告诉dst进入那一个步骤。 postcopy_ram_fault_thread(利用了USERFAULT机制让qemu知道了pagefault)-\u0026gt;migrate_send_rp_req_pages postcopy_ram_listen_thread //恢复ram，通过userfaultfd通知内核pagefault处理完成，还有其它fd通知共享内存的其它进程等流程 ram_load-\u0026gt;ram_load_postcopy-\u0026gt;postcopy_place_page-\u0026gt;qemu_ufd_copy_ioctl pre_copy 有可能无法收敛，达到一定时间就失败了，无法收敛是由于CPU dirty page产生过快，有人就想着让CPU执行慢一点； post_copy 的问题是一些情况是失败了无法恢复，pre_copy迁移失败，在src节点上照样可以恢复执行，而post_copy在dst上修改了状态，无法再同步给src，失败就恢复不了了。 //TODO: 2 热迁移流程分析 3 热迁移中的脏页同步 4 热迁移中的相关参数控制 ","date":"2024-10-17T09:58:09+08:00","permalink":"https://zcxggmu.github.io/p/vm-live-migration-2/","title":"Vm Live Migration 2"},{"content":"0 资料 KVM虚拟机热迁移优化策略研究 - 中国知网 (cnki.net) KVM虚拟机热迁移算法分析及优化 - 中国知网 (cnki.net) QEMU/KVM源码解析与应用-李强编著-微信读书 (qq.com) x86 kvm和qemu虚拟化介绍-腾讯云开发者社区-腾讯云 (tencent.com) qemu live migration代码分析-腾讯云开发者社区-腾讯云 (tencent.com) 1 虚拟化热迁移技术概述 虚拟机迁移包括两种 ， 即 虚拟机冷迁移 和 虚拟机热迁移 。\n**虚拟机冷迁移，**顾名思义 ，迁移时需要关闭虚拟机，停机时间比较漫长，业务连续性得不到保证。虚拟机冷迁移，首先需要关闭虚拟机， 然后通过离线网络将虚拟机的配置和磁盘镜像文件拷贝至目的主机，然后在目的宿主机上开启虚拟机，服务重新启动。由于业务中断的时间比较长，因此常用于非共享存储、跨数据中心的迁移。 **虚拟机热迁移，**又称为虚拟机动态迁移，主要指将一台虚拟机从一台服务器迁移至另外一台服务器上。在迁移过程中，虚拟机仍然正常运行，服务依然正常对外提供，用户感觉不到服务的中， 也就是说热迁移对于操作系统、应用 软件、虚拟机的用户来说，都是完全透明的。 1.1 热迁移性能指标 虚拟机热迁移给虚拟机的管理、维护带来了方便，但虚拟机热迁移本身也是需要占用资源的，我们该如何来评估热迁移呢？评估热迁移的指标主要包括总迁移时间 (total migration time) 、停机时间 (migration downtime) 、迁移传输数据量、对应用程序的性能影响等。\n**总迁移时间。**总迁移时间是指，从在源宿主机上发起迁移命令到虚拟机在目的宿主机上恢复运行所耗费的总时间。热迁移包括多个阶段，如内 存迭代拷贝、停机拷贝、虚拟机恢复等阶段，总迁移时间包括了这些阶段的总时间。由于热迁移技术往往用于负载均衡、服务器在线维护等， 这要求总迁移的时间能够尽可能短。热迁移本身对外界条件要求也高，一些突发情况（如网络中断）往往会导致迁移失败，这也要求迁移的时 间尽可能短。 **停机时间。**停机时间主要指，虚拟机在热迁移过程中，对外提供服务中断的时间。在这段时间內，虚拟机内部的应用程序、服务、网络处于停止状态，虚拟机不会响应外界的任何请求。停机时间是评估虚拟机热迁移的重要指标，特别是在互联网时代，很多业务的连续性、实时性要求都很高，这对虚拟机热迁移提出了更高的要求。 迁移数据传输量。迁移数据传输量，是指在热迁移过程中，在源宿主机和目的宿主机之间需要传输的数据总量。传输的数据主要包括迁移协议控制信息、内存数据、CPU状态数据、外设状态数据等，假如是块设备热迁移的话，还包括虚拟机虚拟磁盘数据。由于数据中心往往托管了上万台虚拟机，一台服务器上往往也有二三十台虚拟机，他们对网络资源的竞争较为激烈，迁移过程中，需要占用网络资源，假如同时迁移多台虚巧机，将占用更多的带宽资源。因此迁移时应尽量减少数据传输量。 **对应用程序的性能影响。**热迁移本身除了占用网络带宽资源，也需要占用一定的CPU资源，因此在迁移过程中，可能影响被迁移虚拟机的性能，也有可能影响源宿主机上的其他虚拟机。迁移过程中，应尽量减少对虚拟机应用程序的性能影响。 1.2 热迁移应用场景 虚拟机热迁移作为虚拟化技术的重要功能，主要用于负载均衡、能耗控制、服务器在线维护等领域。\n**负载均衡。**在数据中心，应该尽量保证服务的性能均衡，避免负载的过高或过低。负载假如过高，虚拟机竞争激烈，性能受到严重影响。反之负载过低，物理资源使用率不高，浪费计算机资源。通过虚拟机热迁移，我们可将虚拟机从负载高的机器迁移至负载低的机器，这样保证了虚拟机的性能，也提高了资源利用率。 **能耗控制。**利用热迁移，我们可将低负载服务器上的虚拟机迁移至同一物理机，然后关闭空闲出来的物理服务器，这样既可减少物理机的损耗，也可降低整个数据中心的能耗，提高了数据中心的资源利用率，从而达到节能减排的目的。 服务器在线维护。在数据中心，往往需要对硬件进行维修，如增加网卡、磁盘、内存等，有时候还需要对操作系统、应用软件进行升级，这时可先将上面的虚拟机热迁移至其他机器，使服务器在线维护成为可能，同时，也保证了虚拟机的性能稳定。 1.3 两种主流的热迁移算法 热迁移主要包括內存迁移、设备状态迁移、关闭虚拟机、恢复虚拟机几个阶段，根据这几个阶段的不同顺序，热迁移算法主要可分为两类，即预拷贝（pre-copy）热迁移算法和后拷贝（post-copy）热迁移算法。\n预拷贝热迁移算法: pre_copy 2005年，Clark等人首先在Xen平台上实现了虚拟机热迁移功能，它就属于预拷贝热迁移算法。预拷贝热迁移算法是目前学术界、工业界最成熟的虚拟机热迁移算法，主流的虚拟化平台Xen、KVM其热迁移算法都是基于预拷贝算法实现的。\n一台计算机的资源主要包括CPU、内存、外设，因此对虚拟机进行迁移也就是对这些资源进行迁移。预拷贝热迁移机制如下图所示：\n首先将所有的内存都拷贝过去，在拷贝过程中，虚拟机仍然在运行，会不停地修改内存页，记录这些被修改的内存页，进入迭代预拷贝阶段。在这一阶段，重复拷贝上一轮的脏页。随着迭代的进行，脏页将越来越少，当脏页低于某个阀值时，暂停源宿主机上的虚拟机，将剩余的脏页、CPU状态以及外设状态拷贝至目的宿主机，送一阶段称为停机拷贝阶段。最后在目的宿主机上恢复虚拟机的运行。假如在迁移过程中发生故障，则立即在源宿主机上恢复虚拟机运行。\n通常情况下，在迭代拷贝阶段，脏页会越来越少，但假如内存被频繁访问，或是网络带宽较低、网络拥堵，脏页往往达不到停止拷贝的阀值，这将导致迁移时间过长，甚至迁移永不结束等问题。\n后拷贝热迁移算法: post_copy 后拷贝热迁移算法最早于2009年，由Hinesiw在Xen平台上实现了，后拷贝热迁移算法更多的只是在学术界进行讨论，在工业界并没有大规模使用。后拷贝热迁移算法，相比于预拷贝热迁移算法，它将内存拷贝这一阶段放置在CPU状态的拷贝之后。如下图所示：\n迁移时首先暂停虚拟机运行，将虚拟机的CPU、外设状态拷贝至目的宿主机，然后直接在宿主机上恢复虚拟机的运行，将虚拟机的内存逐步拷贝至目的宿主机，在这过程中，虚拟机经常会出现缺页（PAGE_FAULT），这是由于所访问的页面还没有拷贝至目的宿主机，缺页时应该优先把所缺内存页拷贝至目的宿主机。后拷贝迁移算法，保证了所有的内存页最多拷贝一次，这避免了预拷贝时内存页的重复传输。\n后拷贝迁移算法，由于首先将CPU、外设状态拷贝至目的宿主机，随后直接在目的宿主机上恢复虚拟机的运行，因此其停机时间很短。但由于内存未拷贝至目的宿主机，因此运行过程中会产生缺页，缺页需要通过网络传输，这一过程中，虚拟机必须暂停运行。而网络速度与内存访问速度相差巨大，这势必导致迁移初期，虚拟机上的应用程序性能损失巨大，当然也有人提出了内存页预缓存等技术进行改善。后拷贝热迁移算法还存在一个严重的问题，由于迁移时，虚拟机的状态己经分成两部分，目的宿主机拥有CPU、外设状态、部分内存状态，而源宿主机拥有部分内存状态，假如网络中断、或是目的宿主机、源宿主机出现停机情况，不仅热迁移过程失败，虚拟机由于状态不完整，将会出现崩溃。\n2 基于x86的KVM虚拟化技术 虚拟机的迁移主要渉及到CPU、内存、外设三方面的迁移，而也正是这三方面构建了一台完整的虚拟机。毫无疑问，剖析KVM的虚拟化实现将有助于我们更好地理解虚拟机热迁移实现，为虚拟机热迁移优化做准备。KVM虚拟化的实现主要依靠内核KVM模块以及QEMU，其中KVM模块负责虚拟CPU、内存及部分中断功能，而QEMU则负责IO的虚拟化，本章节首先将剖析QEMU，介绍QEMU的多线程事件驱动模型，接着我们将剖析KVM模块，介绍KVM的CPU、内存虚拟化，最后介绍QEMU是如何实现IO虚拟化的。\n2.1 QEMU在KVM中的应用 QEMU是一个开源的快速模拟计算机，类似的模拟器还有Bochs、PearPC等，但QEMU具有高速度以及跨平台等特征。它支持在一种体系结构上（x86、ARM等）模拟另外一种体系结构，并在上面运行相应体系结构的操作系统、应用软件。QEMU支持两种工作模式，分别为系统模式（system mode）和用户模式（user mode）。\n在系统模式下，QEMU可模拟多种不同体系结构下完整的硬件平台，包括CPU、内存以及外围设备，在上面可运行未经修改的操作系统，是一台完整的虚拟机； 在用户模式下，主要应用于Linux环境下，它支持将某些CPU体系结构下的Linux程序运行在其他不同体系结构的CPU上，常用于交叉编译器的测试以及CPU模拟器的测试。 我们将主要讨论系统模式下的QEMU。\nQEMU多线程事件驱动模型 运行一台完整的虚拟机，需要运行客户机代码、处理时钟、处理IO，同时对用户的命令进行及时响应等。要及时完成这些工作，这需要一种良好的软件架构能够合理地协调各种资源，当一些比较耗时的任务（如磁盘IO模拟、热迁移虚拟机等）正在进行时，虚拟机不能停止，客户机的代码依然正常执行。\n对于这类需要处理多种资源请求的软件，业界有两种比较流行的架构：并行架构（Parallel architecture）和事件驱动架构（Event-driven archtecture）。QEMU使用了混合模式，即它的架构既采用了多线程，也采用了事件驱动，我们称它为多线程事件驱动模型。\n利用事件驱动，可以同时对多种事件进行监听响应； 利用多线程，可以充分利用服务器的多核结构，将一些耗时的任务，如虚拟机迁移，委托给专门的线程执行，这样就不会阻塞了整台虚拟机的运行。 QEMU事件驱动 QEMU的主线程，负责整个软件的事件驱动，主要借助函数 main_loop_wait。它主要用来执行以下任务：\n**执行poll操作，**查询文件描述符是否可读或可写。QEMU中检测的文件描述符主要包括以下几类： block io，虚拟磁盘相关的io，为了保证高性能，主要采用异步IO实现； eventfd，主要用于QEMU与KVM之间的通信交流； socket，主要用于虚拟机迁移、虚拟机生命周期管理等。 **执行到期的定时器。**QEMU里面有三种时钟：realtime clock、virtual clock、host clock。QEMU利用这三种时钟的定时器，执行一些周期性的任务，如利用定时器定期发出SIGALRM信号，通知QEMU处理异常和中断。 **执行下半部（bottom-halves）。**下半部本质上是一种回调函数。QEMU通过异步IO来模拟磁盘IO，QEMU截获了虚拟机的磁盘访问请求后，使用独立的线程模拟IO，当线程完成IO时，需要通知虚揪机，通知虚拟机的工作就放在下半部完成。虚拟机迁移任务，当虚拟机迁移完成时，需要做关闭套接字等清理工作，这部分工作也放在下半部中。 QEMU多线程 QEMU是一个多线程模型，如图所示：\n一个QEMU进程往往包括多个线程，如主线程、VCPU线程、VNC线程、工作线程、迁移线程：\n**主线程：**主要负责虚拟机的初始化，各种虚拟机设备的初始化，其他线程都由这个线程创建的。主线程在虚拟机运行过程中，运行着main loop，负责监听、响应各类文件描述符。 **VCPU线程：**负责模拟CPU，VCPU线程往往有多个，VCPU线程的个数等于虚拟机核数。VCPU线程负责客户机代码运行，既可以通过TCG机制翻译执行客户机代码，也可以通过KVM、Xen等硬件辅助虚拟化的方式执行。 VNC线程：主要负责计算密集型图像加码解码工作，鼠标、键盘的输入捕捉以及虚拟机的图形化界面都是由这个线程完成的，这算是一个特殊的工作线程。 **工作线程：**即worker thread，这一类线程主要负责IO模拟，QEMU通过线程池的方式模拟磁盘IO，磁盘IO请求组成一个队列，工作线程取出队列中的请求，进行模拟，工作线程根据负载情况有多个，动态创建、销毁。 **迁移线程：**当用户需要迁移虚拟机时，主线程会创建一个迁移线程，专门负责完成迁移工作。 客户机代码的执行 客户机代码的执行，指虚拟机操作系统、应用软件代码的执行，也就是CPU的虚拟化。QEMU中，客户机代码的执行主要由两种机制：TCG和KVM。\nTCG，全称为Tiny Code Generator，即微指令翻译器，最初是作为一个Ｃ编译器的通用后端，后面被简化后用于QEMU中。如下图所示：\nTCG可以分为三部分，即前端解码器、中端分析器、以及后端翻译器。其中：\n前端解码器读取客户机指令，根据客户机指令架构，将其翻译为中间码（TCG instructions，类似RISC指令的微指令）； 中端分析器读取翻译的中间码，进行优化，如去除一些冗余的指令； 优化后的中间码，输入后端翻译器，后端翻译器根据宿主机的CPU体系架构，将中间码翻译为宿主机目标代码； TCG借助动态翻译执行客户机源代码，可以实现跨CPU体系架构，但由于是翻译执行，因此QEMU的效率并不是很高，假如客户机的代码能直接在物理CPU上运行，效率可以极大提高。KVM就是以这种方式模拟CPU，KVM主要借助硬件辅助虚拟化技术，让客户机的代码直接在物理ＣPU上运行，性能接近裸机性能，我们接下来将剖析KVM的CPU虚拟化实现。\n2.2 CPU虚拟化 Intel的VT-x技术 传统的x86架构对虚拟化并不友好，x86处理器通过Ring级别控制访问权限，分为Ring0、Ring1、Ring2、Ring3，其中Ring0级别最高，可以执行特权指令，Ring3级别最低。如图所示：\n操作系统运行于Ring0级别，可以控制各种物理资源，而应用软件运行在Ring3级别，对资源的访问需要向操作系统请求。为了实现虚拟化，通常采用特权等级下降（Ring deprivileging）的方式实现。如上图所示，其中VMM运行于RING0级别，客户机操作系统运行于RING1级别，而客户机应用软件运行于RING3级别，但这种虚拟化实现方式存在内存地址空间缩小、部分指令不能虚拟化、RING Aliasing等问题。为了解决上述问题，提高虚拟化效率，Intel和AMD分别扩展了x86处理器的指令集，在硬件层面添加了部分虚拟化指令，称为硬件辅助虚拟化，Intel推出了VT-x技术，而AMD相应的推出了AMD-v技术，这里我们将主要分析Intel的VT-x技术。\nIntel拓展了原有的x86架构，引入了两种专门为虚拟化打造的操作模式，即VMX根操作模式和VMX非根操作模式，如图所示：\nVMX根操作模式类似于原有的x86架构，可像原先一样运行操作系统和应用软件，也可以运行虚拟机监视器；而VMX非根操作模式，则专门用于运行客户机操作系统，里面许多指令的特权级别都下降了。这两种CPU操作模式都有完整的四个RING级别，这样客户机的所有指令可以在原先的RING级别下执行，而虚拟机监控器也有足够的灵活度控制各类资源。\n由于CPU拥有了多种操作模式，因此VT-x技术也定义了与此相对应的状态切换指令，包括VMXON、VMXOFF、VMENTRY和VMEXIT。其中：\nVMXON和VMXOFF是分别用来开启和关闭该项技术； VMENTRY是指CPU从VMX根操作模式切换到VMX非根操作模式，客户机占有CPU，客户机操作系统及应用软件开始执行； VMEXIT是指CPU由于异常从VMX非根操作模式退回到VMX根操作模式，这时虚拟机监视器开始执行； 就像Linux进程切换时需要将上下文信息保存在进程控制块中，虚拟机监控器与客户机由于共享了处理器，因此也需要一个内存区域来自动保存和恢复执行客户机和宿主机的运行上下文。在Intel VT-x技术中，我们称该内存区域为虚拟机控制块（VMCS），送是一个极为关键的结构，通常占用一个page的大小，它由虚拟机监控器（VMM）分配，一个虚拟CPU（VCPU）需要分配一个VMCS，VMCS由硬件进行读写，类似于页表，因此速度极快。VMCS主要由以下六个部分组成：客户机状态区、宿主机状态区、执行控制域、VM-Entry控制域、VM-Exit控制域、VM-Exit信息域。\n其中客户机状态区和宿主机状态区用于保存客户机和宿主机上下文的状态； 执行控制域可以灵活的指定哪些指令和事件将导致VM Exit，通过IO bitmap可以自定义哪些IO访问将导致VM Exit； VM-Entry控制域和VM-Exit控制域主要用来控制发生VM-Entry和VM-Exit时对CPU模式的一些操作，如是处于32位还是64位模式； VM-Exit信息域主要记录VM-Exit产生的原因和具体信息。 通过VMCS，当发生VM-Entry时，硬件将自动从VMCS的客户状态区加载客户机操作系统的上下文，恢复客户机的执行；同时根据VM-Entry控制域的内容，决定向客户机塞入哪种中断或异常；在客户机执行期间，将根据执行控制域的信息，决定哪些指令和事件将触发VM-Exit；VMX-Exit时，将退出原因写入VM-Exit信息域，将虚拟机上下文保存至客户机状态域，加载宿主机状态域信息，然后根据VM-Exit的原因进行相应处理。\nKVM: CPU虚拟化实现 KVM用户接口 KVM是作为Linux内核的一个字符设备提供出来的，它包含两个内核模块 kvm.ko 和 kvm-intel.ko 或 kvm-amd.ko，其中kvm.ko 提供了核心虚拟化架构，而 kvm-intel.ko 和 kvm-amd.ko 与处理器的架构相关。\n当内核加载KVM模块时，将出现一个 /dev/kvm 的字符设备，一台完整的虚拟机由QEMU和KVM构成，如下图所示：\n​ QEMU 在用户空间模拟PCI总线、BIOS、VGA、键盘、鼠标、网卡、磁盘等外设，而KVM主要虚拟CPU和内存，QEMU通过一系列的 ioctl 系统调用来操作 /dev/kvm 完成对CPU和内存的虚拟化。\nKVM主要提供了三类接口：kvm_dev_ioctl/kvm_vm_ioctl/kvm_vcpu_ioctl，如下表所示：\nQEMU打开 /dev/kvm 设备后会获得一个KVM文件描述符，将这个文件描述符及相应的命令（如KVM_CREATE_VM）传入 kvm_dev_ioctl，会创建一台虚拟机，建立相应的数据结构，同时返回一个 kvm-vm 文件描述符； 通过 kvm_vm_ioctl 可以对这台虚拟机进行操控，比如创建VCPU、设置其内存空间、创建IRQCHIP、PIT、时钟等。创建VCPU后，会返回一个 kvm-vcpu 文件描述符； 将 kvm-vcpu 文件描述符传入 kvm_vcpu_ioctl 可以对虚拟CPU进行控制，比如可以运行CPU、获取设置CPU的设备寄存器、给CPU注入中断和事件等。 KVM虚拟机的创建和运行 QEMU属于用户态，它初始化完各种外设后，将开始调用KVM提供的用户态接口，创建虚拟机的CPU部分。QEMU首先会打开 /dev/kvm，会返回一个kvm fd，通过传递这个文件描述符以及KVM_CREATE_VM给 kvm_dev_ioctl，KVM将创建一台虚拟机，执行函数为 kvm_dev_ioctl_create_vm 函数，该函数会分配一个 kvm 结构体，一个 kvm 结构体就代表一台虚拟机，初始化该结构体，并将该结构体加入虚拟机链表中，同时返回用户态一个 kvm-vm 文件描述符。\n这时的虚拟机是一台缺少CPU的虚拟机。QEMU会根据虚拟机的CPU个数创建多个VCPU线程。每个VCPU线程将kvm-vm fd和KVM_CREATE_VCPU传递给 kvm_vm_ioctl 创建 vcpu，内核中将调用 kvm_vm_ioctl_create_vcpu 函数创建 vcpu，该函数将为每一个vcpu分配一个VMCS的结构，并进行初始化，并返回一个kvm-vcpu文件描述符给用户态。\nVCPU创建完成后，虚拟机就开始运行了。QEMU的VCPU线程主要包含一个主循环，QEMU通过KVM_RUN这个 ioctl 调用进入客户机。内核中的KVM部分具体调用函数 vcpu_enter_guest 进入客户机，其底层是通过虚拟化指令VMRESUME切换到VMX非根操作模式。客户机代码开始运行，直到遇到敏感指令或IO指令，这时将产生VM-Exit事件，客户机退出，由KVM进行异常处理。一般的异常，如对某些控制寄存器的访问、EPT缺页异常等在KVM这一层就可以得到处理，CPU将重新进入客户机模式执行，但还有一类异常，如部分IO指令，KVM不能处理，需要QEMU进行处理，因此将返回用户态，由用户态的QEMU进行处理，QEMU处理完IO后，将进入内核态，客户机又重新恢复运行，永远循环下去，整个执行过程如下图所示。\nKVM: CPU热迁移 VMCS这个结构体的客户机状态域保存了客户机CPU的上下文，因此CPU的迁移思路很简单。我们可以从VMCS获取所有客户机所有寄存器的状态，传递到目标机器后，再写入VMCS相应的域，恢复虚拟机的运行即可，而KVM也提供了相应的接口KVM_GET_REGS、KVM_SET_REGS等，可以供QEMU的迁移线程调用。\n2.3 内存虚拟化 Intel的EPT技术 为了保证客户机操作系统看到的内存空间布局与实际物理环境相一致，也就是客户机拥有从零开始、连续的内存空间，同时与其他虚拟机隔离，我们需要实现内存的虚拟化。在内存的虚拟化中，主要涉及到四种地址：客户机虚拟地址GVA、客户机物理地址GPA、宿主机虚拟地址HVA和宿主机物理地址HPA。对于虚拟机来说，它的内存物理地址空间要求必须是连续的，但对于宿主机来说，却并不是连续的。如下图所示，虚拟机的内存空间是由宿主机的若干段不连续的内存片段拼凑而成。\n客户机页表可以将GVA转换为GPA，但GPA不能直接用于宿主机的MMU（内存管理单元）进行寻址，必须将GPA转换为HPA，在KVM中有一个 kvm_memory_slot 的数据结构体，它记录了GPA与HVA的对应关系，可以实现GPA到HVA的转换，然后再通过宿主机的页表转换为HPA，进行实际的内存访问。\n传统的内存虚拟化是通过纯软件的方式实现的，即通过影子页表（Shadow Page Table）实现的，影子页表直接完成客户机虚拟地址空间到宿主机物理空间的映射。客户机在访存的时候，宿主机的MMU将直接加载客户机页表所对应的影子页表，从而实现GVA到HPA的直接转换，TLB缓存的也是GVA到HPA的转换。影子页表存在很大的缺陷，如：\n需要为客户机的每个进程都维护一套影子页表，因此会消耗较大的内存空间； 每一次发生VM-Exit、VM-Entry时，都会导致TLB的清空； 此外影子页表与客户机页表间的同步也比较复杂； 为此Intel推出了EPT技术，AMD也相应推出了NPT技术，为内存虚拟化提供硬件支持，二者原理相同。\nEPT技术在原有的客户机页表基础上，引入了EPT页表。客户机虚拟地址到客户机物理地址的转换借助客户机页表来完成，而客户机物理地址到宿主机物理地址的转换则借助EPT页表来完成，两次转换的机制相同，且都是由硬件自动完成的。如下图所示：\n客户机运行时，CR3 寄存器保存有客户机页表基址，负责载入客户机页表，EPTP 寄存器保存有EPT页表基址，负责载入EPT页表，两套页表实现了对内存地址的转换。在客户机运行过程中，客户机页表由客户机操作系统进行维护，客户机内部产生的缺页并不会导致客户机退出；而EPT页表则由虚拟机监视器（VMM）维护，EPT页表产生的异常，如缺页、写权限不足，将导致客户机退出，产生EPT异常，由虚拟机监视器（VMM）进行处理。EPT技术由于借助硬件方式实现，相比影子页表的软件方式，优势在：\n实现复杂度降低很多，且效率高； 客户机内部的缺页异常并不会导致客户机退出，提高了客户机的运行性能； 每一台虚拟机只需要维护一套EPT页表，也节省了许多内存空间； KVM: 内存虚拟化实现 KVM的內存虚拟机有两种方式实现，即影子页表和EPT技术，我们将主要探讨KVM如何使用EPT技术完成内存的虚拟化。\nQEMU内存模型 QEMU的内存模型，是指KVM虚拟机的内存是由QEMU在用户态进行申请管理，并将部分申请的内存注册到KVM模块中。这种模型的好处在于：\n策略与机制分离，加速的机制可以由KVM实现，QEMU只需负责如何调用； QEMU可以设置多种内存模型，如UMA、NUMA等； 方便QEMU对特殊内存的管理，如MMIO； 内存的分配、回收、交换都可以采用Linux原有的内存管理机制，KVM不需要单独开发。 QEMU主要通过AddressSpace这个数据结构来维护内存。AddressSpace包含一个MemoryRegion类型的变量，MemoryRegion存储了一个内存块的具体信息，包括该内存块的宿主机虚拟地址、该内存块的大小以及该内存块所对应的客户机物理地址。QEMU两个主要的MemoryRegion为 system_memory 和 system_io，即系统内存和MMIO。这些内存块通过KVM_SET_USER_MEMORY_REGION这个ioctl调用传递给KVM，KVM根据传递的信息建立一个kvm_memory_slot结构体，该结构体中的 base_gfn、npages、userspace_addr 分别代表客户机地址、页面数以及QEMU的用户态地址，这样就建立起客户机物理地址和宿主机虚拟机地址的映射关系，KVM可以开始管理这些内存块。\nEPT页表的构建 通过EPT技术，虚拟机访存时，借助客户机页表和EPT页表进行地址转换，且这种转换是由硬件自动完成的，效率极高。那么如何构建上述两种页表呢？\n其中客户机页表是由客户机操作系统负责构建，而EPT页表则由KVM负责构建。和普通页表的建立过程类似，EPT页表主要通过缺页异常进行处理逐步建立的。\n客户机正常运行，当遇到缺页异常时，客户机并不会退出非根操作模式，而是由客户机缺页异常处理程序进行处理，它将为客户机分配一个客户机物理内存页，并将客户机物理地址填写至客户机页表，这时候GVA和GPA的映射关系建立了，但完成一次完整的地址转换，还需要将GPA转换为HPA，因此将访问EPT页表，这时EPT页表为空，虚拟机将退出非根操作模式，并产生一个 ept_violtion 异常，KVM截获到该异常，将调用相应的异常处理函数，完成EPT页表的建立。KVM建立EPT页表的过程如下图所示：\n可以看到，虚拟机由于EPT缺页异常退出后，将由 handle_ept_violation 这个函数去处理该异常，该函数将调用 tdp_page_fault 这个函数去处理EPT缺页异常，具体的处理过程由 gfn_to_pfn 和 __direct_map 函数完成。其中：\ngfn_pfn 完成GPA到HPA的转换，该过程由 gfn_to_hva 和 hva_to_pfn 两个函数完成。\ngfn_to_hva 通过kvm_memory_slot得到GPA到HVA的映射； hva_to_pfn 可完成HVA到HPA的转换，最终得到GPA到HPA的映射。 __direct_map 负责建立具体的EPT页表项，EPT页表和客户机页表一样也是四级页表，流程如下：\n__direct_map 将从EPT页表基地址，即第４级页表开始遍历，假如当前层是最后一层（第１级），则直接调用 mmu_set_spte 设置页表项，__direct_map 函数结束； 反之假如当前层不是最后一层，且该层EPT页表项没有初始化，则调用 kvm_mmu_get_page 申请一个EPT页表，调用 link_shadow_page 新申请的EPT页表添加到未初始化的页表项，接着遍历下一级页表，直至遍历至最后一级页表（第１级）。 KVM: 内存热迁移 内存的热迁移是热迁移中最耗费时间、最容易出错的地方。\n内存热迁移最简单的方式，可以选择开始时就挂起虚拟机，然后拷贝所有的内存到目的宿主机上，再恢复运行虚拟机，但这样会导致停机时间较长。为了减少停机时间，我们必须在虚拟机挂起前就完成大部分内存的拷贝任务。典型思路是首先将虚拟机的所有内存拷贝过去，但在拷贝过程中，虚拟机访问内存产生脏页，为了保证虚拟机的状态一致，这部分脏页必须重新拷贝过去，随着一轮一轮的拷贝，写脏的内存页越来越少，当低于某个阀值时，挂起虚拟机，将脏页传输过去，在目的宿主机上恢复虚拟机的运行。这个思路的关键在于**如何记录哪些内存页被写脏了，**KVM为我们提供了相应的用户接口：\n通过传递KVM_MEM_LOG_DIRTY_PAGES给这个 ioctl(KVM_SET_USER_MEMORY_REGION) 调用，就可以开启内存脏页记录，本质上是通过对客户机页表写保护（write protection），当客户机写内存页时，会产生write fault，从而记录脏页； 此外KVM还提供获取脏页位图信息的接口，即通过KVM_GET_DIRTY_LOG这个ioctl调用，我们可以获取虚拟机的的脏页位图。 2.4 IO虚拟化 IO全虚拟化 KVM的IO虚拟化由QEMU和KVM模块配合完成。其中IO设备，如显示器、鼠标、键盘、磁盘、网卡、声卡、光驱等设备由QEMU在用户态进行模拟，QEMU为这些设备建立相应的数据结构，而KVM则负责拦截虚拟机的IO请求，将这些请求传递给QEMU，由QEMU去模拟IO执行，整个IO处理流程如下图所示：\n首先KVM虚拟机执行IO指令，如读取虚拟磁盘的某个sector，发生VM-Exit，CPU同时会将退出的原因写入VMCS结构体。KVM捕获到该IO指令，通过VMCS知道是由于IO操作引起虚拟机退出。KVM首先会在内核态遍历注册的设备，看看能否在内核态处理该请求，假如能在内核态完成处理，处理后返回虚拟机内部。若不能处理，则返回到用户态，由QEMU去模拟相关的IO请求。最后QEMU将通过KVM_RUN这个ioctl重新进入虚拟机。\nIO半虚拟化 上述传统的IO虚拟化方法，我们需要为虚拟机提供一个完整的虚拟设备。虚拟机操作系统通过标准的IO接口对虚拟设备进行操作，KVM截获IO指令，发生VM-Exit，由QEMU去模拟。这种方式的好处是，由于QEMU模拟的设备都是现在物理机已有的通用设备，因此虚拟机操作系统可以使用原先的驱动直接操作虚拟设备，但由于需要截获每一条IO请求，当虚拟机IO繁忙时，VMX模式会发生频繁的切换，导致IO性能低下。\n为了解决上述问题，virtio驱动被引入，这是一种通用的半虚拟化驱动。如下图所示：\nvirtio主要由三部分组成：virtio前端驱动、virtio后端驱动以及virtio_ring。\nvirtio前端驱动位于虚拟机操作系统内部，Linux内核默认集成了virtio前端驱动，而windows虚拟机需要手动安装virtio前端驱动； virtio后端驱动位于VMM中，准确来说virtio后端驱动位于QEMU中； virtio_ring是一个共享缓冲区，对于虚拟机操作系统和VMM来说都是可见的； 当虚拟机需要做IO（磁盘、网络等）时，通过virtio前端驱动将IO Request放入virtio_ring，虚拟机并不会立即发生VM-Exit，当IO Request达到一定数目或virtio_ring满时，虚拟机执行一次kick，虚拟机发生VM-Exit，通知VMM处理IO Requests。随后virtio后端驱动从virtio_ring取出IO请求，逐一进行模拟，完成请求后再通知虚拟机。\nvirtio驱动避免了虚拟机的频繁退出，性能得到显著提升。值得注意的是，virtio是一套通用框架，针对每一种外设（磁盘、网络设备），都有具体的实现。目前己经实现了磁盘 virtio-blk、网络设备 virtio-net、pci设备 virtio-pci、控制台设备 virtio-console、balloon设备 virtio-balloon 等，你也可以利用这套框架自己实现一个设备。\nKVM: IO热迁移 KVM虚拟机的外设是由QEMU模拟的，外设的状态都有相应的数据结构保存，因此IO的迁移比较简单，只需要保存设备的状态后，传递到目的宿主机上恢复即可，值得注意的是两类IO资源：磁盘和网络。\n首先磁盘，磁盘拥有大量的数据，我们可以采用共享存储的方式，避免了磁盘数据的大量迂移，对于停机后剩余的IO请求，QEMU会完成这些IO请求后再进行设备迁移； 网络迁移要解决的问题是，迁移后整个局域网要迅速感知到这种变化，将数据包重定向到新的宿主机上，这个可以通过目的宿主机主动发送ＡARP，实现虚拟机IP与目的宿主机MAC地址的重新绑定； 3 KVM虚拟机热迁移实现 3.1 KVM热迁移的内容 虚拟机的热迁移需要将一台正在运行的虚拟机从一台宿主机完整地迁移到另外一台宿主机上，且迁移期间，虚拟机对外提供的服务不能中断。为了确保虚拟机热迁移后依然能够正常运行，我们必须传递尽可能多的状态信息，如CPU、内存、各种IO设备等。下面我们将分别介绍KVM虚拟机CPU、内存、IO设备的迁移。\nCPU热迁移 KVM虚拟机中的CPU虚拟化是由KVM内核模块完成的，准确的来说，是QEMU调用KVM模块完成了CPU的虚拟化。在QEMU中，有一个重要的数据结构体CPUState代表了CPU的状态，CPUState有一个数据成员为 kvm_run，这个数据成员是对kvm-vcpu文件描述符进行一个 mmap 操作获得的，也就是VMCS结构体，而VMCS结构体的客户机状态域保存了KVM虚拟机CPU的运行状态，也就是说 kvm_run 这个结构体保存了CPU的运行状态。因此CPU的迁移比较简单，只需保存CPUState这个结构体的所有信息，传递给目的宿主机上，然后重新恢复，即可恢复虚拟机CPU上下文。\n内存热迁移 内存的热迁移是热迁移中最耗费时间、最容易出错的地方。内存的热迁移可以分为 push 、stop-and-copy 、pull 个阶段：\npush 阶段：该阶段，虚拟机依然在源宿主机上运行，对外正常提供服务。后台则借助高速网络将内存传输至目的宿主机。由于虚拟机在运行过程中，会不断产生脏页，为了确保虚拟机迁移前后状态一致，需要将重新传输脏页。 stop-and-copy 阶段：该阶段，首先挂起虚拟机，然后拷贝剩余的内存脏页至目的宿主机上，拷贝完成后，直接在目的宿主机上恢复虚拟机的运行。 pull 阶段：将CPU和设备状态拷贝至目的宿主机上后，直接恢复虚拟机的运行，由于未拷贝内存，因此虚拟机这时会产生缺页，缺页将通过高速网络传输至目的宿主机。 内存的热迁移算法可以同时包括上述全部阶段，但在生产环境中的热迁移算法往往只包含上述一个或两个阶段。例如，纯粹的 stop-and-copy 迁移算法需要先暂停虚拟机，拷贝所有内存至目的宿主机上后，再恢复虚拟机运行。这种策略的好处是迁移很简单，但停机时间就等于迁移时间，内存越大，停机时间往往成正比例增长，对于那些要求高可用性的应用来说，这显然不可接受。\n而 post-copy 算法，结合 stop-and-copy 和 pull 两个阶段。该算法首先经历一个 stop-and-copy 阶段，挂起虚拟机后，将CPU和外设的状态传送至目的宿主机，然后直接在目的宿主机上恢复虚拟机的运行，虚拟机这时会产生缺页异常，开始进入 pull 阶段，通过网络传输缺页以及未拷贝的内存页。该算法的好处是，可以做到很短的停机时间，但由于会产生大量的缺页异常（PAGE_FAULT），这些缺页都通过网络传输，因此虚拟机的性能将出现大幅度下降。\nKVM的内存热迁移算法属于预拷贝算法（pre-copy），该算法由 pull 和 stop-and-copy 两个阶段构成：\n迁移时，首先进入 pull 阶段，该阶段采用迭代拷贝的方式拷贝内存至目的宿主机上。首轮需要拷贝所有的内存页，由于虚拟机仍然在运行，会产生脏页，因此接下来每一轮都将拷贝前一轮所产生的脏页。随着迭代的增加，脏页将越来越少（前提是脏页产生的速率要低于脏页传输的速率），当脏页数量收敛到某个阀值时，进入 stop-and-copy 阶段。 在 stop-and-copy 阶段，首先挂起虚拟机，然后将剩余的脏页及CPU状态、IO设备状态拷贝至目的宿主机。拷贝完成后，在目的宿主机上恢复虚拟机的运行，热迁移顺利结束。 IO设备的迁移 IO设备的迁移包括磁盘、网络、鼠标、键盘等的迁移，这些设备都是由QEMU软件模拟的。KVM在热迁移虚拟机时，要求在目的宿主机上创建一台和原虚拟机配置相同的虚拟机，包括所有的IO设备，这就保证了IO设备完全相同，因此IO设备的迁移是需要把IO设备的运行状态（如某些控制寄存器的状态）拷贝至虚拟机即可。其中磁盘、网络这两类资源需要格外关注。\n磁盘的迁移 磁盘的迁移准确来说应该包括磁盘控制器和磁盘镜像的迁移。\n磁盘控制器的迁移只包括少量运行状态； 磁盘镜像一般比较大，至少十几GB，磁盘镜像的迁移往往需要占据大量的带宽和时间； KVM的热迁移本身是支持磁盘镜像的热迁移的，但实际在构建数据中心时，虚拟机的磁盘镜像往往是共享的，即将磁盘镜像放在共享的存储设备上。磁盘镜像可以存放在商用的NAS设备，也可以放在开源的分布式存储上面，如Ceph、Sheepdog等。\n网络的迁移 对于网络资源，我们要求在迁移过程中，所有的网络状态，如已建立的TCP连接以及IP地址等都能够维持不变。因此，目的宿主机和源宿主机必须位于同一局域网以内，这就保证了迁移后IP地址能保持不变。迁移后，与该IP相关的数据包也必须转发至目的宿主机，需要将IP地址和目的宿主机的MAC地址进行绑定。因此在迁移结束时，目的宿主机将发送一个ARP广播，通知局域网内的其他设备，该虚拟机己经迁移至新的位置。\n3.2 KVM热迁移的具体实现 KVM热迁移流程 KVM虚拟机热迁移工作由用户态的QEMU和KVM内核模块配合实施的。迁移开始前，需要在目的宿主机创建一台和原虚拟机配畳相同的虚拟机，该虚拟机负责接收待迁移虚拟机的所有状态。用户发起请求后，原虚拟机将创建一个专门的迁移线程负责迁移，而新的虚拟机将创建一个协程（Coroutine），负责接收、恢复虚拟机，迁移具体步骤可以分为四个阶段，即资源预留（Reservation）、迭代预拷贝（iterative pre-copy）、停机拷贝（stop-and-copy）、激活（Activation），如下图所示。\n我们将源宿主机称做主机Ａ，目的宿主机称做主机Ｂ。\n**资源预留：**该阶段的工作通常由第三方软件（如 libvirt ）或人工完成，即在目的宿主机上创建和待迁移的虚拟机配置相同的虚拟机，该虚拟机一启动就被暂停，无法继续运行，同时开始监听外界状态； **迭代预拷贝：**该阶段虚拟机依旧在主机Ａ上运行，同时开启了KVM的脏页日志功能。专门的迁移线程（migration thread）负责将内存迭代地拷贝至主机Ｂ上，第一轮拷贝所有内存页，接下来的每一轮将拷贝上一轮的脏页至主机Ｂ。 **停机拷贝：**挂起主机Ａ上的虚拟机，将剩余的脏页、CPU状态以及各种设备状态拷贝到主机Ｂ上。 **激活：**激活主机Ｂ上的虚拟机，同时广播一个ARP包，通知局域网内的交换机和其他计算机，虚拟机已迁移至主机Ｂ上。 重要数据结构 下面将介绍KVM虚拟机热迁移过程中涉及到的主要数据结构以及函数，由它们负责实施热迁移：\nmigration_thread：迁移线程函数，原虚拟机接受迁移请求后，将用这个函数创建一个线程，完成迁移工作。 process_incoming_migration_co：迁移接收协程（Coroutine）函数，目的宿主机将用这个函数创建一个协程，负责接收虚拟机的状态并恢复。 MigrationState：这是一个全局变量结构，负责保存迁移时的各种状态，如总迁移时间、停机时间、拷贝数据量、迭代次数、迁移所处阶段（MIGRATION_STATUS_SETUP、MIGRATION_STATUS_ACTIVE等），同时还包含 QEMUFile 这个成员。 QEMUFile：该结构体负责虚拟机状态数据的传输，它绑定了一个套接字（Socket），专门用于支持网络传输。QEMUFile包含两个主要成员buf[IO_BUF_SIZE] 和 QEMUFileOps，其中： buf 是一个缓冲区，可以往该缓冲区读取或写入要传输的数据； QemuFileOps 定义了一系列针对QEMUFile的操作，包括读取、写入QEMUFile的buf，以及发送buf中内容、接收数据到buf等； migration_bitmap：脏页位图，QEMU每一轮迭代都将从KVM获取内存脏页信息并存入migration_bitmap，一个bit代表一个内存页，4G内存大概需要128kb的migration_bitmap。 VMStateDescription：KVM虚拟机中的虚拟设备（CPU、网卡、磁盘、BIOS）都有一个相关的结构体，这个结构体定义了一系列的VMStateField，即设备的状态，同时还定义了相应的保存和恢复函数，用于支持设备状态的保存和恢复。 savevm_handlers：一个SaveStateEntry类型的链表，每一种设备（包括内存）都有一个SaveStateEntry，包含 name、instance_id、ops 和 vmsd 等成员，其中只有内存和磁盘的 ops 不为空，ops 定义了内存、磁盘的热迁移各个阶段的handlers（包括保存和恢复）。其余设备状态的保存和恢复借助 vmsd 这个变量，其类型正是前面提到的 VMStateDescription。 loadvm_handlers：—个LoadStateEntry类型的链表，包含了成员变量 se，有趣的是 se 也是一个SaveStateEntry类型的变量，内存的恢复、设备状态的恢复主要是借助 se 来完成的，也就是说其实savevm_handlers完成设备状态的恢复，估计为了代码的易读性、统一性，将 SaveStateEntry 包装成 LoadStateEntry，并定义出loadvm_handlers。 脏页位图 内存迁移的关键步骤就是记录脏页，即记录上一轮预拷贝中被修改过的内存页。\n这个功能由 migration_bitmap_sync 函数完成，将脏页信息保存到 migration_bitmap 这个变量中，供迁移使用。migration_bitmap_sync 函数底层是通过KVM_GET_DIRTY_LOG这个ioctl调用，向内核KVM模块获取脏页信息。\n现阶段，KVM的脏页日志机制为：对影子页表进行写保护（write protection），当客户机要修改内存时，将会产生写错误（write fault），这时虚拟机会退出，由KVM进行处理，KVM将检查影子页表对应的客户机PTE的访问权，假如该内存页可写，则KVM负责将对应影子页表PTE改为可写权限，同时记录脏页位置。现在在最新的EPT技术里面，有硬件记录脏页的功能，称为PML（Page Modification Logging），该功能直接由硬件实现，避免了原有机制下虚拟机的频繁退出，提高了虚拟机的运行性能。\n迭代预拷贝阶段 KVM虚拟机热迁移功能由 migration_thread 函数完成，该函数位于文件 migration.c 中，代码整体流程如图所示。\n其中 qemu_savevm_state_begin 函数负责迁移前期的初始化工作，初始化MigrationState结构体，其状态变为MIG_STATE_SETUP，开启KVM的脏页记录功能，并为 migration_bitmap 分配内存空间，初始化 migration_bitmap 的所有位都为1，即代表第一轮所有内存均为脏页，MigrationState状态变为MIG_STATE_ACTIVE，进入迭代预拷贝阶段。\n迭代预拷贝由 qemu_savevm_state_pending 和 qemu_savevm_state_iterate 这两个函数完成：\n首先看 qemu_savevm_state_pending，该函数将遍历savevm_handlers链表，调用每一种设备的 save_live_pending 函数，只有内存和磁盘设备定义了该函数，其余设备这个函数为空。对内存来说，将调用 ram_save_pending 函数，该函数首先将调用 migration_bitmao_sync 函数，去同步脏页位图，同时将返回脏页的数据量，即 pending_size。进入内存拷贝传输的阶段之前，会进行一次判断，即 pending_size 和 max_size 的大小。max_size 是进入停机拷贝阶段的最大脏页数据量，它是根据最大允许停机时间（max_downtime）和带宽（bandwidth）计算出来的，若 pending_size 小于 max_size，则进入停机拷贝阶段。max_size 计算公式如下：\nmax_size = max_downtime * bandwidth\n具体的迭代拷贝工作由 qemu_savevm_state_iterate 函数完成，这个函数将调用内存的 ram_save_iterate 函数，该函数是一个循环函数，负责对内存的迭代拷贝，调用 migration_bitmap_find_and_reset_dirty 函数查找migration_bitmap中下一个脏页，save_block_hdr 函数负责传输这个脏页，将其写入QEMUFile的缓冲区，缓冲区满时，批量传输内存页到目的宿主机上去。当 pending_size 小于 max_size 时，迁移进入停机拷贝阶段。\n停机拷贝阶段 停机拷贝工作主要由 vm_stop_force_state 和 qemu_savevm_state_complete 两个函数完成。\n首先由 vm_stop_force_state 函数完成停机操作，注意这里的停机操作并不是关机，停机主要是让CPU不再切换至非根操作模式运行，此时客户机的上下文环境己经保存在VMCS这个虚拟机控制块中了。通过设置QEMU中的CPU结构体的 stop 标志为True，vcpu线程调用ioctl(KVM_RUN) 之前会检查这个flag，若为True，则不会调用 ioctl(KVM_RUN) 下陷，虚拟机将不会运行。 接下来的工作便是拷贝剩余的脏页数据，以及CPU、外设的运行状态至目的宿主机，该工作主要由 qemu_savevm_state_complete 去执行。这个函数将遍历savevm_handlers两次。 第一次遍历主要是为了发送剩余内存脏页，通过执行内存的 ram_save_complete 函数，该函数首先会调用 migration_bitmap_sync 同步脏页位图，然后发送剩余的内存脏页； 第二次遍历主要是为了同步CPU、外设运行状态，由 vmstate_save 函数执行，该函数将调用CPU以及其他外设在初始化时注册的状态保存函数，完成设备状态的迁移； ","date":"2024-10-17T09:58:02+08:00","permalink":"https://zcxggmu.github.io/p/vm-live-migration-1/","title":"Vm Live Migration 1"},{"content":"0 参考 1 背景 基于[v9, 00/10] riscv: support kernel-mode Vector，目前v6.7主线未合入。\n从多hart多task角度出发，返回用户态前，hart上装载的如果不是即将被调度task的vector_state是否能感知到？\nhart视角下：TIF_RISCV_V_DEFER_RESTORE没问题 task视角下：如果task还被调度到原hart上时，原hart的vector_state不变，原实现感知不到。 2 框架 为了减少不必要地保存和恢复vector状态的次数，内核需要跟踪两件事：\na) 对于每个任务，内核需要记住最后一个将任务的vector状态加载到寄存器上的hart是哪一个；\nb) 对于每个hart，内核需要记住最近加载到寄存器上的用户态vector状态属于哪个任务，或者在此期间是否已被用于执行内核模式vector操作。\n对于a），向 thread_struct 添加了一个 vector_cpu 字段，每当vector状态被加载到hart上时，该字段会更新为当前hart的ID。 对于b），添加了per-hart变量 vector_last_state，其中包含最近加载到hart上的任务的用户空间vector状态的地址，如果在此之后执行了内核模式vector，则为NULL。 基于以上，我们在任务切换时就不再需要立即恢复下一个vector状态，可以将这个检查推迟到用户空间的恢复阶段，在这个阶段需要验证hart的 vector_last_state 和任务的 vector_cpu 是否仍然保持同步。如果同步，就可以省略vector的恢复操作。\n为了更好地描述上述的 task-hart 的双向同步机制，使用统一线程标识 TIF_FOREIGN_VSTATE 来指示当前任务的用户态vector状态是否存在于hart中。如果当前hart的vetor寄存器包含当前任务的最新用户态vector状态，不设置该标志，否则设置。\n对于某个任务，其可能的执行序列如下：\n**任务被调度：**如果任务的vector_cpu字段包含当前hart的ID，且hart的 vector_last_state per-cpu变量指向任务的vector_state，TIF_FOREIGN_VSTATE 标志位被清除，否则被设置； **任务返回到用户空间：**如果设置了 TIF_FOREIGN_VSTATE 标志，任务的用户空间vector状态将从内存复制到寄存器中，任务的vector_cpu字段将设置为当前hart的ID，当前hart的 vector_last_state 指针将设置为该任务的vstate，并清除 TIF_FOREIGN_VSTATE 标志； **该任务执行一个普通的系统调用：**当返回到用户空间时，TIF_FOREIGN_VSTATE 标志仍将被清除，因此不会恢复vector状态； **该任务执行一个系统调用，该系统调用执行一些vector指令：**在此之前，调用 kernel_vector_begin() 函数，将任务的vector寄存器内容复制到内存中，清除vector_last_state变量，并设置 TIF_FOREIGN_VSTATE 标志； **在调用kernel_vector_end()之后，任务被抢占：**由于我们还没有从第二个系统调用中返回，TIF_FOREIGN_VSTATE仍然被设置，因此vector寄存器中的内容不会被保存到内存中，而是被丢弃。 task0首次被调度：\n判断是否保持同步:\nTIF_FOREIGN_VSTATE = (task0-\u0026gt;vector_cpu != hart0 || vector_last_state != task0)\ntask0返回用户态：\n* 判断TIF_FOREIGN_VSTATE，这里为TRUE， 那就恢复vector_state到寄存器上； * task0-\u0026gt;vector_cpu = hart0; * vector_last_state = task0; * TIF_FOREIGN_VSTATE = false;\ntask0让出CPU控制权\ntask0再次被调度运行，目标CPU仍然为hart0：\n还是判断和1）相同的两个变量，看是否同步，此时： task0-\u0026gt;vector_cpu = hart0； vector_last_state = task0; =\u0026gt; TIF_FOREIGN_VSTATE = false;\ntask0再次返回用户态：\ntask0再次被调度运行，目标CPU仍然为hart0：\n后续工作：\n考虑能否基于原 DEFER 进程标识实现； 依次验证进程调度、内核模式vector操作、信号处理等非常虚拟化场景下的开销； 接入KVM： 分析现有 vcpu_load/restore 路径上的vector切换是否存在问题； 关于 TIF_FOREIGN_VSTATE 的设置时机； ","date":"2024-10-17T09:54:23+08:00","permalink":"https://zcxggmu.github.io/p/fpsimd-context-switch/","title":"Fpsimd Context Switch"},{"content":"0 前言 Kernel版本：4.14 ARM64处理器：Contex-A53，双核 之前的文章中主要分析了中断控制器驱动以及Linux中断框架，本文将重点关注Linux内核针对中断的一些优化设计机制，包括 Top-half/Bottom-half、Softirq、Workqueue 等机制。\n1 Softirq/tasklet 1.1 softirq 什么是软中断 中断请求的处理程序应该要短且快，这样才能减少对正常进程运行调度地影响，而且中断处理程序可能会暂时关闭中断，这时如果中断处理程序执行时间过长，可能在还未执行完中断处理程序前，会丢失当前其他设备的中断请求。\nLinux 系统为了解决中断处理程序执行过长和中断丢失的问题，将中断过程分成了两个阶段，分别是「上半部和下半部分」。\n上半部用来快速处理中断，一般会暂时关闭中断请求，主要负责处理跟硬件紧密相关或者时间敏感的事情。 下半部用来延迟处理上半部未完成的工作，一般以「内核线程」的方式运行。 举一个计算机中的例子，常见的网卡接收网络包的例子。\n网卡收到网络包后，通过 DMA 方式将接收到的数据写入内存，接着会通过硬件中断通知内核有新的数据到了，于是内核就会调用对应的中断处理程序来处理该事件，这个事件的处理也是会分成上半部和下半部。\n上部分要做的事情很少，会先禁止网卡中断，避免频繁硬中断，而降低内核的工作效率。接着，内核会触发一个软中断，把一些处理比较耗时且复杂的事情，交给「软中断处理程序」去做，也就是中断的下半部，其主要是需要从内存中找到网络数据，再按照网络协议栈，对网络数据进行逐层解析和处理，最后把数据送给应用程序。\n所以，中断处理程序的上部分和下半部可以理解为：\n上半部直接处理硬件请求，也就是硬中断，主要是负责耗时短的工作，特点是快速执行； 下半部是由内核触发，也就说软中断，主要是负责上半部未完成的工作，通常都是耗时比较长的事情，特点是延迟执行； 还有一个区别，硬中断（上半部）是会打断 CPU 正在执行的任务，然后立即执行中断处理程序，而软中断（下半部）是以内核线程的方式执行，并且每一个 CPU 都对应一个软中断内核线程，名字通常为**「ksoftirqd/CPU 编号」，**比如 0 号 CPU 对应的软中断内核线程的名字是 ksoftirqd/0\n不过，软中断不只是包括硬件设备中断处理程序的下半部，一些内核自定义事件也属于软中断，比如内核调度等、RCU 锁（内核里常用的一种锁）等。\nLinux系统中的软中断类型 在 Linux 系统里，我们可以通过查看 /proc/softirqs 的 内容来知晓「软中断」的运行情况，以及 /proc/interrupts 的 内容来知晓「硬中断」的运行情况。\n接下来，就来简单的解析下 /proc/softirqs 文件的内容，在我服务器上查看到的文件内容如下：\n你可以看到，每一个 CPU 都有自己对应的不同类型软中断的累计运行次数，有 3 点需要注意下。\n第一点，要注意第一列的内容，它是代表着软中断的类型，在我的系统里，软中断包括了 10 个类型，分别对应不同的工作类型，比如 NET_RX 表示网络接收中断，NET_TX 表示网络发送中断、TIMER 表示定时中断、RCU 表示 RCU 锁中断、SCHED 表示内核调度中断。 第二点，要注意同一种类型的软中断在不同 CPU 的分布情况，正常情况下，同一种中断在不同 CPU 上的累计次数相差不多，比如我的系统里，NET_RX 在 CPU0 、CPU1、CPU2、CPU3 上的中断次数基本是同一个数量级，相差不多。 第三点，这些数值是系统运行以来的累计中断次数，数值的大小没什么参考意义，但是系统的中断次数的变化速率才是我们要关注的，我们可以使用 watch -d cat /proc/softirqs 命令查看中断次数的变化速率。 前面提到过，软中断是以内核线程的方式执行的，我们可以用 ps 命令可以查看到，下面这个就是在我的服务器上查到软中断内核线程的结果：\n可以发现，内核线程的名字外面都有有中括号，这说明 ps 无法获取它们的命令行参数，所以一般来说，名字在中括号里的都可以认为是内核线程。而且，你可以看到有 4 个 ksoftirqd 内核线程，这是因为我这台服务器的 CPU 是 4 核心的，每个 CPU 核心都对应着一个内核线程。\n如何定位软中断CPU使用率过高的问题？ 要想知道当前的系统的软中断情况，我们可以使用 top 命令查看，下面是一台服务器上的 top 的数据：\n上图中的黄色部分 si，就是 CPU 在软中断上的使用率，而且可以发现，每个 CPU 使用率都不高，两个 CPU 的使用率虽然只有 3% 和 4% 左右，但是都是用在软中断上了。另外，也可以看到 CPU 使用率最高的进程也是软中断 ksoftirqd，因此可以认为此时系统的开销主要来源于软中断。如果要知道是哪种软中断类型导致的，我们可以使用 watch -d cat /proc/softirqs 命令查看每个软中断类型的中断次数的变化速率。\n一般对于网络 I/O 比较高的 Web 服务器，NET_RX 网络接收中断的变化速率相比其他中断类型快很多。如果发现 NET_RX 网络接收中断次数的变化速率过快，接下来就可以使用 sar -n DEV 查看网卡的网络包接收速率情况，然后分析是哪个网卡有大量的网络包进来。\n接着，在通过 tcpdump 抓包，分析这些包的来源，如果是非法的地址，可以考虑加防火墙，如果是正常流量，则要考虑硬件升级等。\n小结 为了避免由于中断处理程序执行时间过长，而影响正常进程的调度，Linux 将中断处理程序分为上半部和下半部：\n上半部 Top Half，对应硬中断，由硬件触发中断，用来快速处理中断； 下半部 Bottom Half，对应软中断，由内核触发中断，用来异步处理上半部未完成的工作； Linux 中的软中断类型包括网络收发、定时、调度、RCU 锁等，可以通过查看 /proc/softirqs 来观察软中断的累计中断次数情况，如果要实时查看中断次数的变化率，可以使用 watch -d cat /proc/softirqs 命令。\n每一个 CPU 都有各自的软中断内核线程，我们还可以用 ps 命令来查看内核线程，一般名字在中括号里面到，都认为是内核线程。\n如果在 top 命令发现，CPU 在软中断上的使用率比较高，而且 CPU 使用率最高的进程也是软中断 ksoftirqd 的时候，这种一般可以认为系统的开销被软中断占据了。\n这时我们就可以分析是哪种软中断类型导致的，一般来说都是因为网络接收软中断导致的，如果是的话，可以用 sar 命令查看是哪个网卡的有大量的网络包接收，再用 tcpdump 抓网络包，做进一步分析该网络包的源头是不是非法地址，如果是就需要考虑防火墙增加规则，如果不是，则考虑硬件升级等。\n初始化 流程分析 软中断注册 软中断执行 中断处理后 Bottom-half Enable后 1.2 tasklet 数据结构 流程分析 接口 2 Workqueue ","date":"2024-10-17T09:52:32+08:00","permalink":"https://zcxggmu.github.io/p/linux-interrupt-handle_2/","title":"Linux Interrupt Handle_2"},{"content":"0 参考 RISC-V Is Getting MSIs! – Stephen Marz\nsgmarz/riscv_msi: Message Signaled Interrupts for RISC-V (github.com)\nAIA草案手册是由约翰·豪泽（John Hauser）编写的，可以在此处找到：https://github.com/riscv/riscv-aia。\n1 概述 消息信号中断是一种在没有专用中断请求引脚（IRQ）的情况下发出中断信号的方法。MSI在PCI总线上是最常见的用途之一，PCI规范定义了MSI和MSI-X标准。其优势包括：\n减少设备与CPU或中断控制器之间的直接线路数量； 通过设计强制使用带内信号来提高信号传输性能； 改善虚拟化环境中的宿主/客户端信号传输。 该代码是用Rust编写的RV32I版本。最初我是为RV64GC编写的，但我写的其他所有内容也都是为RV64GC编写的，所以我认为我应该扩展和拓宽自己的视野。\n2 MSI中断 一个MSI是由一个 “消息” 触发的，这个术语是指 “内存写入”。实际上，我们可以通过简单地解引用一个MMIO指针来触发一个消息。\n1 2 3 4 5 6 7 // Note that 0xdeadbeef is not really a valid message. // The AIA specifies messages 1 through 2047 are valid due to the number of registers // available. But, the following is just an example. let message = 0xdeadbeef; // QEMU\u0026#39;s \u0026#39;virt\u0026#39; machine attaches the M-mode IMSIC for HART 0 to 0x2400_0000 // The AIA specifies that this must be a word, a double word will cause a trap. write_volatile(0x2400_0000 as *mut u32, message); 2.1 中断文件的MMIO地址 上面的代码将写入到 MMIO 地址 0x2400_0000 ，这是QEMU的 virt 机器连接到 HART 0 的M模式IMSIC的位置。HART 0 的S模式IMSIC将连接到 0x2800_0000 。每个HART相隔一页，意味着HART 1的M模式IMSIC位于 0x2400_1000，而S模式IMSIC位于 0x2800_1000。\n对于许多嵌入式系统来说，这些值可以来自规范或包含扁平设备树（FDT）的开放固件（OF）包。下面是QEMU的 virt FDT 的纯文本示例。对于这个代码仓，我硬编码了MMIO地址，而不是读取设备树。\n1 2 3 4 5 6 7 8 9 10 imsics@24000000 { phandle = \u0026lt;0x09\u0026gt;; riscv,ipi-id = \u0026lt;0x01\u0026gt;; riscv,num-ids = \u0026lt;0xff\u0026gt;; reg = \u0026lt;0x00 0x24000000 0x00 0x4000\u0026gt;; interrupts-extended = \u0026lt;0x08 0x0b 0x06 0x0b 0x04 0x0b 0x02 0x0b\u0026gt;; msi-controller; interrupt-controller; compatible = \u0026#34;riscv,imsics\u0026#34;; }; 标准化设备树格式的组织可以在此处找到：https://www.devicetree.org/\n关于Linux中的设备树的更多信息可以在此处找到：https://www.kernel.org/doc/html/latest/devicetree/usage-model.html\nIMSICs最近被添加到QEMU的virt机器中，因此您可能需要克隆和构建自己的QEMU。QEMU的存储库可以在此处找到：https://github.com/qemu。\n2.2 通过发送消息来触发一个中断 设备将一个字写入特定的MMIO地址后，中断被触发。这意味着设备不需要连接到IRQ控制器（如RISC-V的平台级中断控制器或PLIC）的线路。只要设备可以进行内存写入，它就可以触发中断。\n尽管触发消息如此简单，但我们需要一种机制来启用和优先处理这些消息。在某些情况下，我们可能不想获取某些消息。这就是IMSIC的作用所在。\n3 IMSIC 为了支持MSIs，某些设备需要能够接收内存写入并将其转换为中断。此外，该设备还需要提供一种机制来启用/禁用和优先处理中断，就像常规的中断控制器一样。这是通过传入的MSI控制器（IMSIC）设备来实现的。\n高级中断架构（AIA）手册仍在进行中，已经进行了一些重大更改，删除或添加了CSR或其他相关信息。因此，一些代码和表可能已经过时。\nIMSIC的寄存器机制由几个控制和状态寄存器（CSR）以及通过选择机制可访问的内部寄存器组成。\n3.1 新增的寄存器 AIA 在M模式和S模式之间定义了几个新的CSR（控制和状态寄存器）。以下是AIA所定义的新寄存器：\nRegister Name Register Number Description MISELECT 0x350 Machine register select SISELECT 0x150 Supervisor register select MIREG 0x351 A R/W view of the selected register in MISELECT SIREG 0x151 A R/W view of the selected register in SISELECT MTOPI 0xFB0 Machine top-level interrupt STOPI 0xDB0 Supervisor top-level interrupt MTOPEI 0x35C Machine top-level external interrupt (requires IMSIC) STOPEI 0x15C Supervisor top-level external interrupt (requires IMSIC) MISELECT 和 MIREG 允许我们通过将其编号写入 MISELECT 寄存器来选择寄存器。然后，MIREG 将表示所选寄存器。例如，如果我们从 MIREG 读取，我们将从所选寄存器读取，如果我们向 MIREG 写入，我们将向所选寄存器写入。\n有四个可选择的寄存器。这些寄存器有M/S模式版本。例如，如果我们写入 SISELECT ，我们将访问相应寄存器的S模式版本。\nRegister Name MISELECT/SISELECT Description EIDELIVERY 0x70 External Interrupt Delivery Register EITHRESHOLD 0x72 External Interrupt Threshold Register EIP0 through EIP63 0x80 through 0xBF External Interrupt Pending Registers EIE0 through EIE63 0xC0 through 0xFF External Interrupt Enable Registers MISELECT/SISELECT 可选择的寄存器，并可通过 MIREG/SIREG 进行读写。\n首先，我们需要做的第一件事是使能IMSIC本身。这是通过一个名为 “使能中断传递” 的寄存器 EIDELIVERY 完成的。该寄存器可以包含三个值之一：\nValue Description 0 Interrupt delivery is disabled 1 Interrupt delivery is enabled 0x4000_0000 Optional interrupt delivery via a PLIC or APLIC 3.2 使能IMSIC 因此，我们需要将 1（使中断传递可用）写入 EIDELIVERY 寄存器中：\n1 2 3 4 5 6 // First, enable the interrupt file // 0 = disabled // 1 = enabled // 0x4000_0000 = use PLIC instead imsic_write(MISELECT, EIDELIVERY); imsic_write(MIREG, 1); 3.3 中断优先级 EITHRESHOLD 寄存器保存了一个中断优先级阈值，只优先级更高的中断才能被响应。例如，如果一个中断的优先级值低于 EITHRESHOLD 中的值（值低这说明优先级更高），它将被“响应”或解屏蔽。否则，它将被屏蔽，无法被响应。例如，EITHRESHOLD 为 5 只允许消息1、2、3和4被响应。注意，消息 0 被保留为 “无消息” 的意义。\n由于较高的阈值打开了更多的消息，因此具有较低编号的消息具有较高的优先级。\n1 2 3 4 5 6 // Set the interrupt threshold. // 0 = enable all interrupts // P = enable \u0026lt; P only imsic_write(MISELECT, EITHRESHOLD); // Only hear 1, 2, 3, and 4 imsic_write(MIREG, 5); AIA规范中使用消息本身作为优先级。因此，消息1的优先级为1，而消息1、2、3、4的优先级为1、2、3、4。这更方便，因为我们可以直接控制消息。然而，由于每个消息号都有关联的启用和挂起位，因此最高编号的中断存在限制。规范的最大总消息数为 32×64-1=2,047（我们减去1以去除0作为有效消息）。\n3.4 使能信号 EIE 寄存器将控制消息的启用或禁用。对于RV64，这些寄存器的宽度为64位，但仍占用两个相邻的寄存器编号。因此，对于RV64，只能选择偶数编号的寄存器（例如，EIE0、EIE2、EIE4，\u0026hellip;，EIE62）。如果尝试选择奇数编号的EIE，将会触发无效指令陷阱。尽管文档中明确说明这是期望的行为，但我花了很多时间才弄清楚这一点。对于RV32，EIE 寄存器仅为32位，EIE0到EIE63都是可选的。\nEIE 寄存器是一个位集。如果相应消息的位为 1，则未屏蔽且已启用，否则被屏蔽且已禁用。对于RV64，消息0到63都位于 EIE0[63:0] 中。位表示消息。我们可以使用以下公式确定RV64要选择的寄存器：\n1 2 3 4 5 6 7 8 9 // Enable a message number for machine mode (RV64) fn imsic_m_enable(which: usize) { let eiebyte = EIE0 + 2 * which / 64; let bit = which % 64; imsic_write(MISELECT, eiebyte); let reg = imsic_read(MIREG); imsic_write(MIREG, reg | 1 \u0026lt;\u0026lt; bit); } RV32的行为基本相同，唯一不同的是我们不需要将其乘以2来进行缩放。\n1 2 3 4 5 6 7 8 9 // Enable a message number for machine mode (RV32) fn imsic_m_enable(which: usize) { let eiebyte = EIE0 + which / 32; let bit = which % 32; imsic_write(MISELECT, eiebyte); let reg = imsic_read(MIREG); imsic_write(MIREG, reg | 1 \u0026lt;\u0026lt; bit); } 使用上述代码，我们现在可以启用我们想要响应的消息。以下示例启用了2、4和10号消息：\n1 2 3 imsic_m_enable(2); imsic_m_enable(4); imsic_m_enable(10); 3.5 挂起信号 EIP 寄存器的行为与 EIE 寄存器完全相同，只是其中的某一位为1表示特定的消息处于挂起状态，即已发送带有该消息编号的写操作到IMSIC。\nEIP 寄存器可读写。通过从中读取，我们可以确定哪些消息处于挂起状态。通过向其写入，我们可以通过将 1 写入相应的消息位来手动触发中断消息。\n1 2 3 4 5 6 7 8 9 // Trigger a message by writing to EIP for Machine mode in RV64 fn imsic_m_trigger(which: usize) { let eipbyte = EIP0 + 2 * which / 64; let bit = which % 64; imsic_write(MISELECT, eipbyte); let reg = imsic_read(MIREG); imsic_write(MIREG, reg | 1 \u0026lt;\u0026lt; bit); } 3.6 测试 既然我们可以启用传递以及单独的消息，我们可以通过以下两种方式触发它们：\n将消息直接写入MMIO地址； 将相应消息的中断挂起位设置为1。 1 2 3 4 5 6 7 unsafe { // We are required to write only 32 bits. // Write the message directly to MMIO to trigger write_volatile(0x2400_0000 as *mut u32, 2); } // Set the EIP bit to trigger imsic_m_trigger(2); 3.7 信号Traps 每当一个未屏蔽的消息发送到一个启用的IMSIC时，它将作为外部中断传递到指定的HART。对于M模式的IMSIC，这将作为异步原因 11 传递，而对于S模式的IMSIC，这将作为异步原因 9 传递。\n当我们接收到由于消息传递而引发的中断时，我们需要通过从 MTOPEI 或 STOPEI 寄存器中读取来 “弹出” 顶级待处理中断，具体取决于特权模式。这将为我们提供一个值，其中位 26:16 包含消息编号和位 10:0 包含中断优先级。\n是的，消息编号和消息优先级是相同的数字，因此我们可以选择任意一个。\n1 2 3 4 5 6 7 8 9 10 // Pop the top pending message fn imsic_m_pop() -\u0026gt; u32 { let ret: u32; unsafe { // 0x35C is the MTOPEI CSR. asm!(\u0026#34;csrrw {retval}, 0x35C, zero\u0026#34;, retval = out(reg) ret), } // Message number starts at bit 16 ret \u0026gt;\u0026gt; 16 } 我的编译器不支持此规范中的CSR名称，因此我使用了CSR编号。这就是为什么你看到的是 0x35C 而不是 mtopei，但它们表示的是同样的意思。\n当我们从 MTOPEI 寄存器（0x35C）读取时，它将给出最高优先级消息的消息编号。上面代码片段中的 csrrw 指令将原子地将CSR的值读入返回寄存器，然后将值零存储到CSR中。\n当我们将零写入 MTOPEI 寄存器（0x35C）时，我们告诉IMSIC我们正在 “声明” 我们正在处理最顶层的消息，这将清除相应消息编号的 EIP 位。\n1 2 3 4 5 6 7 8 9 10 /// Handle an IMSIC trap. Called from `trap::rust_trap` pub fn imsic_m_handle() { let msgnum = imsic_m_pop(); match msgnum { 0 =\u0026gt; println!(\u0026#34;Spurious message (MTOPEI returned 0)\u0026#34;), 2 =\u0026gt; println!(\u0026#34;First test triggered by MMIO write successful!\u0026#34;), 4 =\u0026gt; println!(\u0026#34;Second test triggered by EIP successful!\u0026#34;), _ =\u0026gt; println!(\u0026#34;Unknown msi #{}\u0026#34;, v), } } 第 0 条消息无效，因为当我们从 MTOPEI 中弹出时，0表示 “无中断”。\n4 测例输出 如果您使用新的QEMU运行仓库，成功测试后应该会看到以下内容。\n5 结论 在本篇文章中，我们看到了为支持 RISC-V 的新寄存器添加了对即将到来的MSI控制器（IMSIC）的支持。我们还启用了IMSIC传送、单个消息，并处理了发送消息的两种方式：**通过MMIO直接发送或通过设置相应的EIP位。**最后，我们处理了来自陷阱的中断。\nAIA手册的第二部分包括新的高级平台级中断控制器（APLIC）。我们将研究这个系统，并编写驱动程序来开始研究这个新的APLIC如何使用电线或消息进行信号传递。\n在APLIC之后，我们将为PCI设备编写驱动程序，并使用它来发送MSI-X消息。\n","date":"2024-10-17T09:51:39+08:00","permalink":"https://zcxggmu.github.io/p/riscv-msis/","title":"Riscv MSIs"},{"content":"0 参考 The RISC-V APLIC’s New Features – Stephen Marz\n本博客系列涉及此处编写的代码：https://github.com/sgmarz/riscv_msi。\nAPLIC规范（仍处于草案阶段）是高级中断架构（AIA）规范的一部分，保存在此处：https://github.com/riscv/riscv-aia。\n1 介绍 高级平台级中断控制器（APLIC）是SiFive公司的PLIC的高级版本。其主要提升在于**支持通过消息发送中断。**因此，当与IMSIC配对时，APLIC可以像任何其他硬件设备一样发送消息。\n原始PLIC的主要目的是聚合、优先处理和发送硬件中断信号。它为操作系统提供了一种声明、处理和完成中断请求的方法。APILC通过 “外部中断” 引脚向特定的HART（硬件线程，即CPU核心）发送通知。然后，该HART通过从特定的PLIC寄存器（称为 claim 寄存器）读取来确定中断的来源。这将返回一个标识设备的数字。通常，这个数字指向连接硬件设备和PLIC的特定线路。\n2 APLIC 新的APLIC与SiFive PLIC不兼容。其中一些概念和特性是相同的，但寄存器文件和映射是不同的。从技术上讲，可以实现一个没有完整APLIC的系统，但AIA文档明确指出：“ 完全符合高级中断架构需要APLIC ”。\n首先，APLIC寄存器布局如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 struct Aplic { pub domaincfg: u32, // Domain CSR that controls how this APLIC functions pub sourcecfg: [u32; 1023], // Source configuration for 1023 interrupts _reserved1: [u8; 0xBC0], pub mmsiaddrcfg: u32, // Machine-level MSI address (for APLIC to write MSIs) pub mmsiaddrcfgh: u32, pub smsiaddrcfg: u32, // Supervisor-level MSI address pub smsiaddrcfgh: u32, _reserved2: [u8; 0x30], pub setip: [u32; 32], // Bitset to set pending interrupts (32 IRQS per element) _reserved3: [u8; 92], pub setipnum: u32, // Sets a pending interrupt by number _reserved4: [u8; 0x20], pub clrip: [u32; 32], // Bitset to clear pending interrupts (opposite of setip) _reserved5: [u8; 92], pub clripnum: u32, // Clears a pending interrupt by number _reserved6: [u8; 32], pub setie: [u32; 32], // Bitset to enable interrupts _reserved7: [u8; 92], pub setienum: u32, // Enable an interrupt by number _reserved8: [u8; 32], pub clrie: [u32; 32], // Bitset to disable interrupts (opposite of setie) _reserved9: [u8; 92], pub clrienum: u32, // Disable an interrupt by number _reserved10: [u8; 32], pub setipnum_le: u32, // Set an interrupt pending by number always little end first pub setipnum_be: u32, // Set an interrupt pending by number always big end first _reserved11: [u8; 4088], pub genmsi: u32, // Used to generate MSIs pub target: [u32; 1023], // Target control per interrupt } 下面介绍一些重点寄存器：domaincfg/sourcecfg/setip/clrip/genmsi/target。\n2.1 Domain Configuration Register (domaincfg) 域配置寄存器（32位）该寄存器有三个可用字段：\n中断使能（IE，第8位） 传递模式（DM，第2位） 大小端模式（BE，第0位） 首先的 8 位是一个字节顺序标记，目的在于测试寄存器是大端序还是小端序。它被设置为 0x80，以便我们可以测试寄存器的字节顺序。例如，如果我们执行一个 “加载字”（ lw 指令在RISC-V中），并且在第一个字节中得到 0x80，那意味着机器是大端序。如果没有得到 0x80，则是小端序，并且加载的字节包含传输模式和大端序位。\n中断使能位（Interrupt enable bit）用于使 APLIC 能够发送中断信号（1 = 使能，0 = 禁用）。这并不意味着中断一定会被接收到，而仅仅表示 APLIC 可以通过触发一个挂起位来发送中断信号。\n中断投递模式（DM）位允许配置 APLIC 以发送常规中断，类似于旧的PLIC，或者以消息形式发送中断（MSI）。如果 DM 位设置为 0，则会发送直接中断，类似于旧的 APLIC。如果 DM 位设置为 1，则会发送 MSIs。\n大端位允许 APLIC 以大端模式（BE = 1）或以小端模式（BE = 0）写入消息。但是，BE位还会影响多字节域配置寄存器的顺序。最高有效字节被故意设置为 0x80，作为一种字节顺序标记。\n我们可以写一个Rust函数来设置域注册。该注册函数中的所有参数都是布尔类型。\n1 2 3 4 5 6 7 8 9 10 impl Aplic { pub fn set_domaincfg(\u0026amp;mut self, bigendian: bool, msimode: bool, enabled: bool) { // Rust library assures that converting a bool into u32 will use // 1 for true and 0 for false let enabled = u32::from(enabled); let msimode = u32::from(msimode); let bigendian = u32::from(bigendian); self.domaincfg = (enabled \u0026lt;\u0026lt; 8) | (msimode \u0026lt;\u0026lt; 2) | bigendian; } } 2.2 Source Configuration Registers (sourcecfg[u32; 1023]) If bit 10 (delegate) is 0, SM is in bits 2:0. If D=1, bits 9:0 mark the child index.\n如果位10（代理）为0，则SM位于位 2:0。如果D=1，则位 9:0 标记子索引。\n每个可能的中断都有一个 sourcecfg 寄存器。请记住，中断0是不可能的，所以中断 1 的源配置寄存器在 sourcecfg[0] 中。这就是为什么我提供的 Rust 代码示例会从中断源的值中减去 1 的原因。\n委托位 D（位10）可以读取以确定所给中断是否已被委托。该位是可读写的。如果我们将1写入该字段，则将其委托给子域。如果该特定源没有子域，该位将始终被读取为0。\n源模式位 SM（位2:0）控制着对于那些未委派的中断如何触发。如果一个中断被委派（D=1），位9:0描述了它被委派给的子中断的索引。如果中断未被委派（D=0），那么每个中断源可以通过以下方式之一配置为触发 “挂起” 中断。\n3-bit “SM” value Register Name Description 0 Inactive The interrupt source cannot generate an interrupt and is inactive. 1 Detached The interrupt can only be generated by writing directly to the APLIC. 2, 3 – Reserved 4 Edge1 The interrupt is asserted on a rising edge (from 0 to 1). 5 Edge0 The interrupt is asserted on a falling edge (from 1 to 0). 6 Level1 The interrupt is asserted when high (device IRQ pin is asserted). 7 Level0 The interrupt is asserted when low (device IRQ pin is deasserted). 一个不活动的中断源无法通过APLIC产生中断，我们也无法通过写入中断挂起寄存器来手动触发中断。一个分离的中断源无法被设备触发，但是我们可以手动写入MMIO APLIC寄存器来触发中断。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #[repr(u32)] enum SourceModes { Inactive = 0, Detached = 1, RisingEdge = 4, FallingEdge = 5, LevelHigh = 6, LevelLow = 7, } impl Aplic { pub fn set_sourcecfg(\u0026amp;mut self, irq: u32, mode: SourceModes) { assert!(irq \u0026gt; 0 \u0026amp;\u0026amp; irq \u0026lt; 1024); self.sourcecfg[irq as usize - 1] = mode as u32; } pub fn sourcecfg_delegate(\u0026amp;mut self, irq: u32, child: u32) { assert!(irq \u0026gt; 0 \u0026amp;\u0026amp; irq \u0026lt; 1024); self.sourcecfg[irq as usize - 1] = 1 \u0026lt;\u0026lt; 10 | (child \u0026amp; 0x3ff); } } 2.3 Set/Clear Pending/Enable Interrupt (setip/clrip) Registers 这些寄存器控制着中断是否处于挂起状态。当启用的中断被触发时，APLIC将自动设置这些位；然而，我们也可以手动将中断设置为挂起中断。\n这些寄存器与IMSIC寄存器不同。有一组寄存器用于设置待处理中断，另一组寄存器用于清除待处理中断。\nsetipnum 和 clripnum 寄存器的功能与以下使用 setip/clrip 寄存器的 Rust 函数类似。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 impl Aplic pub fn set_ip(\u0026amp;mut self, irq: u32, pending: bool) { assert!(irq \u0026gt; 0 \u0026amp;\u0026amp; irq \u0026lt; 1024); let irqidx = irq as usize / 32; let irqbit = irq as usize % 32; if pending { // self.setipnum = irq; self.setip[irqidx] = 1 \u0026lt;\u0026lt; irqbit; } else { // self.clripnum = irq; self.clrip[irqidx] = 1 \u0026lt;\u0026lt; irqbit; } } } 中断使能寄存器 setie/clrie 的作用类似于中断挂起寄存器，但它们允许中断被信号化。如果一个中断未被使能，则该中断会被屏蔽，无法被触发。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 impl Aplic { pub fn set_ie(\u0026amp;mut self, irq: u32, enabled: bool) { assert!(irq \u0026gt; 0 \u0026amp;\u0026amp; irq \u0026lt; 1024); let irqidx = irq as usize / 32; let irqbit = irq as usize % 32; if enabled { // self.setienum = irq; self.setie[irqidx] = 1 \u0026lt;\u0026lt; irqbit; } else { // self.clrienum = irq; self.clrie[irqidx] = 1 \u0026lt;\u0026lt; irqbit; } } } 2.4 Generate MSI Register (genmsi) genmsi 寄存器中有两个读/写字段和一个只读字段。即使直接写入IMSIC更高效，但可以使用 genmsi 寄存器来触发MSI写入。\nHART Index是您想要发送MSI的目标HART，EIID（外部中断标识符）是要写入IMSIC的值。通常，EIID与您想要触发的中断号相同。\n2.5 Target Control Registers (target[u32; 1032]) 目标控制寄存器根据 APLIC 的配置方式有两种不同形式。如果 APLIC 配置为**直接传递模式，**则寄存器包含一个 HART Index和一个优先级。较小的优先级表示较高，因此 10 的优先级高于 20。\n1 2 3 4 5 6 impl Aplic { pub fn set_target_direct(\u0026amp;mut self, irq: u32, hart: u32, prio: u32) { assert!(irq \u0026gt; 0 \u0026amp;\u0026amp; irq \u0026lt; 1024); self.target[irq as usize - 1] = (hart \u0026lt;\u0026lt; 18) | (prio \u0026amp; 0xFF); } } **在MSI交付模式下，**寄存器包含一个HART索引、客户索引和外部中断标识符（EIID）。\n1 2 3 4 5 6 impl Aplic { pub fn set_target_msi(\u0026amp;mut self, irq: u32, hart: u32, guest: u32, eiid: u32) { assert!(irq \u0026gt; 0 \u0026amp;\u0026amp; irq \u0026lt; 1024); self.target[irq as usize - 1] = (hart \u0026lt;\u0026lt; 18) | (guest \u0026lt;\u0026lt; 12) | eiid; } } 2.6 Interrupt Delivery Control 正如我之前提到的，APLIC可以配置为MSI模式或直接模式：\n在MSI模式下，消息由输入MSI控制器（IMSIC）处理； 然而，在直接模式下，APLIC本身将控制中断。这是通过APLIC的中断传递控制（IDC）部分来实现的。 对于QEMU上的 virt 虚拟机，中断传递控制寄存器 IDC 被内存映射到0x地址。寄存器的布局如下。\nOffset Size (bytes) Register Name 0 4 idelivery 4 4 iforce 8 4 ithreshold 24 4 topi 28 4 claimi 每个IDC用于一个独立的HART，长度为32字节。因此，HART #0的寄存器从偏移量0开始，HART #1的寄存器从偏移量32开始，以此类推。\nidelivery 寄存器启用了APLIC的直接传递模式。如果该寄存器设置为1，则APLIC可以传递中断，否则，如果该寄存器设置为0，则APLIC不传递中断。\nThe idelivery register\n中断强制寄存器强制 APLIC 提交中断 #0。如果我们向该寄存器写入1，并且 APLIC 的中断已启用，则这将发出一个中断信号。\nThe iforce register\n中断阈值寄存器设置了能够被监听到的中断优先级的阈值。阈值为0意味着所有中断都可以被监听到。如果值非零，则该值或更高优先级的任何中断都会被屏蔽，因此无法被监听到。在这种情况下，与其他所有情况一样，数字越小，优先级越高。不要被0所迷惑，它只是一个特殊值，可以解除所有优先级的屏蔽。然而，阈值为1意味着所有中断都会被屏蔽，而阈值为2意味着只有优先级1的中断可以被监听到。\nThe ithreshold register\ntopi 寄存器保存的是最高优先级且已启用的中断号。该寄存器分为两部分：\n25:16 位保存中断号 7:0 位保存中断优先级 The topi and claim registers\n申请寄存器与顶级中断寄存器相同，只是它表示我们正在申请顶级中断。当我们从该寄存器读取时，给定寄存器的挂起位将被清零。\n我制作的仓库上的操作系统只使用MSI传递模式，因为这种传递控制方式与旧的PLIC非常相似。\n3 结论 QEMU的 virt 虚拟机将UART连接到外部IRQ＃10，因此我们可以在UART接收器有数据时使用APLIC发送消息。这要求我们使用上述寄存器来设置APLIC。\n在下面的代码中，我在M/S模式之间进行了分割，并将UART委派给S模式APLIC（索引0）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 pub fn aplic_init() { // The root APLIC let mplic = Aplic::as_mut(AplicMode::Machine); // The delgated child APLIC let splic = Aplic::as_mut(AplicMode::Supervisor); // Enable both the machine and supervisor PLICS mplic.set_domaincfg(false, true, true); splic.set_domaincfg(false, true, true); // Write messages to IMSIC_S mplic.set_msiaddr(AplicMode::Supervisor, crate::imsic::IMSIC_S); // Delegate interrupt 10 to child 0, which is APLIC_S // Interrupt 10 is the UART. So, whenever the UART receives something // into its receiver buffer register, it triggers an IRQ #10 to the APLIC. mplic.sourcecfg_delegate(10, 0); // The EIID is the value that is written to the MSI address // When we read TOPEI in IMSIC, it will give us the EIID if it // has been enabled. splic.set_target_msi(10, 0, 0, 10); // Level high means to trigger the message delivery when the IRQ is // asserted (high). splic.set_sourcecfg(10, SourceModes::LevelHigh); // The order is important. QEMU will not allow enabling of the IRQ // unless the source configuration is set properly. // mplic.set_irq(10, true); splic.set_ie(10, true); } 当UART发送中断时，我们现在可以在IMSIC中编写UART处理程序。\n1 2 3 4 5 6 7 8 9 10 pub fn imsic_handle(pm: PrivMode) { let msgnum = imsic_pop(pm); match msgnum { 0 =\u0026gt; println!(\u0026#34;Spurious \u0026#39;no\u0026#39; message.\u0026#34;), 2 =\u0026gt; println!(\u0026#34;First test triggered by MMIO write successful!\u0026#34;), 4 =\u0026gt; println!(\u0026#34;Second test triggered by EIP successful!\u0026#34;), 10 =\u0026gt; console_irq(), _ =\u0026gt; println!(\u0026#34;Unknown msi #{}\u0026#34;, msgnum), } } 上面的代码将任何消息#10转发到 console_irq，而 console_irq 则从UART设备的接收缓冲寄存器中弹出一个值。\n一切都说完了，我们可以开始接收UART消息了。\n","date":"2024-10-17T09:51:11+08:00","permalink":"https://zcxggmu.github.io/p/riscv_aia_aplic/","title":"Riscv_aia_aplic"},{"content":"0 问题 ez4yunfeng2/riscv-kvm-demo (github.com) 1 参与linux/kvm-riscv 填写email/name：\nkvm-riscv Info Page (infradead.org)； linux-riscv Info Page (infradead.org)； 后续，用上述email发送邮件至 kvm-riscv-join@lists.infradead.org，邮件内容仅添加纯文本的 subscribe，标题可以不用写。\nPlease subscribe by email instead: Due to abuse, the web-based subscription has been disabled. You can join by sending email to the list\u0026rsquo;s -join address instead.\nIt looks like you were trying to subscribe to linux-riscv@lists.infradead.org.\nIn that case you should be able to subscribe by sending a PLAIN TEXT (not HTML) mail containing only the single word subscribe to linux-riscv-join@lists.infradead.org.\nIt must be plain text, and it must go to the -join address, not to the list itself.\n稍后，你将收到一封确认订阅的邮件，简单回复或进入link确认：\nConfirm subscription request (infradead.org) Subscription request confirmed (infradead.org) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 // /MAINTAINERS RISC-V ARCHITECTURE M:\tPaul Walmsley \u0026lt;paul.walmsley@sifive.com\u0026gt; M:\tPalmer Dabbelt \u0026lt;palmer@dabbelt.com\u0026gt; M:\tAlbert Ou \u0026lt;aou@eecs.berkeley.edu\u0026gt; L:\tlinux-riscv@lists.infradead.org S:\tSupported Q:\thttps://patchwork.kernel.org/project/linux-riscv/list/ C:\tirc://irc.libera.chat/riscv P:\tDocumentation/arch/riscv/patch-acceptance.rst T:\tgit git://git.kernel.org/pub/scm/linux/kernel/git/riscv/linux.git F:\tarch/riscv/ N:\triscv KERNEL VIRTUAL MACHINE FOR RISC-V (KVM/riscv) M:\tAnup Patel \u0026lt;anup@brainfault.org\u0026gt; R:\tAtish Patra \u0026lt;atishp@atishpatra.org\u0026gt; L:\tkvm@vger.kernel.org L:\tkvm-riscv@lists.infradead.org L:\tlinux-riscv@lists.infradead.org S:\tMaintained T:\tgit https://github.com/kvm-riscv/linux.git F:\tarch/riscv/include/asm/kvm* F:\tarch/riscv/include/uapi/asm/kvm* F:\tarch/riscv/kvm/ F:\ttools/testing/selftests/kvm/*/riscv/ F:\ttools/testing/selftests/kvm/riscv/ 2 向Linux提交PATCH流程 RISC-V Linux 内核开发与 Upstream 实践 - 谭老师_哔哩哔哩_bilibili\n[向 Linux kernel 社区提交patch补丁步骤总结（已验证成功）_发patch包-CSDN博客](https://blog.csdn.net/jcf147/article/details/123719000#:~:text=一、详细步骤 1 1.安装git和git send-email yum install git \u0026hellip;,0 warnings。 \u0026hellip; 8 8.测试发送 在正式发送之前，先发给自己测试一下： \u0026hellip; 更多项目)\n谢宝友: 手把手教你给Linux内核发patch - 知乎 (zhihu.com)\n[linux内核][邮件列表]: 如何给linux kernel 提交 patch - 知乎 (zhihu.com)\n【学习分享】 记录开源小白的第一次 PR - 知乎 (zhihu.com)\n2.1 准备工作 在开始工作之前，请准备如下工作：\n安装一份Linux\n不论是ubuntu、centos还是其他Linux发行版本，都是可以的。我个人习惯使用ubuntu 22.04版本。\n安装Git\n默认的Linux发行版，一般都已经安装好git。如果没有，随便找一本git的书都可以。这里不详述。\n配置git\n配置用户名和邮箱\n在配置用户名的时候，请注意社区朋友习惯用英语沟通，也就是名在前，姓在后。这一点会影响社区邮件讨论，因此需要留意。在配置邮箱时，也要注意。社区会将国内某些著名的邮件服务器屏蔽。因此建议你申请一个gmail邮箱。以下是我的配置：\n1 2 3 $git config -l | grep \u0026#34;user\u0026#34; user.email=baoyou.xie@linaro.org user.name=Baoyou Xie 配置 sendemail\n你可以手工修改~/.gitconfig，或者git仓库下的.git/config文件，添加 [sendemail] 节。该配置用于指定发送补丁时用到的邮件服务器参数。以下是我的配置，供参考：\n1 2 3 4 5 [sendemail] smtp encryption= tls smtp server= smtp.gmail.com smtp user= baoyou.xie@linaro.org smtp serverport= 587 gmail邮箱的配置比较麻烦，需要按照google的说明，制作证书。配置完成后，请用如下命令，向自己发送一个测试补丁：\n1 git send-email your.patch --to your.mail --cc your.mail 下载源码\n首先，请用如下命令，拉取linus维护的Linux主分支代码到本地：\n1 git clone ssh://git@dev-private.git.linaro.org/zte/kernel.git 这个过程比较长，请耐心等待。一般情况下，Linux主分支代码不够新，如果你基于这个代码制作补丁，很有可能不会顺利的合入到Maintainer那里，换句话说，Maintainer会将补丁发回给你，要求你重新制作。所以，一般情况下，你需要再用以下命令，添加其他分支，特别是 linux-next 分支。强调一下，你需要习惯基于 linux-next 分支进行工作。\n1 2 3 git remote add linux-nexthttps://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git git remote add staging https://git.kernel.org/pub/scm/linux/kernel/git/gregkh/staging.git git remote add net git://git.kernel.org/pub/scm/linux/kernel/git/davem/net.git 然后用如下命令拉取这三个分支的代码到本地：\n1 2 3 git fetch --tags linux-next git fetch --tags staging git fetch --tags net 有些Maintainer维护了自己的代码分支，那么，你可以在内核源码目录 \\MAINTAINERS 文件中，找一下相应文件的维护者，及其git地址。例如，watchdog模块的信息如下：\n1 2 3 4 5 6 7 8 9 10 11 12 WATCHDOGDEVICE DRIVERS M: Wim Van Sebroeck \u0026lt;wim@iguana.be\u0026gt; R: Guenter Roeck \u0026lt;linux@roeck-us.net\u0026gt; L: linux-watchdog@vger.kernel.org W: http://www.linux-watchdog.org/ T: gitgit://www.linux-watchdog.org/linux-watchdog.git S: Maintained F: Documentation/devicetree/bindings/watchdog/ F: Documentation/watchdog/ F: drivers/watchdog/ F: include/linux/watchdog.h F: include/uapi/linux/watchdog.h 其中，http://www.linux-watchdog.org/linux-watchdog.git 是其git地址。你可以用如下命令拉取watchdog代码到本地：\n1 2 git remote add watchdog git://www.linux-watchdog.org/linux-watchdog.git git fetch --tags watchdog 当然，这里友情提醒一下，MAINTAINERS里面的信息可能不一定准确，这时候你可能需要借助google，或者问一下社区的朋友，或者直接问一下作者本人。不过，一般情况下，基于 linux-next 分支工作不会有太大的问题。实在有问题再去打扰作者本人。\n阅读Documentation/SubmittingPatches，这很重要。\n检出源码\n1 git branch mybranch next-20170807 这个命令表示将 linux-next 分支的20170807这个tag作为本地 mybranch 的基础。\n1 git checkout mybranch 2.2 寻找软柿子 如果没有奇遇，大厨一般都是从小工做起的。我们不可能一开始就维护一个重要的模块，或者修复一些非常重要的故障。那么我们应当怎么样入手参与社区？这当然要寻找软柿子了。拿着软柿子做出来的补丁，可以让Maintainer无法拒绝合入你的补丁。当然，这么做主要还是为了在Maintainer那里混个脸熟。否则，以后你发的重要补丁，人家可能不会理你。\n什么样的柿子最软？下面是答案：\n消除编译警告。 编码格式，例如注释里面的单词拼写错误、对齐不规范、代码格式不符合社区要求。 建议是从 “消除编译警告” 入手。社区很多大牛，都是这样成长起来的。\n我们平时编译内核，基本上遇不到编译警告。是不是内核非常完美，没有编译警告，非矣！你用下面这个步骤试一下：\n首先，配置内核，选择所有模块：\n1 make ARCH=arm64 allmodconfig 请注意其中 allmodconfig，很有用的配置，我们暂且可以理解为，将所有模块都编译。这样我们就可以查找所有模块中的编译警告了。\n下面这个命令开始编译所有模块：\n1 make ARCH=arm64 EXTRA_CFLAGS=\u0026#34;-Wmissing-declarations -Wmissing-prototypes\u0026#34; CROSS_COMPILE=/toolchains/aarch64-linux-gnu/bin/aarch64-linux-gnu- EXTRA_CFLAGS=\u0026quot;-Wmissing-declarations-Wmissing-prototypes\u0026quot; 参数表示追踪所有missing-declarations、missing-prototypes类型的警告。 \u0026ldquo;CROSS_COMPILE=/toolchains/aarch64-linux-gnu/bin/aarch64-linux-gnu-\u0026rdquo; 是指定交叉编译工具链路径，需要根据你的实际情况修改。当然，如果是x86架构，则不需要指定此参数。 在编译的过程中，我们发现如下错误：\n1 scripts/Makefile.build:311:recipe for target \u0026#39;drivers/staging/fsl-mc/bus/dpio/qbman-portal.o\u0026#39; failed 我们可以简单的忽略 drivers/staging/fsl-mc/bus/dpio/qbman-portal.c 这个文件。在 drivers/staging/fsl-mc/bus/dpio/Makefile 文件中，发现这个文件的编译依赖于宏 CONFIG_FSL_MC_DPIO。\n于是，我们修改编译命令，以如下命令继续编译：\n1 make CONFIG_ACPI_SPCR_TABLE=n ARCH=arm64 EXTRA_CFLAGS=\u0026#34;-Wmissing-declarations -Wmissing-prototypes\u0026#34; CROSS_COMPILE=/toolchains/aarch64-linux-gnu/bin/aarch64-linux-gnu- 请注意该命令中的 CONFIG_ACPI_SPCR_TABLE=n，它强制关闭了 CONFIG_ACPI_SPCR_TABLE 配置。\n当编译完成以后，我们是不是发现有很多警告？特别是在drivers目录下。下面是我在 next-20170807 版本中发现的警告：\n1 2 3 4 5 6 7 8 9 10 /dimsum/git/kernel.next/drivers/clk/samsung/clk-s3c2410.c:363:13:warning: no previous prototype for \u0026#39;s3c2410_common_clk_init\u0026#39;[-Wmissing-prototypes] void__init s3c2410_common_clk_init(struct device_node *np, unsigned long xti_f, ^ CC drivers/clk/samsung/clk-s3c2412.o /dimsum/git/kernel.next/drivers/clk/samsung/clk-s3c2412.c:254:13:warning: no previous prototype for \u0026#39;s3c2412_common_clk_init\u0026#39;[-Wmissing-prototypes] void__init s3c2412_common_clk_init(struct device_node *np, unsigned long xti_f, ^ CC drivers/clk/samsung/clk-s3c2443.o /dimsum/git/kernel.next/drivers/clk/samsung/clk-s3c2443.c:388:13:warning: no previous prototype for \u0026#39;s3c2443_common_clk_init\u0026#39; [-Wmissing-prototypes] void__init s3c2443_common_clk_init(struct device_node *np, unsigned long xti_f, 下一节，我们就基于这几个警告来制作补丁。\n2.3 制作PATCH 修改错误，制作补丁 要消除这几个警告，当然很简单了。将这几个函数声明为 static 即可。下面是我的修改：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 git diff diff --git a/drivers/clk/samsung/clk-s3c2410.c b/drivers/clk/samsung/clk-s3c2410.c index e0650c3..8f4fc5a 100644 --- a/drivers/clk/samsung/clk-s3c2410.c +++ b/drivers/clk/samsung/clk-s3c2410.c @@ -360,7 +360,7 @@ static void __inits3c2410_common_clk_register_fixed_ext( samsung_clk_register_alias(ctx, \u0026amp;xti_alias, 1); } -void __init s3c2410_common_clk_init(structdevice_node *np, unsigned long xti_f, +static void __init s3c2410_common_clk_init(struct device_node *np, unsigned long xti_f, intcurrent_soc, void__iomem *base) { diff --git a/drivers/clk/samsung/clk-s3c2412.c b/drivers/clk/samsung/clk-s3c2412.c index b8340a4..2a2ce06 100644 --- a/drivers/clk/samsung/clk-s3c2412.c +++ b/drivers/clk/samsung/clk-s3c2412.c @@ -251,7 +251,7 @@ static void __init s3c2412_common_clk_register_fixed_ext( samsung_clk_register_alias(ctx, \u0026amp;xti_alias, 1); } -void __init s3c2412_common_clk_init(struct device_node *np, unsigned long xti_f, +static void __init s3c2412_common_clk_init(struct device_node *np, unsigned long xti_f, unsigned long ext_f, void __iomem *base) { struct samsung_clk_provider *ctx; diff --gita/drivers/clk/samsung/clk-s3c2443.c b/drivers/clk/samsung/clk-s3c2443.c index abb935c..f0b88bf 100644 --- a/drivers/clk/samsung/clk-s3c2443.c +++ b/drivers/clk/samsung/clk-s3c2443.c @@ -385,7 +385,7 @@ static void __inits3c2443_common_clk_register_fixed_ext( ARRAY_SIZE(s3c2443_common_frate_clks)); } -void __init s3c2443_common_clk_init(struct device_node *np, unsigned long xti_f, +static void __init s3c2443_common_clk_init(struct device_node *np, unsigned long xti_f, int current_soc, void __iomem *base) 再编译一次，警告果然被消除了。原来，社区工作如此简单：）。\n但是请允许我浇一盆冷水！你先试着用下面的命令做一个补丁出来看看：\n1 2 3 4 5 6 7 8 git add drivers/clk/samsung/clk-s3c2410.c git add drivers/clk/samsung/clk-s3c2412.c git add drivers/clk/samsung/clk-s3c2443.c git commit drivers/clk/samsung/ [zxic/67184930591\\] this is my test 3 files changed, 3 insertions(+), 3deletions(-) git format-patch -s -v 1 -1 生成的补丁内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 cat v1-0001-this-is-my-test.patch From 493059190e9ca691cf08063ebaf945627a5568c7 Mon Sep 17 00:00:00 2001 From: Baoyou Xie\u0026lt;baoyou.xie@linaro.org\u0026gt; Date: Thu, 17 Aug 2017 19:23:13 +0800 Subject: [PATCH v1] this is my test Signed-off-by: Baoyou Xie\u0026lt;baoyou.xie@linaro.org\u0026gt; --- drivers/clk/samsung/clk-s3c2410.c | 2 +- drivers/clk/samsung/clk-s3c2412.c | 2 +- drivers/clk/samsung/clk-s3c2443.c | 2 +- 3files changed, 3 insertions(+), 3 deletions(-) diff --git a/drivers/clk/samsung/clk-s3c2410.c b/drivers/clk/samsung/clk-s3c2410.c index e0650c3..8f4fc5a 100644 --- a/drivers/clk/samsung/clk-s3c2410.c +++ b/drivers/clk/samsung/clk-s3c2410.c @@ -360,7 +360,7 @@ static void __init s3c2410_common_clk_register_fixed_ext( samsung_clk_register_alias(ctx,\u0026amp;xti_alias, 1); } -void __init s3c2410_common_clk_init(struct device_node *np, unsigned long xti_f, +static void __init s3c2410_common_clk_init(struct device_node *np, unsigned long xti_f, int current_soc, void __iomem *base) { diff --git a/drivers/clk/samsung/clk-s3c2412.c b/drivers/clk/samsung/clk-s3c2412.c index b8340a4..2a2ce06 100644 --- a/drivers/clk/samsung/clk-s3c2412.c +++ b/drivers/clk/samsung/clk-s3c2412.c @@ -251,7 +251,7 @@ static void __inits3c2412_common_clk_register_fixed_ext( samsung_clk_register_alias(ctx,\u0026amp;xti_alias, 1); } -void __init s3c2412_common_clk_init(structdevice_node *np, unsigned long xti_f, +static void __init s3c2412_common_clk_init(struct device_node *np, unsigned long xti_f, unsigned long ext_f, void __iomem *base) { struct samsung_clk_provider *ctx; diff --git a/drivers/clk/samsung/clk-s3c2443.c b/drivers/clk/samsung/clk-s3c2443.c index abb935c..f0b88bf 100644 --- a/drivers/clk/samsung/clk-s3c2443.c +++ b/drivers/clk/samsung/clk-s3c2443.c @@ -385,7 +385,7 @@ static void __inits3c2443_common_clk_register_fixed_ext( ARRAY_SIZE(s3c2443_common_frate_clks)); } -void __init s3c2443_common_clk_init(structdevice_node *np, unsigned long xti_f, +static void __init s3c2443_common_clk_init(struct device_node *np, unsigned long xti_f, int current_soc, void __iomem *base) { -- 2.7.4 你可以试着用 git send-email v1-0001-this-is-my-test.patch --to baoyou.xie@linaro.org 将补丁发给Maintainer。记得准备好一个盆子，接大家的口水：）\n在制作正确的补丁之前，我们需要这个错误的补丁错在何处：\n应该将它拆分成三个补丁。\n也许这一点值得商酌，因为这三个文件都是同一个驱动：clk: samsung。也许Maintainer认为它是同一个驱动，做成一个补丁也是可以的。我觉得应该拆分成三个。当然了，应当以Maintainer的意见为准。不同的Maintainer也许会有不同的意见。\n补丁描述实在太LOW。\n补丁格式不正确。\n补丁内容不正确。\n下一节我们逐个解决这几个问题。但是首先我们应当将补丁回退。使用如下命令：\n1 git reset HEAD~1 正确的PATCH 首先需要修改补丁描述。补丁第一行是标题，比较重要。它首先应当是模块名称。但是我们怎么找到 drivers/clk/samsung/clk-s3c2412.c文件属于哪个模块？可以试试下面这个命令，看看 drivers/clk/samsung/clk-s3c2412.c 文件的历史补丁：\n1 2 3 4 5 6 root@ThinkPad-T440:/dimsum/git/kernel.next# git log drivers/clk/samsung/clk-s3c2412.c commit 02c952c8f95fd0adf1835704db95215f57cfc8e6 Author:Martin Kaiser \u0026lt;martin@kaiser.cx\u0026gt; Date: Wed Jan 25 22:42:25 2017 +0100 clk: samsung:mark s3c...._clk_sleep_init() as __init ok，模块名称是 clk:samsung。下面是我为这个补丁添加的描述，其中第一行是标题：\n1 2 3 4 5 6 7 8 9 clk: samsung: mark symbols static where possible for s3c2410 We get 1 warnings when building kernel withW=1: /dimsum/git/kernel.next/drivers/clk/samsung/clk-s3c2410.c:363:13:warning: no previous prototype for \u0026#39;s3c2410_common_clk_init\u0026#39;[-Wmissing-prototypes] void __init s3c2410_common_clk_init(struct device_node *np, unsigned long xti_f, In fact, this function is only used in thefile in which they are declared and don\u0026#39;t need a declaration, but can be made static. So this patch marks these functions with \u0026#39;static\u0026#39;. 这段描述是我从其他补丁中拷贝出来的，有几下几点需要注意：\n标题中故意添加了“for s3c2410”，以区别于另外两个补丁 “1 warnings”这个单词中，错误的使用了复数，这是因为复制的原因 “/dimsum/git/kernel.next/”这个路径名与我的本地路径相关，不应当出现在补丁中 警告描述超过了80个字符，但是这是一个特例，这里允许超过80字符 这些问题，如果不处理的话，Maintainer会不高兴的！如果Maintainer表示了不满，而你不修正的话，这个补丁就会被忽略。\n修正后的补丁描述如下：\n1 2 3 4 5 6 7 8 9 clk: samsung: mark symbols static wherepossible for s3c2410 We get 1 warning when building kernel withW=1: drivers/clk/samsung/clk-s3c2410.c:363:13:warning: no previous prototype for \u0026#39;s3c2410_common_clk_init\u0026#39;[-Wmissing-prototypes] void__init s3c2410_common_clk_init(struct device_node *np, unsigned long xti_f, In fact, this function is only used in thefile in which they are declared and don\u0026#39;t need a declaration, but can be made static. So this patch marks these functions with \u0026#39;static\u0026#39;. 我们的补丁描述一定要注意用词，不要出现将“unused”写为“no used”这样的错误。反复使用 git add，git commit 将补丁提交到git仓库。\n终于快成功，是不是想庆祝一下。用 git 命令看看我们刚才提交的三个补丁：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 root@ThinkPad-T440:/dimsum/git/kernel.next#git log drivers/clk/samsung/ commit0539c5bc17247010d17394b0dc9f788959381c8f Author: Baoyou Xie\u0026lt;baoyou.xie@linaro.org\u0026gt; Date: Thu Aug 17 20:43:09 2017 +0800 clk: samsung: mark symbols static where possible for s3c2443 We get 1 warning when building kernel with W=1: drivers/clk/samsung/clk-s3c2443.c:388:13: warning: no previous prototypefor \u0026#39;s3c2443_common_clk_init\u0026#39; [-Wmissing-prototypes] void __init s3c2443_common_clk_init(struct device_node *np, unsignedlong xti_f, In fact, this function is only used in the file in which they are declared and don\u0026#39;t need a declaration, but can be made static. So this patch marks these functions with \u0026#39;static\u0026#39;. commitc231d40296b4ee4667e3559e34b00f738cae1e58 Author: Baoyou Xie\u0026lt;baoyou.xie@linaro.org\u0026gt; Date: Thu Aug 17 20:41:38 2017 +0800 clk: samsung: mark symbols static where possible for s3c2412 We get 1 warning when building kernel with W=1: drivers/clk/samsung/clk-s3c2412.c:254:13: warning: no previous prototypefor \u0026#39;s3c2412_common_clk_init\u0026#39; [-Wmissing-prototypes] void __init s3c2412_common_clk_init(struct device_node *np, unsignedlong xti_f, In fact, this function is only used in the file in which they are declared and don\u0026#39;t need a declaration, but can be made static. So this patch marks these functions with \u0026#39;static\u0026#39;. commit ff8ea5ed4947d9a643a216d51f14f6cb87abcb97 Author: Baoyou Xie\u0026lt;baoyou.xie@linaro.org\u0026gt; Date: Thu Aug 17 20:40:50 2017 +0800 clk: samsung: mark symbols static where possible for s3c2410 **但是，你发现补丁描述里面还有什么不正确的吗？？**不过Maintainer也许发现不了这个问题，最后这个补丁也可能被接收入内核。\n下面我们生成补丁：\n1 2 3 4 root@ThinkPad-T440:/dimsum/git/kernel.next#git format-patch -s -3 0001-clk-samsung-mark-symbols-static-where-possible-for-s.patch 0002-clk-samsung-mark-symbols-static-where-possible-for-s.patch 0003-clk-samsung-mark-symbols-static-where-possible-for-s.patch 实际上，我们的补丁仍然是错误的。在发送补丁前，我们需要用脚本检查一下补丁：\n1 2 3 4 5 6 7 8 9 10 11 12 13 root@ThinkPad-T440:/dimsum/git/kernel.next#./scripts/checkpatch.pl 000* --------------------------------------------------------------- 0001-clk-samsung-mark-symbols-static-where-possible-for-s.patch --------------------------------------------------------------- WARNING: Possible unwrapped commit description (prefer a maximum 75 chars per line) #9: void__init s3c2410_common_clk_init(struct device_node *np, unsigned long xti_f, WARNING: line over 80 characters #29: FILE:drivers/clk/samsung/clk-s3c2410.c:363: +static void __init s3c2410_common_clk_init(struct device_node *np, unsigned long xti_f, total: 0 errors, 2 warnings, 8 lineschecked 请留意输出警告，其中第一个警告是说我们的描述中，有过长的语句。前面已经提到，这个警告可以忽略。但是第二个警告提示我们代码行超过80个字符了。这是不能忽略的警告，必须处理。\n使用 git reset HEAD~3 命令将三个补丁回退。重新修改代码：\n1 static void __init s3c2410_common_clk_init(struct device_node *np, unsigned long xti_f, 修改为\n1 2 static void __init s3c2410_common_clk_init(struct device_node *np, unsigned long xti_f, 重新提交补丁，并用 git format-patch 命令生成补丁。\n2.4 发送PATCH 生成正确的补丁后，请再次用checkpatch.pl检查补丁正确性。确保无误后，可以准备将它发送给Maintainer了。但是应该将补丁发给谁？这可以用get_maintainer.pl来查看：\n1 2 3 4 5 6 7 8 9 10 11 12 root@ThinkPad-T440:/dimsum/git/kernel.next#./scripts/get_maintainer.pl 000* Kukjin Kim \u0026lt;kgene@kernel.org\u0026gt;(maintainer:ARM/SAMSUNG EXYNOS ARM ARCHITECTURES) Krzysztof Kozlowski \u0026lt;krzk@kernel.org\u0026gt;(maintainer:ARM/SAMSUNG EXYNOS ARM ARCHITECTURES) Sylwester Nawrocki\u0026lt;s.nawrocki@samsung.com\u0026gt; (supporter:SAMSUNG SOC CLOCK DRIVERS) Tomasz Figa \u0026lt;tomasz.figa@gmail.com\u0026gt;(supporter:SAMSUNG SOC CLOCK DRIVERS) Chanwoo Choi \u0026lt;cw00.choi@samsung.com\u0026gt;(supporter:SAMSUNG SOC CLOCK DRIVERS) Michael Turquette\u0026lt;mturquette@baylibre.com\u0026gt; (maintainer:COMMON CLK FRAMEWORK) Stephen Boyd \u0026lt;sboyd@codeaurora.org\u0026gt;(maintainer:COMMON CLK FRAMEWORK) linux-arm-kernel@lists.infradead.org(moderated list:ARM/SAMSUNG EXYNOS ARM ARCHITECTURES) linux-samsung-soc@vger.kernel.org (moderatedlist:ARM/SAMSUNG EXYNOS ARM ARCHITECTURES) linux-clk@vger.kernel.org (open list:COMMONCLK FRAMEWORK) linux-kernel@vger.kernel.org (open list) 接下来，可以用 git send-email 命令发送补丁了：\n1 git send-email 000* --tokgene@kernel.org,krzk@kernel.org,s.nawrocki@samsung.com,tomasz.figa@gmail.com,cw00.choi@samsung.com,mturquette@baylibre.com,sboyd@codeaurora.org--cc linux-arm-kernel@lists.infradead.org,linux-samsung-soc@vger.kernel.org,linux-clk@vger.kernel.org,linux-kernel@vger.kernel.org 注意，哪些人应当作为邮件接收者，哪些人应当作为抄送者。在本例中，补丁是属于实验性质的，可以不抄送给邮件列表帐户。\n提醒：你应当将补丁先发给自己，检查无误后再发出去。如果你有朋友在社区有较高的威望，也可以抄送给他，必要的时候，也许他能给你一些帮助。这有助于将补丁顺利的合入社区。\n重要提醒：本文讲述的，主要是实验性质的补丁，用于打开社区大门。真正重要的补丁，可能需要经过反复修改，才能合入社区。我知道有一些补丁，超过两年时间都没能合入社区，因为总是有需要完善的地方，也许还涉及一些社区政治：）\n3 如何在github上规范的提交PR 如何在 Github 上规范的提交 PR（图文详解） - 知乎 (zhihu.com)\n如何在github上提交PR(Pull Request)-腾讯云开发者社区-腾讯云 (tencent.com)\n如何参与开源项目 - 细说 GitHub 上的 PR 全过程 - 胡涛的个人网站 | Seven Coffee Cups (danielhu.cn)\nGit工作流和核心原理 | GitHub基本操作 | VS Code里使用Git和关联GitHub_哔哩哔哩_bilibili\n给学完Git，还不会用GitHub的朋友们_哔哩哔哩_bilibili\n3.1 如何参与开源项目？ 寻找一个合适的开源项目 如果你就只是想开始参与开源，暂时还不知道该参与哪个社区，那么我有几个小建议：\n不要从特别成熟的项目开始。比如现在去参与 Kubernetes 社区，一方面由于贡献者太多，很难抢到一个入门级的 issue 来开始第一个 PR；另外一方面也由于贡献者太多，你的声音会被淹没，社区维护者并不在意多你一个或者少你一个（当然可能没有人会承认，但是你不得不信），如果你提个 PR 都遇到了各种问题还不能自己独立解决，那么很可能你的 PR 会直接超时关闭，没有人在意你是不是有一个好的参与体验； 不要从特别小的项目开始。这就不需要我解释了吧？很早期的开源项目可能面临着非常多的问题，比如代码不规范、协作流程不规范、重构频繁且不是 issue 驱动的，让外部参与者无所适从…… 选择知名开源软件基金会的孵化项目，这类项目一方面不是特别成熟，所以对新贡献者友好；另一方面也不会特别不成熟，不至于给人很差的参与体验，比如 Apache 基金会、Linux 基金会、CNCF 等。 寻找贡献点 开源项目的参与方式很多，最典型的方式是提交一个特性开发或者 bug 修复相关的 PR，但是其实文档完善、测试用例完善、bug 反馈等等也都是非常有价值的贡献。不过本文还是从需要提 PR 的贡献点开始上手，以 DevStream 项目为例（其他项目也一样），在项目 GitHub 代码库首页都会有一个 Issues 入口，这里会记录项目目前已知的 bug、proposal(可以理解成新需求)、计划补充的文档、亟需完善的 UT 等等，如下图：\n在 Issues 里我们一般可以找到一个“good first issue”标签标记的 issues，点击这个标签可以进一步直接筛选出所有的 good first issues，这是社区专门留给新贡献者的相对简单的入门级 issues：\n没错，从这里开始，浏览一下这些 good first issues，看下有没有你感兴趣的而且还没被分配的 issue，然后在下面留言，等待项目管理员分配任务后就可以开始编码了，就像这样：\n如图所示，如果一个 issue 还没有被认领，这时候你上去留个言，等待管理员会将这个任务分配给你，接着你就可以开始开发了。\n3.2 如何提交PR？ 一般开源项目代码库根目录都会有一个 CONTRIBUTING.md 或者其他类似名字的文档来介绍如何开始贡献。在 DevStream 的 Contributing 文档里我们放了一个 Development Workflow，其实就是 PR 工作流的介绍，不过今天，我要更详细地聊聊 PR 工作流。\nstep-1: Fork项目仓库 GitHub 上的项目都有一个 Fork 按钮，我们需要先将开源项目 fork 到自己的账号下，以 DevStream 为例：\n点一下 Fork 按钮，然后回到自己账号下，可以找到 fork 到的项目了：\n这个项目在你自己的账号下，也就意味着你有任意修改的权限了。我们后面要做的事情，就是将代码变更提到自己 fork 出来的代码库里，然后再通过 Pull Request 的方式将 commits 合入上游项目。\nstep-2: 克隆项目仓库到本地 对于任意一个开源项目，流程几乎都是一样的。我直接写了一些命令，大家可以复制粘贴直接执行。当然，命令里的一些变量还是需要根据你自己的实际需求修改，比如对于 DevStream 项目，我们可以先这样配置几个环境变量：\n1 2 3 4 export WORKING_PATH=\u0026#34;~/gocode\u0026#34; export USER=\u0026#34;daniel-hutao\u0026#34; export PROJECT=\u0026#34;devstream\u0026#34; export ORG=\u0026#34;devstream-io\u0026#34; 同理对于 DevLake，这里的命令就变成了这样：\n1 2 3 4 export WORKING_PATH=\u0026#34;~/gocode\u0026#34; export USER=\u0026#34;daniel-hutao\u0026#34; export PROJECT=\u0026#34;incubator-devlake\u0026#34; export ORG=\u0026#34;apache\u0026#34; 记得 USER 改成你的 GitHub 用户名，WORKING_PATH 当然也可以灵活配置，你想把代码放到哪里，就写对应路径。\n接着就是几行通用的命令来完成 clone 等操作了：\n1 2 3 4 5 6 7 8 9 10 mkdir -p ${WORKING_PATH} cd ${WORKING_PATH} # You can also use the url: git@github.com:${USER}/${PROJECT}.git # if your ssh configuration is proper git clone https://github.com/${USER}/${PROJECT}.git cd ${PROJECT} git remote add upstream https://github.com/${ORG}/${PROJECT}.git # Never push to upstream locally git remote set-url --push upstream no_push 如果你配置好了 ssh 方式来 clone 代码，当然，git clone 命令用的 url 可以改成\n1 git@github.com:${USER}/${PROJECT}.git 完成这一步后，我们在本地执行 git remote -v，看到的 remote 信息应该是这样的：\n1 2 3 4 origin\tgit@github.com:daniel-hutao/devstream.git (fetch) origin\tgit@github.com:daniel-hutao/devstream.git (push) upstream\thttps://github.com/devstream-io/devstream (fetch) upstream\tno_push (push) 记住啰，你本地的代码变更永远只提交到 origin，然后通过 origin 提交 Pull Request 到 upstream。\nstep-3: 更新本地分支代码 如果你刚刚完成 fork 和 clone 操作，那么你本地的代码肯定是新的。但是“刚刚”只存在一次，**接着每一次准备开始写代码之前，你都需要确认本地分支的代码是新的，**因为基于老代码开发你会陷入无限的冲突困境之中。你需要做以下两件事：\n更新本地的 main 分支代码\n1 2 3 git fetch upstream git checkout main git rebase upstream/main 当然，我不建议你直接在 main 分支写代码，虽然你的第一个 PR 从 main 提交完全没有问题，但是如果你需要同时提交2个 PR 呢？总之建议新增一个 feat-xxx 或者 fix-xxx 等更可读的分支来完成开发工作。\n创建分支\n1 git checkout -b feat-xxx 这样，我们就得到了一个和上游 main 分支代码一样的特性分支 feat-xxx 了，接着可以开始愉快地写代码啦！\nstep-4: 写代码 改代码吧！\nstep-5: add/commit/push 通用的流程如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 git add \u0026lt;file\u0026gt; git commit -s -m \u0026#34;some description here\u0026#34; git push origin feat-xxx Counting objects: 80, done. Delta compression using up to 10 threads. Compressing objects: 100% (74/74), done. Writing objects: 100% (80/80), 13.78 KiB | 4.59 MiB/s, done. Total 80 (delta 55), reused 0 (delta 0) remote: Resolving deltas: 100% (55/55), completed with 31 local objects. remote: remote: Create a pull request for \u0026#39;feat-1\u0026#39; on GitHub by visiting: remote: https://github.com/daniel-hutao/devstream/pull/new/feat-1 remote: To github.com:daniel-hutao/devstream.git * [new branch] feat-1 -\u0026gt; feat-1 当然，这里大家需要理解这几个命令和参数的含义，灵活调整。比如你也可以用 git add --all 完成 add 步骤，在 push 的时候也可以加 -f 参数，用来强制覆盖远程分支（假如已经存在，但是 commits 记录不合你意）。但是请记得 git commit 的 -s 参数一定要加哦！\n到这里，本地 commits 就推送到远程了。\nstep-6: 开一个PR 在完成 push 操作后，我们打开 GitHub，可以看到一个黄色的提示框，告诉我们可以开一个 Pull Request 了：\n如果你没有看到这个框，也可以直接切换到 feat-1 分支，然后点击下方的“Contribute”按钮来开启一个 PR，或者直接点 Issues 边上的 Pull requests 进入对应页面。Pull Request 格式默认是这样的：\n这里我们需要填写一个合适的标题（默认和 commit message 一样），然后按照模板填写 PR 描述。PR 模板其实在每个开源项目里都不太一样，我们需要仔细阅读上面的内容，避免犯低级错误。\n比如 DevStream 的模板里目前分为4个部分：\nPre-Checklist：这里列了3个前置检查项，提醒 PR 提交者要先阅读 Contributing 文档，然后代码要有完善的注释或者文档，尽可能添加测试用例等； Description：这里填写的是 PR 的描述信息，也就是介绍你的 PR 内容的，你可以在这里描述这个 PR 解决了什么问题等； Related Issues：记得吗？我们在开始写代码之前其实是需要认领 issue 的，这里要填写的也就是对应 issue 的 id，假如你领的 issue 链接是 https://github.com/devstream-io/devstream/issues/796，并且这个 issue 通过你这个 PR 的修改后就完成了，可以关闭了，这时候可以在 Related Issues 下面写“close #796”； New Behavior：代码修改后绝大多数情况下是需要进行测试的，这时候我们可以在这里粘贴测试结果截图，这样 reviewers 就能够知道你的代码已经通过测试，功能符合预期，这样可以减少 review 工作量，快速合入。 这个模板并不复杂，我们直接对着填写就行。像这样：\n然后点击右下角 Create pull request 就完成了一个 PR 的创建了。不过我这里不能去点这个按钮，我用来演示的修改内容没有意义，不能合入上游代码库。不过我还是想给你看下 PR 创建出来后的效果，我们以 pr655 为例吧。\n这是上个月我提的一个 PR，基本和模板格式一致。除了模板的内容，可能你已经注意到这里多了一个 Test 小节，没错，模板不是死的，模板只是为了降低沟通成本，你完全可以适当调整，只要结果是“往更清晰的方向走”的。我这里通过 Test 部分添加了本地详细测试结果记录，告诉 reviewers 我已经在本地充分测试了，请放心合入。\n提交了 PR 之后，我们就可以在 PR 列表里找到自己的 PR 了，这时候还需要注意 ci 检查是不是全部能够通过，假如失败了，需要及时修复。以 DevStream 为例，ci 检查项大致如下：\nstep-7: PR合入 如果你的 PR 很完美，毫无争议，那么过不了太长时间，项目管理员会直接合入你的 PR，那么你这个 PR 的生命周期也就到此结束了。\n但是，没错，这里有个“但是”，但是往往第一次 PR 不会那么顺利，我们接下来就详细介绍一下可能经常遇到的一些问题和对应的解决办法。\n3.3 提交PR可能遇到的问题 多数情况下，提交一个 PR 后是不会被马上合入的，reviewers 可能会提出各种修改意见，或者我们的 PR 本身存在一些规范性问题，或者 ci 检查就直接报错了，怎么解决呢？继续往下看吧。\nQ1: 基于Reviewer的意见更新PR 很多时候，我们提交了一个 PR 后，还需要继续追加 commit，比如提交后发现代码还有点问题，想再改改，或者 reviewers 提了一些修改意见，我们需要更新代码。\n一般我们遵守一个约定：\n在 review 开始之前，更新代码尽量不引入新的 commits 记录，也就是能合并就合并，保证 commits 记录清晰且有意义； 在 review 开始之后，针对 reviewers 的修改意见所产生的新 commit，可以不向前合并，这样能够让二次 review 工作更有针对性。 不过不同社区要求不一样，可能有的开源项目会**要求一个 PR 里只能包含一个 commit，**大家根据实际场景灵活判断即可。\n说回如何更新 PR，我们只需要在本地继续修改代码，然后通过和第一个 commit 一样的步骤，执行这几个命令：\n1 2 3 git add \u0026lt;file\u0026gt; git commit -s -m \u0026#34;some description here\u0026#34; git push origin feat-xxx 这时候别看 push 的是 origin 的 feat-xxx 分支，其实 GitHub 会帮你把新增的 commits 全部追加到一个未合入 PR 里去。没错，你只管不断 push，PR 会自动更新。\n至于如何合并 commits，我们下一小节具体介绍。\nQ2: 合并多且混乱的Commits 很多情况下我们需要去合并 commits，比如你的第一个 commit 里改了100行代码，然后发现少改了1行，这时候又提交了一个 commit，那么第二个 commit 就太 “没意思” 了，我们需要合并一下。\n比如我这里有2个同名的 commits，第二个 commit 其实只改了一个标点：\n这时候我们可以通过 rebase 命令来完成2个 commits 的合并：\n1 git rebase -i HEAD~2 执行这个命令会进入一个编辑页面，默认是 vim 编辑模式，内容大致如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 pick 3114c0f docs: just for test pick 9b7d63b docs: just for test # Rebase d640931..9b7d63b onto d640931 (2 commands) # # Commands: # p, pick = use commit # r, reword = use commit, but edit the commit message # e, edit = use commit, but stop for amending # s, squash = use commit, but meld into previous commit # f, fixup = like \u0026#34;squash\u0026#34;, but discard this commit\u0026#39;s log message # x, exec = run command (the rest of the line) using shell # d, drop = remove commit # # These lines can be re-ordered; they are executed from top to bottom. # # If you remove a line here THAT COMMIT WILL BE LOST. # # However, if you remove everything, the rebase will be aborted. 我们需要把第二个 pick 改成 s，然后保存退出：\n1 2 pick 3114c0f docs: just for test s 9b7d63b docs: just for test 接着会进入第二个编辑页面：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # This is a combination of 2 commits. # This is the 1st commit message: docs: just for test Signed-off-by: Daniel Hu \u0026lt;tao.hu@merico.dev\u0026gt; # This is the commit message #2: docs: just for test Signed-off-by: Daniel Hu \u0026lt;tao.hu@merico.dev\u0026gt; # Please enter the commit message for your changes. Lines starting # with \u0026#39;#\u0026#39; will be ignored, and an empty message aborts the commit. # ... 这是用来编辑合并后的 commit message 的，我们直接删掉多余部分，只保留这样几行：\n1 2 3 docs: just for test Signed-off-by: Daniel Hu \u0026lt;tao.hu@merico.dev\u0026gt; 接着同样是 vim 的保存退出操作，这时候可以看到日志：\n1 2 3 4 [detached HEAD 80f5e57] docs: just for test Date: Wed Jul 6 10:28:37 2022 +0800 1 file changed, 2 insertions(+) Successfully rebased and updated refs/heads/feat-1. 这时候可以通过git log命令查看下 commits 记录是不是符合预期：\n好，我们在本地确认 commits 已经完成合并，这时候就可以继续推送到远程，让 PR 也更新掉：\n1 git push -f origin feat-xxx 这里需要有一个 -f 参数来强制更新，合并了 commits 本质也是一种冲突，需要刷掉远程旧的 commits 记录。\nQ3: 解决PR冲突 冲突可以在线解决，也可能本地解决，我们逐个来看。\n在线解决冲突 我们要尽可能避免冲突，养成每次写代码前更新本地代码的习惯。不过，冲突不可能完全避免，有时候你的 PR 被阻塞了几天，可能别人改了同一行代码，还抢先被合入了，这时候你的 PR 就出现冲突了，类似这样（同样，此刻我不能真的去上游项目构造冲突，所以下面用于演示的冲突在我在自己的 repo 里）：\n每次看到这个页面都会让人觉得心头一紧。我们点击 Resolve conflicts 按钮，就可以看到具体冲突的内容了：\n可以看到具体冲突的行了，接下来要做的就是解决冲突。我们需要删掉所有的 \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;、\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; 和 ======= 标记，只保留最终想要的内容，如下：\n接着点击右上角的“Mark as Resolved”：\n最后点击“Commit merge”：\n这样就完成冲突解决了，可以看到产生了一个新的 commit：\n到这里，冲突就解决掉了。\n本地解决冲突 更多时候，我们需要在本地解决冲突，尤其是冲突太多，太复杂的时候。同样，我们构造一个冲突，这次尝试在本地解决冲突。\n首先，可以在线看一下冲突的内容：\n接着我们在本地执行：\n1 2 3 4 5 6 # 先切回到 main 分支 git checkout main # 拉取上游代码（实际场景肯定是和上游冲突，我们这里的演示环境其实是 origin） git fetch upstream # 更新本地 main（这里也可以用 rebase，但是 reset 不管有没有冲突总是会成功） git reset --hard upstream/main 到这里，本地 main 分支就和远程(或者上游) main 分支代码完全一致了，然后我们要做的是将 main 分支的代码合入自己的特性分支，同时解决冲突。\n1 2 git checkout feat-1 git rebase main 这时候会看到这样的日志：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 First, rewinding head to replay your work on top of it... Applying: docs: conflict test 1 Using index info to reconstruct a base tree... M README.md Falling back to patching base and 3-way merge... Auto-merging README.md CONFLICT (content): Merge conflict in README.md error: Failed to merge in the changes. Patch failed at 0001 docs: conflict test 1 The copy of the patch that failed is found in: .git/rebase-apply/patch Resolve all conflicts manually, mark them as resolved with \u0026#34;git add/rm \u0026lt;conflicted_files\u0026gt;\u0026#34;, then run \u0026#34;git rebase --continue\u0026#34;. You can instead skip this commit: run \u0026#34;git rebase --skip\u0026#34;. To abort and get back to the state before \u0026#34;git rebase\u0026#34;, run \u0026#34;git rebase --abort\u0026#34;. 我们需要解决冲突，直接打开 README.md，找到冲突的地方，直接修改。这里的改法和上一小节介绍的在线解决冲突没有任何区别，我就不赘述了。代码里同样只保留最终内容，然后继续 git 命令走起来：\n可能此时你并不放心，那就通过git log命令看一下 commits 历史记录吧：\n这里的“conflict test 2”是我提交到 main 分支的记录，可以看到这个时间比“conflict test 1”还要晚了一些，但是它先合入了。我们在 rebase 操作后，这个记录在前，我们特性分支的“conflict test 1”在后，看起来很和谐，我们继续将这个变更推送到远程，这个命令已经出现很多次了：\n1 git push -f origin feat-xxx 这时候我们再回到 GitHub 看 PR 的话，可以发现冲突已经解决了，并且没有产生多余的 commit 记录，也就是说这个 PR 的 commit 记录非常干净，好似冲突从来没有出现过：\n至于什么时候可以在线解决冲突，什么时候适合本地解决冲突，就看大家如何看待“需不需要保留解决冲突的记录”了，不同社区的理解不一样，可能特别成熟的开源社区会希望使用本地解决冲突方式，因为在线解决冲突产生的这条 merge 记录其实“没营养”。\nQ4: CI检查不过 commit-message问题修复 前面我们提到过 commit message 的规范，但是第一次提交 PR 的时候还是很容易出错，比如 feat: xxx 其实能通过 ci 检查，但是 feat: Xxx 就不行了。假设现在我们不小心提交了一个 PR，但是里面 commit 的 message 不规范，这时候怎么修改呢？\n这个比较简单，直接执行：\n1 git commit --amend 这条命令执行后就能进入编辑页面，随意更新 commit message 了。改完之后，继续 push：\n1 git push -f origin feat-xxx 这样就能更新 PR 里的 commit message 了。\nDCO(sign)问题修复 相当多的开源项目会要求所有合入的 commits 都包含一行类似这样的记录：\n1 Daniel Hu \u0026lt;tao.hu@merico.dev\u0026gt; 所以 commit message 看起来会像这样：\n1 2 3 feat: some description here Signed-off-by: Daniel Hu \u0026lt;tao.hu@merico.dev\u0026gt; 这行信息相当于是对应 commit 的作者签名。要添加这样一行签名当然很简单，我们直接在 git commit 命令后面加一个 -s 参数就可以了，比如 git commit -s -m \u0026quot;some description here\u0026quot; 提交的 commit 就会带上你的签名。\n但是如果如果你第一次提交的 PR 里忘记了在 commits 中添加 Signed-off-by 呢？这时候，如果对应开源项目配置了 DCO 检查，那么你的 PR 就会在 ci 检查中被 “揪出来” 没有正确签名。\n我们同样先构造一个没有加签名的 commit：\n如果提了 PR，看到的效果是这样的：\n我们看下如何解决，执行以下命令即可：\n1 git commit -amend -s 这样一个简单的命令，就能直接在最近一个 commit 里加上 Signed-off-by 信息。执行这行命令后会直接进入 commit message 编辑页面，默认如下：\n1 2 3 docs: dco test Signed-off-by: Daniel Hu \u0026lt;tao.hu@merico.dev\u0026gt; 完成签名后呢？当然是来一个强制 push 了：\n1 git push -f origin feat-xxx 这样，你 PR 中的 DCO 报错就自然修复了。\n4 Linux/upstream 4.1 perf-nev 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # b4 tools sudo apt install b4 -y # 从lkml上拉取patch b4 am https://lore.kernel.org/all/20240422080833.8745-1-liangshenlin@eswincomputing.com/ # 打patch git am ./v3_20240422_liangshenlin_perf_kvm_add_kvm_stat_support_on_riscv.mbx # 生成patch git format-patch --cover-letter --thread --subject-prefix=\u0026#34;PATCH\u0026#34; --base=auto - git clone --depth=1 -b perf-kvm-stat-v3 https://github.com/zcxGGmu/linux.git -j4 1 2 3 4 5 6 // perf dependency apt install --no-install-recommends -y util-linux haveged openssh-server systemd kmod initramfs-tools conntrack ebtables ethtool iproute2 iptables mount socat ifupdown iputils-ping vim dhcpcd5 neofetch sudo chrony python-devel python-dev apt install -y pkgconf libzstd-dev libtraceevent-dev libelf-dev libslang2-dev elfutils libdebuginfod-dev systemtap-sdt-dev libdw-dev libcapstone-dev libcap-dev libaudit-dev libpython3-dev libnuma-dev libbabeltrace-dev libssl-dev libpfm4-dev libperl-dev // perf install make VF=1 NO_LIBBPF=1 install -j4 1 2 3 4 5 6 7 8 9 10 // qemu dependency apt install gcc g++ wget flex bison bc cpio make pkg-config git libglib2.0-dev libfdt-dev libpixman-1-dev zlib1g-dev libncurses-dev libssl-dev ninja-build // qemu compile wget https://mirrors.aliyun.com/blfs/conglomeration/qemu/qemu-9.0.0.tar.xz tar -xvf qemu-9.0.0.tar.xz cd qemu-9.0.0 mkdir build \u0026amp;\u0026amp; cd ./build ./configure --target-list=\u0026#34;riscv64-softmmu\u0026#34; make -j`nproc` 4.2 Thunderbird 1 2 3 4 // Colored Diffs // Owl for Exchange // composition =\u0026gt; palin text ","date":"2024-10-17T09:50:37+08:00","permalink":"https://zcxggmu.github.io/p/riscv-upstream-work/","title":"Riscv Upstream Work"},{"content":"0 参考 QEMU+GDB调试linux内核驱动 - 知乎 (zhihu.com)\n构建riscv两层qemu的步骤 | Sherlock\u0026rsquo;s blog (wangzhou.github.io)\n用 QEMU/Spike+KVM 运行 RISC-V Host/Guest Linux - 泰晓科技 (tinylab.org)\nriscv KVM虚拟化分析 | Sherlock\u0026rsquo;s blog (wangzhou.github.io)\n无标题文档 (yuque.com)\n1 QEMU/GDB调试内核驱动 2 riscv两层qemu环境配置 可以构建一个两层qemu的环境来调试问题，第一层qemu启动的时候打开qemu的虚拟化扩展，这个可以作为一个支持虚拟化扩展的riscv硬件平台，第二层qemu启动的时候 打开kvm支持。\n注意，第二层qemu的编译比较有意思，因为qemu编译需要依赖很多动态库，我用的都是交叉编译编译riscv的程序，所以，需要先交叉编译qemu依赖的动态库，然后再交叉编译qemu，太麻烦了。我们这里用编译buildroot的方式一同编译小文件系统里的qemu, buildroot编译qemu的时候就会一同编译qemu依赖的各种库, 这样编译出的host文件系统里就带了qemu。\n2.1 在主机环境中完成的工作 具体流程如下：\nqemu的编译安装 编译第一层qemu运行的内核，需要开启linux内核的kvm模块 编译根文件系统（如果想在第一层qemu启动后的环境中直接运行qemu，需要编译buildroot时一同编译qemu） qemu编译安装 创建工作目录：\n1 2 mkdir riscv64-kvm cd riscv64-kvm 下载qemu源码：\nblfs-conglomeration-qemu安装包下载_开源镜像站-阿里云 (aliyun.com)\n1 2 wget https://download.qemu.org/qemu-8.0.0.tar.xz tar xvJf qemu-8.0.0.tar.xz 编译安装：\n1 2 3 4 5 6 7 cd qemu-8.0.0/ mkdir build cd build ../configure --enable-kvm --enable-virtfs --target-list=riscv64-linux-user,riscv64-softmmu \\ --prefix=/opt/software/toolchain/qemu make -j`nproc` sudo make install 其中的 ：\n--target-list 为将要生成的目标平台：\nriscv-64-linux-user 为用户模式，可以运行基于riscv指令集编译的程序文件\nsoftmmu 为系统模式，可以运行基于riscv指令集编译的linux镜像；\n--enable-kvm 为把kvm编译进qemu里；\n--enable-virtfs 为qemu使用共享文件夹的参数\n使用此选项需要安装一些依赖：\n1 sudo apt install libcap-ng-dev libcap-dev libcap-ng-utils libattr1 libattr1-dev 验证安装是否成功：\n1 qemu-system-riscv64 --version 若输出qemu版本信息则安装成功。\n内核编译 下载内核源码：\nlinux-kernel安装包下载_开源镜像站-阿里云 (aliyun.com)\n这里下载的是5.19内核。\n1 2 3 git clone https://github.com/kvm-riscv/linux.git # wget https://mirrors.aliyun.com/linux-kernel/v5.x/linux-5.19.tar.xz # tar xvJf linux-5.19.tar.xz 接着，创建编译目录并配置处理器架构和交叉编译器等环境变量：\n1 2 3 export ARCH=riscv export CROSS_COMPILE=riscv64-linux-gnu- mkdir build-riscv64 接着，通过 menuconfig 配置内核选项。在配置之前，需要注意最新版 Linux 内核默认关闭 RISC-V SBI 相关选项，需要通过以下命令手动配置开启，相关讨论参见该 issue，具体细节参见 Linux kernel 配置修改 - 知乎 (zhihu.com)。\n1 2 # change options of kernel compiling to generate build-riscv64/.config (output dir) make -C linux-5.19 O=`pwd`/build-riscv64 menuconfig 最后一个环节就是编译了：\n1 make -C linux-5.19 O=`pwd`/build-riscv64 -j`nproc` 编译完，咱们获得了两个重要的二进制文件：\n内核映像：build-riscv64/arch/riscv/boot/Image KVM 内核模块：build-riscv64/arch/riscv/kvm/kvm.ko buildroot编译 这一层的qemu我们可以用一种更便捷的方法，用编译buildroot的方式一同编译根文件系统里的qemu, buildroot编译qemu的时候就会一同编译qemu依赖的各种库, 这样编译出的根文件系统里就带了qemu。\n首先下载buildroot工具：\n1 2 3 git clone https://github.com/buildroot/buildroot.git cd buildroot make menuconfig 选择RISC-V架构：\nTarget options ---\u0026gt; Target Architecture （i386）---\u0026gt; (x) RISCV\n选择ext文件系统：\nFilesystem images ---\u0026gt; [*] ext2/3/4 root filesystem\n下方的 exact size 可以调整ext文件系统大小配置，默认为60M，这里需要调整到500M以上，因为需要编译qemu文件进去\nbuildroot配置qemu\n1 2 3 4 5 6 7 8 9 10 BR2_TOOLCHAIN_BUILDROOT_GLIBC=y # Toolchain -\u0026gt; C library (\u0026lt;choice\u0026gt; [=y]) glibc BR2_USE_WCHAR=y # BR2_TOOLCHAIN_USES_GLIBC=y BR2_PACKAGE_QEMU=y # QEMU BR2_TARGET_ROOTFS_CPIO=y # cpio the root filesystem (for use as an initial RAM filesystem) BR2_TARGET_ROOTFS_CPIO_GZIP=y Prompt: gzip │ Location: │ -\u0026gt; Filesystem images │ (1) -\u0026gt; cpio the root filesystem (for use as an initial RAM filesystem) (BR2_TARGET_ROOTFS_CPIO [=n]) │ -\u0026gt; Compression method (\u0026lt;choice\u0026gt; [=n]) 在可视化页面按/ 即可进入搜索模式，在搜索模式分别输入上述参数：\n全部开启后保存退出\nmake -j 编译，完成后在output/images目录下得到rootfs.ext2，将它复制到工作目录。这里编译的qemu版本为8.1.0。\n第二层qemu运行的内核，就使用第一次编译的内核即可。\n2.2 从主机运行脚本 第一层qemu的脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash sudo qemu-system-riscv64 \\ -M virt \\ -cpu \u0026#39;rv64,h=true\u0026#39; \\ -m 2G \\ -kernel Image \\ -append \u0026#34;rootwait root=/dev/vda ro\u0026#34; \\ -drive file=rootfs.ext2,format=raw,id=hd0 \\ -device virtio-blk-device,drive=hd0 \\ -nographic \\ -virtfs local,path=/home/wx/Documents/shared,mount_tag=host0,security_model=passthrough,id=host0 \\ -netdev user,id=net0 -device virtio-net-device,netdev=net0 qemu7.0.0之前的版本使用-cpu rv64,x-h=true能使CPU虚拟化扩展，在qemu v7.0.0以及之后的版本使用-cpu rv64,h=true能使CPU虚拟化扩展\n执行上述命令启动QEMU后，root账号登录Linux系统，然后执行 mount 命令挂载宿主机目录，用于文件共享：\n1 2 mkdir -p /mnt/shared mount -t 9p -o trans=virtio,version=9p2000.L host0 /mnt/shared //error: 第二层qemu的脚本 1 2 3 4 5 6 7 8 9 10 11 #!/bin/sh /usr/bin/qemu-system-riscv64 \\ -M virt --enable-kvm \\ -cpu rv64 \\ -m 256m \\ -kernel ./Image \\ -append \u0026#34;rootwait root=/dev/vda ro\u0026#34; \\ -drive file=rootfs.ext2,format=raw,id=hd0 \\ -device virtio-blk-device,drive=hd0 \\ -nographic error =\u0026gt; qemu-system-riscv64: Unable to read ISA_EXT KVM register ssaia, error -1\n-cpu 未支持 AIA -machine 位置添加 AIA ","date":"2024-10-17T09:49:48+08:00","permalink":"https://zcxggmu.github.io/p/gdb-kvm/","title":"GDB KVM"},{"content":"0 前言 kernel: 4.14\narch: ARM64\nirq_chip: GICv2\n本文重点分析 Linux 中断子系统，中断是处理器用于异步处理外围设备请求的一种机制，可以说中断处理是操作系统管理外围设备的基石，此外系统调度、核间交互等都离不开中断，它的重要性不言而喻。来一张分层图：\n**硬件层：**最下层为硬件连接层，对应的是具体的外设与SoC的物理连接，中断信号是从外设到中断控制器，由中断控制器统一管理，再路由到处理器上； **硬件相关层：**这个层包括两部分代码，一部分是架构相关的，比如ARM64处理器处理中断相关，另一部分是中断控制器的驱动代码； **通用层：**这部分也可以认为是框架层，是硬件无关层，这部分代码在所有硬件平台上是通用的； **用户层：**这部分也就是中断的使用者了，主要是各类设备驱动，通过中断相关接口来进行申请和注册，最终在外设触发中断时，进行相应的回调处理； 本文不会详细分析GICv2的硬件原理，因为不同架构的中断控制器实现都不太一样，但这些 irq_chip 接入 Linux内核后关于中断初始化、中断处理流程的框架是相同的。之前看了关于 riscv AIA Linux support 的一组PATCH，对其中相当多的接口不理解，所以想通过本文搞清楚 Linux 中断框架，将框架和 irq_chip 相关的代码分离开来。\n1 硬件相关层 1.1 GIC驱动分析 设备信息添加 ARM平台的设备信息，都是通过Device Tree设备树来添加，设备树信息放置在arch/arm64/boot/dts/下：\n下图就是一个中断控制器的设备树信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // /arch/arm64/boot/dts/arm/foundation-v8.dts / { gic: interrupt-controller@2c001000 { compatible = \u0026#34;arm,cortex-a15-gic\u0026#34;, \u0026#34;arm,cortex-a9-gic\u0026#34;; #interrupt-cells = \u0026lt;3\u0026gt;; #address-cells = \u0026lt;2\u0026gt;; interrupt-controller; reg = \u0026lt;0x0 0x2c001000 0 0x1000\u0026gt;, \u0026lt;0x0 0x2c002000 0 0x2000\u0026gt;, \u0026lt;0x0 0x2c004000 0 0x2000\u0026gt;, \u0026lt;0x0 0x2c006000 0 0x2000\u0026gt;; interrupts = \u0026lt;1 9 0xf04\u0026gt;; }; }; **compatible字段：**用于与具体的驱动来进行匹配，比如图片中 arm, cortex-a15-gic ，可以根据这个名字去匹配对应的驱动程序； **interrupt-cells字段：**用于指定编码一个中断源所需要的单元个数，这个值为3。比如在外设在设备树中添加中断信号时，通常能看到类似 interrupts = \u0026lt;0 23 4\u0026gt;;的信息，第一个单元0，表示的是中断类型（1：PPI，0：SPI），第二个单元 23 表示的是中断号，第三个单元 4 表示的是中断触发的类型； **interrupt-controller字段：**表示该设备是一个中断控制器，外设可以连接在该中断控制器上； **reg字段：**描述中断控制器的地址信息以及地址范围，比如图片中分别制定了 GIC Distributor（GICD）和GIC CPU Interface（GICC）的地址信息； 关于设备数的各个字段含义，详细可以参考Documentation/devicetree/bindings下的对应信息； 设备树的信息，是怎么添加到系统中的呢？ Device Tree 最终会编译成 dtb 文件，并通过 Uboot 传递给内核，在内核启动后会将 dtb 文件解析成 device_node 结构。关于设备树的相关知识，本文先不展开，后续再找机会补充。来一张图，先简要介绍下关键路径：\n1 2 3 4 5 6 start_kernel +-\u0026gt; setup_arch +-\u0026gt; unflatten_device_stree //解析dtb文件 +-\u0026gt; unflatten_dt_nodes +-\u0026gt; populate_node +-\u0026gt; unflatten_dt_alloc 设备树的节点信息，最终会变成device_node结构，在内存中维持一个树状结构。设备与驱动，会根据compatible 字段进行匹配 。\n驱动流程分析 GIC驱动的执行流程如下图所示：\n关于链接脚本vmlinux.lds，脚本中定义了一个 __irqchip_of_table 段，该段用于存放中断控制器信息，用于最终来匹配设备； 在GIC驱动程序中，使用 IRQCHIP_DECLARE 宏来声明结构信息，包括 compatible 字段和回调函数，该宏会将这个结构放置到 __irqchip_of_table 段中； 在内核启动初始化中断的函数中，of_irq_init 函数会去查找设备节点信息，该函数的传入参数就是__irqchip_of_table 段，由于 IRQCHIP_DECLARE 已经将信息填充好了，of_irq_init 函数会根据arm,gic-400 去查找对应的设备节点，并获取设备的信息。中断控制器也存在级联的情况，of_irq_init 函数中也处理了这种情况； or_irq_init 函数中，最终会回调IRQCHIP_DECLARE声明的回调函数，也就是gic_of_init，而这个函数就是GIC驱动的初始化入口函数了； GIC的工作，本质上是由中断信号来驱动，因此**驱动本身的工作就是完成各类信息的初始化，**注册好相应的回调函数，以便能在信号到来之时去执行；\nset_smp_process_call 设置 __smp_cross_call 函数指向 gic_raise_softirq，本质上就是通过软件来触发GIC的 SGI中断 ，用于核间交互；\ncpuhp_setup_state_nocalls 函数，设置好CPU进行热插拔时GIC的回调函数，以便在CPU热插拔时做相应处理；\nset_handle_irq 函数的设置很关键，它将全局函数指针 handle_arch_irq 指向了 gic_handle_irq，而处理器在进入中断异常时，会跳转到 handle_arch_irq 执行，所以，可以认为它就是中断处理的入口函数了；\n驱动中完成了各类函数的注册，此外还完成了 irq_chip, irq_domain 等结构体的初始化，这些结构在下文会进一步分析。最后，完成GIC硬件模块的初始化设置，以及电源管理相关的注册等工作；\n我们可以看到，gic_* 作为前缀的函数都是和特定的GIC中断控制器相关的，如果换作RISC-V架构，这些函数就会被替换为 plic_*/aplic_* 等等。\n数据结构分析 各数据结构的依赖关系如下图：\nGIC驱动中，使用 struct gic_chip_data 结构体来描述GIC控制器的信息，整个驱动都是围绕着该结构体的初始化，驱动中将函数指针都初始化好，实际的工作是由中断信号触发，也就是在中断来临的时候去进行回调。\nstruct irq_chip 结构，描述的是中断控制器的底层操作函数集，这些函数集最终完成对控制器硬件的操作 struct irq_domain结构，用于硬件中断号和Linux IRQ中断号（virq，虚拟中断号）之间的映射； 还是上一下具体的数据结构代码吧，关键注释如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 struct irq_chip { struct device\t*parent_device; //指向父设备 const char\t*name; // /proc/interrupts中显示的名字 unsigned int\t(*irq_startup)(struct irq_data *data); //启动中断，如果设置成NULL，则默认为enable void\t(*irq_shutdown)(struct irq_data *data); //关闭中断，如果设置成NULL，则默认为disable void\t(*irq_enable)(struct irq_data *data); //中断使能，如果设置成NULL，则默认为chip-\u0026gt;unmask void\t(*irq_disable)(struct irq_data *data); //中断禁止 void\t(*irq_ack)(struct irq_data *data); //开始新的中断 void\t(*irq_mask)(struct irq_data *data); //中断源屏蔽 void\t(*irq_mask_ack)(struct irq_data *data); //应答并屏蔽中断 void\t(*irq_unmask)(struct irq_data *data); //解除中断屏蔽 void\t(*irq_eoi)(struct irq_data *data); //中断处理结束后调用 int\t(*irq_set_affinity)(struct irq_data *data, const struct cpumask *dest, bool force); //在SMP中设置CPU亲和力 int\t(*irq_retrigger)(struct irq_data *data); //重新发送中断到CPU int\t(*irq_set_type)(struct irq_data *data, unsigned int flow_type); //设置中断触发类型 int\t(*irq_set_wake)(struct irq_data *data, unsigned int on); //使能/禁止电源管理中的唤醒功能 void\t(*irq_bus_lock)(struct irq_data *data); //慢速芯片总线上的锁 void\t(*irq_bus_sync_unlock)(struct irq_data *data); //同步释放慢速总线芯片的锁 void\t(*irq_cpu_online)(struct irq_data *data); void\t(*irq_cpu_offline)(struct irq_data *data); void\t(*irq_suspend)(struct irq_data *data); void\t(*irq_resume)(struct irq_data *data); void\t(*irq_pm_shutdown)(struct irq_data *data); void\t(*irq_calc_mask)(struct irq_data *data); void\t(*irq_print_chip)(struct irq_data *data, struct seq_file *p); int\t(*irq_request_resources)(struct irq_data *data); void\t(*irq_release_resources)(struct irq_data *data); void\t(*irq_compose_msi_msg)(struct irq_data *data, struct msi_msg *msg); void\t(*irq_write_msi_msg)(struct irq_data *data, struct msi_msg *msg); int\t(*irq_get_irqchip_state)(struct irq_data *data, enum irqchip_irq_state which, bool *state); int\t(*irq_set_irqchip_state)(struct irq_data *data, enum irqchip_irq_state which, bool state); int\t(*irq_set_vcpu_affinity)(struct irq_data *data, void *vcpu_info); void\t(*ipi_send_single)(struct irq_data *data, unsigned int cpu); void\t(*ipi_send_mask)(struct irq_data *data, const struct cpumask *dest); unsigned long\tflags; }; struct irq_domain { struct list_head link; //用于添加到全局链表irq_domain_list中 const char *name; //IRQ domain的名字 const struct irq_domain_ops *ops; //IRQ domain映射操作函数集 void *host_data; //在GIC驱动中，指向了irq_gic_data unsigned int flags; unsigned int mapcount; //映射中断的个数 /* Optional data */ struct fwnode_handle *fwnode; enum irq_domain_bus_token bus_token; struct irq_domain_chip_generic *gc; #ifdef\tCONFIG_IRQ_DOMAIN_HIERARCHY struct irq_domain *parent; //支持级联的话，指向父设备 #endif #ifdef CONFIG_GENERIC_IRQ_DEBUGFS struct dentry\t*debugfs_file; #endif /* reverse map data. The linear map gets appended to the irq_domain */ irq_hw_number_t hwirq_max; //IRQ domain支持中断数量的最大值 unsigned int revmap_direct_max_irq; unsigned int revmap_size; //线性映射的大小 struct radix_tree_root revmap_tree; //Radix Tree映射的根节点 unsigned int linear_revmap[]; //线性映射用到的查找表 }; struct irq_domain_ops { int (*match)(struct irq_domain *d, struct device_node *node, enum irq_domain_bus_token bus_token); // 用于中断控制器设备与IRQ domain的匹配 int (*select)(struct irq_domain *d, struct irq_fwspec *fwspec, enum irq_domain_bus_token bus_token); int (*map)(struct irq_domain *d, unsigned int virq, irq_hw_number_t hw); //用于硬件中断号与Linux中断号的映射 void (*unmap)(struct irq_domain *d, unsigned int virq); int (*xlate)(struct irq_domain *d, struct device_node *node, const u32 *intspec, unsigned int intsize, unsigned long *out_hwirq, unsigned int *out_type); //通过device_node，解析硬件中断号和触发方式 #ifdef\tCONFIG_IRQ_DOMAIN_HIERARCHY /* extended V2 interfaces to support hierarchy irq_domains */ int (*alloc)(struct irq_domain *d, unsigned int virq, unsigned int nr_irqs, void *arg); void (*free)(struct irq_domain *d, unsigned int virq, unsigned int nr_irqs); void (*activate)(struct irq_domain *d, struct irq_data *irq_data); void (*deactivate)(struct irq_domain *d, struct irq_data *irq_data); int (*translate)(struct irq_domain *d, struct irq_fwspec *fwspec, unsigned long *out_hwirq, unsigned int *out_type); #endif }; 关于IRQ domain IRQ domain用于将硬件的中断号，转换成Linux系统中的中断号（virtual irq, virq），来张图：\n每个中断控制器都对应一个IRQ Domain； 中断控制器驱动通过 irq_domain_add_*() 接口来创建 IRQ Domain； IRQ Domain支持三种映射方式：linear map（线性映射），tree map（树映射），no map（不映射） linear map: 维护固定大小的表，索引是硬件中断号，如果硬件中断最大数量固定，并且数值不大，可以选择线性映射； tree map: 硬件中断号可能很大，可以选择树映射； no map: 硬件中断号直接就是Linux的中断号； 三种映射的方式如下图：\n图中描述了三个中断控制器，对应到三种不同的映射方式。各个控制器的硬件中断号可以一样，最终在Linux内核中映射的中断号是唯一的。\n1.2 架构相关代码 中断也是异常模式的一种，当外设触发中断时，处理器会切换到特定的异常模式进行处理，而这部分代码都是架构相关的。ARM64的代码位于 arch/arm64/kernel/entry.S。\nARM64 处理器有四个异常级别 Exception Level：0~3，EL0 对应用户态程序，EL1 对应操作系统内核态，EL2对应Hypervisor，EL3 对应Secure Monitor。\n当异常触发时，处理器进行模式切换，并且跳转到异常向量表开始执行，针对中断异常，最终会跳转到irq_handler 中。代码比较简单，如下：\n1 2 3 4 5 6 7 8 9 10 /* * Interrupt handling. */ .macro\tirq_handler ldr_l\tx1, handle_arch_irq //之前GIC驱动注册好的gic_handle_irq mov\tx0, sp irq_stack_entry blr\tx1 irq_stack_exit .endm 内核整个中断处理流程为：\n中断触发，处理器去异常向量表找到对应的入口，比如EL0的中断跳转到 el0_irq 处，EL1 则跳转到 el1_irq处。在GIC驱动中，会调用 set_handle_irq 接口来设置 handle_arch_irq 的函数指针，让它指向gic_handle_irq，因此中断触发的时候会跳转到 gic_handle_irq 处执行；\ngic_handle_irq 函数处理时，分为两种情况，一种是外设触发的中断，硬件中断号在 16 ~ 1020 之间，一种是软件触发的中断，用于处理器之间的交互，硬件中断号在16以内； 外设触发中断后，根据 irq domain 去查找对应的Linux IRQ中断号，进而得到中断描述符irq_desc，最终也就能调用到外设的中断处理函数了。 GIC/Arch 相关的介绍就此打住，接下来是Linux内核的通用中断处理框架。\n2 通用框架层 上一章主要讲了底层硬件GIC驱动，以及Arch-Specific的中断代码，本文将研究下通用的中断处理的过程，属于硬件无关层。下面的内容将围绕两个问题：\n用户是怎么使用中断的（中断注册）？ 外设触发中断信号时，最终是怎么调用到中断handler的（中断处理）？ 2.1 数据结构分析 先来看一下总的数据结构，核心是围绕着 struct irq_desc 来展开：\nLinux内核的中断处理，围绕着中断描述符结构 struct irq_desc 展开，内核提供了两种中断描述符组织形式：\n打开 CONFIG_SPARSE_IRQ 宏（中断编号不连续），中断描述符以 radix-tree 来组织，用户在初始化时进行动态分配，然后再插入 radix-tree 中； 关闭 CONFIG_SPARSE_IRQ 宏（中断编号连续），中断描述符以数组的形式组织，并且已经分配好； 不管哪种形式，最终都可以通过 linux irq 号来找到对应的中断描述符； 图的左侧灰色部分，上一章已经介绍过，主要**在中断控制器驱动中进行初始化设置，**包括各个结构中函数指针的指向等，其中 struct irq_chip 用于对中断控制器的硬件操作，struct irq_domain 与中断控制器对应，完成的工作是硬件中断号到 Linux irq 的映射。\n图的上侧灰色部分，中断描述符的创建（这里指 CONFIG_SPARSE_IRQ ），主要在获取设备中断信息的过程中完成的，从而让设备树中的中断能与具体的中断描述符 irq_desc 匹配，在后文的中断注册中会介绍。\n图中剩余部分，在设备申请注册中断的过程中进行设置，比如 struct irqaction 中 handler 的设置，这个用于指向我们设备驱动程序中的中断处理函数了。\n中断的处理主要有以下几个核心功能模块：\n硬件中断号到 Linux irq 中断号的映射，并创建好 irq_desc 中断描述符； 中断注册时，先获取设备的中断号，根据中断号找到对应的 irq_desc ，并将设备的中断处理函数添加到 irq_desc 中； 设备触发中断信号时，根据硬件中断号得到 Linux irq 中断号，找到对应的 irq_desc ，最终调用到设备的中断处理函数。 以上功能模块构成了Linux中断框架的核心部分，下面详细分析。\n2.2 中断框架分析 中断注册 首先思考一个问题: 用户是怎么使用中断的？\n熟悉设备驱动的同学应该都清楚，经常会在驱动程序中调用 request_irq() 接口或者request_threaded_irq() 接口来注册设备的中断处理函数； request_irq()/request_threaded_irq 接口中，都需要用到 irq，也就是中断号，那么这个中断号是从哪里来的呢？它是 Linux irq，它又是如何映射到具体的硬件设备的中断号的呢？ 似乎有些跑题了，但实际上在用户进行**中断注册之前内核必然会进行一些基本的设置。**这样就来到了Linux中断框架中一个重要功能模块: 设备硬件中断到 Linux irq 中断号的映射。\n为什么需要中断映射？\n硬件中断号: GIC为每一个硬件中断源都分配了一个唯一编号，称为硬件中断号，用于区分不同的中断源。GIC-v3支持的硬件中断类型和分配的硬件中断号范围如下: 1 2 3 SGI: 0-15 PPI: 16-31 SPI: 21-1019 软件中断号: 系统在中断注册和中断处理过程中使用到的中断号。有的地方也称为虚拟中断号。 为什么要进行中断映射呢？简单来讲，软件可以不需要关注该中断在硬件上是哪个中断来源。简单的SOC内部对中断的管理也比较简单，通常会有一个全局的中断状态寄存器来记录外设中断，这样直接将硬件中断号线性映射到软件中断号即可。但是随着芯片技术的发展，SOC越来越复杂，通常内部会有多个中断控制器（比如GIC interrupt controller, GPIO interrupt controller）, 每一个中断控制器对应多个中断号，而硬件中断号在不同的中断控制器上是会重复编码， 这时仅仅用硬中断号已经不能唯一标识一个外设中断。尤其在多个中断控制器级联的情况下，会变得更加复杂。这样对软件编程来讲极不友好，作为软件工程师，我们更愿意集中精力关注软件层面的内容。\n中断映射的过程如下图:\n硬件设备的中断信息都在设备树 device tree 中进行了描述，在系统启动过程中，这些信息都已经加载到内存中并得到了解析； 驱动中通常会使用 platform_get_irq 或 irq_of_parse_and_map 接口，去根据设备树的信息去创建映射关系（硬件中断号到linux irq中断号映射）； 之前提到过 struct irq_domain 用于完成映射工作，因此在 irq_create_fwspec_mapping 接口中，会先去找到匹配的 irq domain，再去回调该 irq domain 中的函数集 (irq_domain_ops)，通常 irq domain 都是在中断控制器驱动中初始化的，以 ARM GICv2 为例，最终回调到 gic_irq_domain_hierarchy_ops 中的函数；\n如果已经创建好了映射，那么可以直接进行返回 linux irq 中断号了，否则的话需要 irq_domain_alloc_irqs来创建映射关系。irq_domain_alloc_irqs 完成两个工作:\n针对 linux irq 中断号创建一个 irq_desc 中断描述符； 调用 domain-\u0026gt;ops-\u0026gt;alloc 函数来完成映射，在 ARM GICv2 驱动中对应 gic_irq_domain_alloc 函数，这个函数很关键，所以下文重点介绍一下。 gic_irq_domain_alloc 函数如下:\ngic_irq_domain_translate：负责解析出设备树中描述的中断号和中断触发类型（边缘触发、电平触发等）； gic_irq_domain_map：将硬件中断号和linux软中断号绑定到一个结构中，也就完成了映射，此外还绑定了irq_desc 结构中的其他字段，**最重要的是设置了 irq_desc-\u0026gt;handle_irq 的函数指针，这个最终是中断响应时往上执行的入口，**这个是关键，下文讲述中断处理过程时还会提到； 根据硬件中断号的范围设置 irq_desc-\u0026gt;handle_irq 的指针，共享中断入口为 handle_fasteoi_irq，私有中断入口为 handle_percpu_devid_irq； 上述函数执行完成后，完成了两大工作：\n硬件中断号与Linux中断号完成映射，并为Linux中断号创建了irq_desc中断描述符； 数据结构的绑定及初始化，关键的地方是设置了中断处理执行流程的入口； 内核已经完成了基本的设置，即为中断注册做好了充分准备，我们现在可以正式开启中断注册的流程了：\n设备驱动中，获取到了 irq 中断号后，通常就会采用 request_irq/request_threaded_irq 来注册中断，其中request_irq 用于注册普通处理的中断，request_threaded_irq 用于注册线程化处理的中断。在讲具体的注册流程前，先看一下主要的中断标志位：\n1 2 3 4 5 6 7 8 9 10 11 12 #define IRQF_SHARED\t0x00000080 //多个设备共享一个中断号，需要外设硬件支持 #define IRQF_PROBE_SHARED\t0x00000100 //中断处理程序允许sharing mismatch发生 #define __IRQF_TIMER\t0x00000200 //时钟中断 #define IRQF_PERCPU\t0x00000400 //属于特定CPU的中断 #define IRQF_NOBALANCING\t0x00000800 //禁止在CPU之间进行中断均衡处理 #define IRQF_IRQPOLL\t0x00001000 //中断被用作轮训 #define IRQF_ONESHOT\t0x00002000 //一次性触发的中断，不能嵌套，1）在硬件中断处理完成后才能打开中断；2）在中断线程化中保持关闭状态，直到该中断源上的所有thread_fn函数都执行完 #define IRQF_NO_SUSPEND\t0x00004000 //系统休眠唤醒操作中，不关闭该中断 #define IRQF_FORCE_RESUME\t0x00008000 //系统唤醒过程中必须强制打开该中断 #define IRQF_NO_THREAD\t0x00010000 //禁止中断线程化 #define IRQF_EARLY_RESUME\t0x00020000 //系统唤醒过程中在syscore阶段resume，而不用等到设备resume阶段 #define IRQF_COND_SUSPEND\t0x00040000 //与NO_SUSPEND的用户共享中断时，执行本设备的中断处理函数 中断注册流程如下图：\nrequest_irq 也是调用 request_threaded_irq，只是在传参的时候，线程处理函数 thread_fn 函数设置成 NULL； 由于在硬件中断号和Linux中断号完成映射后，irq_desc 已经创建好，可以通过 irq_to_desc 接口去获取对应的 irq_desc； 创建 irqaction，并初始化该结构体中的各个字段，其中包括传入的中断处理函数赋值给对应的字段； __setup_irq 用于完成中断的相关设置，包括中断线程化的处理： 中断线程化用于减少系统关中断的时间，增强系统的实时性； ARM64 默认开启了 CONFIG_IRQ_FORCED_THREADING，引导参数传入 threadirqs 时，则除了IRQF_NO_THREAD外的中断，其他的都将强制线程化处理； 中断线程化会为每个中断都创建一个内核线程，如果中断进行共享，对应irqaction将连接成链表，每个irqaction 都有 thread_mask 位图字段，当所有共享中断都处理完成后才能 unmask 中断，解除中断屏蔽； 中断处理 当完成中断的注册后，所有结构的组织关系都已经建立好，剩下的工作就是当设备中断信号来临时，进行中断的处理工作。回顾下 Arch-specific 的中断处理流程：\n中断收到之后，首先会跳转到异常向量表的入口处，进而逐级进行回调处理，最终调用到 generic_handle_irq来进行中断处理。generic_handle_irq处理如下图：\ngeneric_handle_irq 函数最终会调用到 desc-\u0026gt;handle_irq()，这个也就是对应到上文中在建立映射关系的过程中，调用 irq_domain_set_info 函数，设置好了函数指针，也就是 handle_fasteoi_irq 和handle_percpu_devid_irq，下面看这两个函数：\n**handle_fasteoi_irq：**处理共享中断，并且遍历 irqaction 链表，逐个调用 action-\u0026gt;handler()函数，这个函数正是设备驱动程序调用request_irq/request_threaded_irq接口注册的中断处理函数，此外如果中断线程化处理的话，还会调用__irq_wake_thread()唤醒内核线程； **handle_percpu_devid_irq：**处理per-CPU中断处理，在这个过程中会分别调用中断控制器的处理函数进行硬件操作，该函数同样也会调用 action-\u0026gt;handler() 来进行中断处理； 中断线程化处理 中断线程化处理后的唤醒流程 __handle_irq_event_percpu-\u0026gt;__irq_wake_thread：\n中断线程初始化：irq_thread 内核线程，将根据是否为强制中断线程化对函数指针 handler_fn 进行初始化，以便后续进行调用。 中断线程执行：irq_thread 内核线程将在 while(!irq_wait_for_interrupt) 循环中进行中断的处理，当满足条件时，执行handler_fn，在该函数中最终调用 action-\u0026gt;thread_fn，也就是完成了中断的处理； 中断线程唤醒条件判定：irq_wait_for_interrupt 函数，将会判断中断线程的唤醒条件，如果满足了，则将当前任务设置成TASK_RUNNING 状态，并返回 0，这样就能执行中断的处理，否则就调用 schedule() 进行调度，让出CPU，并将任务设置成 TASK_INTERRUPTIBLE 可中断睡眠状态； 2.3 总结 中断的处理，总体来说可以分为两部分来看：\n**自顶向下：围绕 irq_desc 中断描述符建立好连接关系，这个过程就包括：中断源信息的解析（设备树），硬件中断号到Linux中断号的映射、irq_desc 结构的分配及初始化（内部各个结构的组织关系）、中断的注册（填充 irq_desc 结构，包括handler处理函数）等，总而言之，就是完成静态关系创建，为中断处理做好准备； ***自底向上：***当外设触发中断信号时，中断控制器接收到信号并发送到处理器，此时处理器进行异常模式切换，并逐步从处理器架构相关代码逐级回调。如果涉及到中断线程化，则还需要进行中断内核线程的唤醒操作，最终完成中断处理函数的执行。 ","date":"2024-10-16T18:14:17+08:00","permalink":"https://zcxggmu.github.io/p/linux-interrupt-handle_1/","title":"Linux Interrupt Handle_1"},{"content":"hugo_auto_workflow\n","date":"2024-10-16T16:13:20+08:00","permalink":"https://zcxggmu.github.io/p/hello_hugo/","title":"Hello_hugo"},{"content":"??? ","date":"2024-10-16T16:10:44+08:00","permalink":"https://zcxggmu.github.io/p/mysecond/","title":"MySecond"},{"content":"hello hugo ","date":"2024-10-16T15:50:18+08:00","permalink":"https://zcxggmu.github.io/p/myfirstblog/","title":"MyFirstBlog"},{"content":"正文测试 而这些并不是完全重要，更加重要的问题是， 带着这些问题，我们来审视一下学生会退会。 既然如何， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 我们不得不面对一个非常尴尬的事实，那就是， 可是，即使是这样，学生会退会的出现仍然代表了一定的意义。 学生会退会，发生了会如何，不发生又会如何。 经过上述讨论， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 学生会退会，到底应该如何实现。 这样看来， 在这种困难的抉择下，本人思来想去，寝食难安。 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 就我个人来说，学生会退会对我的意义，不能不说非常重大。 莎士比亚曾经提到过，人的一生是短的，但如果卑劣地过这一生，就太长了。这似乎解答了我的疑惑。 莫扎特说过一句富有哲理的话，谁和我一样用功，谁就会和我一样成功。这启发了我， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 学生会退会，到底应该如何实现。 一般来说， 从这个角度来看， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 在这种困难的抉择下，本人思来想去，寝食难安。 了解清楚学生会退会到底是一种怎么样的存在，是解决一切问题的关键。 一般来说， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 问题的关键究竟为何？ 而这些并不是完全重要，更加重要的问题是。\n奥斯特洛夫斯基曾经说过，共同的事业，共同的斗争，可以使人们产生忍受一切的力量。　带着这句话，我们还要更加慎重的审视这个问题： 一般来讲，我们都必须务必慎重的考虑考虑。 既然如此， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 带着这些问题，我们来审视一下学生会退会。 我认为， 我认为， 在这种困难的抉择下，本人思来想去，寝食难安。 问题的关键究竟为何？ 每个人都不得不面对这些问题。 在面对这种问题时， 要想清楚，学生会退会，到底是一种怎么样的存在。 我认为， 既然如此， 每个人都不得不面对这些问题。 在面对这种问题时， 那么， 我认为， 学生会退会因何而发生。\n引用 思念是最暖的忧伤像一双翅膀\n让我停不了飞不远在过往游荡\n不告而别的你 就算为了我着想\n这么沉痛的呵护 我怎么能翱翔\n最暖的憂傷 - 田馥甄\n图片 1 2 3 ![Photo by Florian Klauer on Unsplash](florian-klauer-nptLmg6jqDo-unsplash.jpg) ![Photo by Luca Bravo on Unsplash](luca-bravo-alS7ewQ41M8-unsplash.jpg) ![Photo by Helena Hertz on Unsplash](helena-hertz-wWZzXlDpMog-unsplash.jpg) ![Photo by Hudai Gayiran on Unsplash](hudai-gayiran-3Od_VKcDEAA-unsplash.jpg) 相册语法来自 Typlog\n","date":"2020-09-09T00:00:00Z","image":"https://zcxggmu.github.io/p/test-chinese/helena-hertz-wWZzXlDpMog-unsplash_hu4699868770670889127.jpg","permalink":"https://zcxggmu.github.io/p/test-chinese/","title":"Chinese Test"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code A B C D E F Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus ultricies, sapien non euismod aliquam, dui ligula tincidunt odio, at accumsan nulla sapien eget ex. Proin eleifend dictum ipsum, non euismod ipsum pulvinar et. Vivamus sollicitudin, quam in pulvinar aliquam, metus elit pretium purus Proin sit amet velit nec enim imperdiet vehicula. Ut bibendum vestibulum quam, eu egestas turpis gravida nec Sed scelerisque nec turpis vel viverra. Vivamus vitae pretium sapien Code Blocks Code block with backticks 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Diff code block 1 2 3 4 5 [dependencies.bevy] git = \u0026#34;https://github.com/bevyengine/bevy\u0026#34; rev = \u0026#34;11f52b8c72fc3a568e8bb4a4cd1f3eb025ac2e13\u0026#34; - features = [\u0026#34;dynamic\u0026#34;] + features = [\u0026#34;jpeg\u0026#34;, \u0026#34;dynamic\u0026#34;] List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL + ALT + Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nHyperlinked image The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-03-11T00:00:00Z","image":"https://zcxggmu.github.io/p/markdown-syntax-guide/pawel-czerwinski-8uZPynIu-rQ-unsplash_hu6307248181568134095.jpg","permalink":"https://zcxggmu.github.io/p/markdown-syntax-guide/","title":"Markdown Syntax Guide"},{"content":"Lorem est tota propiore conpellat pectoribus de pectora summo.\nRedit teque digerit hominumque toris verebor lumina non cervice subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc caluere tempus inhospita parcite confusaque translucet patri vestro qui optatis lumine cognoscere flos nubis! Fronde ipsamque patulos Dryopen deorum.\nExierant elisi ambit vivere dedere Duce pollice Eris modo Spargitque ferrea quos palude Rursus nulli murmur; hastile inridet ut ab gravi sententia! Nomine potitus silentia flumen, sustinet placuit petis in dilapsa erat sunt. Atria tractus malis.\nComas hunc haec pietate fetum procerum dixit Post torum vates letum Tiresia Flumen querellas Arcanaque montibus omnes Quidem et Vagus elidunt The Van de Graaf Canon\nMane refeci capiebant unda mulcebat Victa caducifer, malo vulnere contra dicere aurato, ludit regale, voca! Retorsit colit est profanae esse virescere furit nec; iaculi matertera et visa est, viribus. Divesque creatis, tecta novat collumque vulnus est, parvas. Faces illo pepulere tempus adest. Tendit flamma, ab opes virum sustinet, sidus sequendo urbis.\nIubar proles corpore raptos vero auctor imperium; sed et huic: manus caeli Lelegas tu lux. Verbis obstitit intus oblectamina fixis linguisque ausus sperare Echionides cornuaque tenent clausit possit. Omnia putatur. Praeteritae refert ausus; ferebant e primus lora nutat, vici quae mea ipse. Et iter nil spectatae vulnus haerentia iuste et exercebat, sui et.\nEurytus Hector, materna ipsumque ut Politen, nec, nate, ignari, vernum cohaesit sequitur. Vel mitis temploque vocatus, inque alis, oculos nomen non silvis corpore coniunx ne displicet illa. Crescunt non unus, vidit visa quantum inmiti flumina mortis facto sic: undique a alios vincula sunt iactata abdita! Suspenderat ego fuit tendit: luna, ante urbem Propoetides parte.\n","date":"2019-03-09T00:00:00Z","image":"https://zcxggmu.github.io/p/placeholder-text/matt-le-SJSpo9hQf7s-unsplash_hu10664154974910995856.jpg","permalink":"https://zcxggmu.github.io/p/placeholder-text/","title":"Placeholder Text"},{"content":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\nCreate a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates like so: 1 2 3 {{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }} To enable KaTeX globally set the parameter math to true in a project\u0026rsquo;s configuration To enable KaTeX on a per page basis include the parameter math: true in content files Note: Use the online reference of Supported TeX Functions\nExamples Inline math: $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…$\nBlock math: $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$","date":"2019-03-08T00:00:00Z","permalink":"https://zcxggmu.github.io/p/math-typesetting/","title":"Math Typesetting"},{"content":"Emoji can be enabled in a Hugo project in a number of ways.\nThe emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site\u0026rsquo;s configuration and then you can type emoji shorthand codes directly in content files; e.g.\n🙈 :see_no_evil: 🙉 :hear_no_evil: 🙊 :speak_no_evil:\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\nN.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n1 2 3 .emoji { font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols; } ","date":"2019-03-05T00:00:00Z","image":"https://zcxggmu.github.io/p/emoji-support/the-creative-exchange-d2zvqp3fpro-unsplash_hu5876398126655421130.jpg","permalink":"https://zcxggmu.github.io/p/emoji-support/","title":"Emoji Support"}]